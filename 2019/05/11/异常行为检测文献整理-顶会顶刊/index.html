<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="EN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/RedFish_32x.ico?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/RedFish_16x.ico?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="2019Latent Space Autoregression for Novelty Detection链接：paper、code 来源：CVPR2019 摘要：将一个参数密度估计器（parametric density estimator）加在自编码器上。参数密度估计器通过自回归过程（autoregressive procedure）学习潜在表示的概率分布。与正常样本的重建相结合进行优化的最大">
<meta name="keywords" content="anomaly detection">
<meta property="og:type" content="article">
<meta property="og:title" content="异常行为检测文献整理(顶会顶刊)">
<meta property="og:url" content="http://yoursite.com/2019/05/11/异常行为检测文献整理-顶会顶刊/index.html">
<meta property="og:site_name" content="求索">
<meta property="og:description" content="2019Latent Space Autoregression for Novelty Detection链接：paper、code 来源：CVPR2019 摘要：将一个参数密度估计器（parametric density estimator）加在自编码器上。参数密度估计器通过自回归过程（autoregressive procedure）学习潜在表示的概率分布。与正常样本的重建相结合进行优化的最大">
<meta property="og:locale" content="EN">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H9497e10861f1499dbb1534ead70ff677Q.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H3479b38018a6479f88ad590934ec1649L.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H8ec91df4835d4f02ab8c900c57e7caa5M.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/Hf4649cb3fac0482bb45e18ec5e1ff2ebH.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H332f813492e644e488e6ec3b89a535f0G.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H9238e8c430f543479d198412b9d711dbJ.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1GzbSdA5E3KVjSZFCq6zuzXXaQ.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/He2d73e0096074fd2b4f8063626527f33e.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H7ed4edce4f85450fb58d79625736dcac5.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H14f2ef3986bf42d1a74ba466bde56b9dZ.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H12ff9c27c01240bda3b1cca75ad4fa805.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/89215767.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/34727984.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/Hdac2389e6ad14d2587238c7c40f8bd9ff.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/86054775.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1bl_1dBKw3KVjSZTE763uRpXaB.png">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/51723764.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H2059b341a83b497d80fb758cb78d7827f.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H1174a34af2d44b7a90ef024c208eaf3fC.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H906777bcca3743abbd727ade2508833ei.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/24681967.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1r_YIckxz61VjSZFt761DSVXal.png">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1GOj2dBCw3KVjSZFl763JkFXaH.png">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H2df84d908df84b04a7bf162ef1856159s.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/95069565.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1s82VdBiE3KVjSZFM762QhVXa1.png">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H7ad08d98bc414fb1a0f2b35dbcfe33578.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H21567749ae354cfbbd51b1705529c265Z.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H120a89f6db754bcb8163c676c55c4380V.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/98412371.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H42606ae38757456da44b5699d7370daa3.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1WxYWdA9E3KVjSZFG76319XXaT.png">
<meta property="og:image" content="https://ae01.alicdn.com/kf/H5edf427c496f4978b538af92baba9a4fD.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/85776732.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/35454189.jpg">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1J7k_aLBj_uVjSZFp7630SXXay.png">
<meta property="og:image" content="https://puui.qpic.cn/fans_admin/0/3_1655376438_1561086415091/0">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB12Or5dBKw3KVjSZTE763uRpXaN.png">
<meta property="og:image" content="https://ae01.alicdn.com/kf/HTB1FIn0dEuF3KVjSZK9762VtXXaF.png">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/31332758.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/42528664.jpg">
<meta property="og:image" content="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/12979627.jpg">
<meta property="og:updated_time" content="2019-12-06T03:33:13.270Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="异常行为检测文献整理(顶会顶刊)">
<meta name="twitter:description" content="2019Latent Space Autoregression for Novelty Detection链接：paper、code 来源：CVPR2019 摘要：将一个参数密度估计器（parametric density estimator）加在自编码器上。参数密度估计器通过自回归过程（autoregressive procedure）学习潜在表示的概率分布。与正常样本的重建相结合进行优化的最大">
<meta name="twitter:image" content="https://ae01.alicdn.com/kf/H9497e10861f1499dbb1534ead70ff677Q.jpg">



  <link rel="alternate" href="/atom.xml" title="求索" type="application/atom+xml" />




  <link rel="canonical" href="http://yoursite.com/2019/05/11/异常行为检测文献整理-顶会顶刊/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>异常行为检测文献整理(顶会顶刊) | 求索</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="EN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">求索</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />Search</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/11/异常行为检测文献整理-顶会顶刊/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Song Yu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/header.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="求索">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">异常行为检测文献整理(顶会顶刊)
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-05-11 13:25:57" itemprop="dateCreated datePublished" datetime="2019-05-11T13:25:57+08:00">2019-05-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-12-06 11:33:13" itemprop="dateModified" datetime="2019-12-06T11:33:13+08:00">2019-12-06</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/paper/" itemprop="url" rel="index"><span itemprop="name">paper</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/11/异常行为检测文献整理-顶会顶刊/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">Comments: </span> <span class="post-comments-count valine-comment-count" data-xid="/2019/05/11/异常行为检测文献整理-顶会顶刊/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/11/异常行为检测文献整理-顶会顶刊/" class="leancloud_visitors" data-flag-title="异常行为检测文献整理(顶会顶刊)">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Views: </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h2><h3 id="Latent-Space-Autoregression-for-Novelty-Detection"><a href="#Latent-Space-Autoregression-for-Novelty-Detection" class="headerlink" title="Latent Space Autoregression for Novelty Detection"></a>Latent Space Autoregression for Novelty Detection</h3><p><strong>链接</strong>：<a href="https://arxiv.org/pdf/1807.01653.pdf" target="_blank" rel="noopener">paper</a>、<a href="https://github.com/aimagelab/novelty-detection" target="_blank" rel="noopener">code</a></p>
<p><strong>来源：</strong>CVPR2019</p>
<p><strong>摘要：</strong>将一个参数密度估计器（parametric density estimator）加在自编码器上。参数密度估计器通过自回归过程（autoregressive procedure）学习潜在表示的概率分布。与正常样本的重建相结合进行优化的最大似然目标，通过最小化潜在向量分布的差分熵（differential entropy），有效地充当了调节器（regularizer）。</p>
<p><strong>创新点</strong>：第一次将entropy minimization的方法用在显著性检测中，之前是用在（deep neural<br>compression）中。</p>
<p><strong>网络结构</strong>：</p>
<p><img src="https://ae01.alicdn.com/kf/H9497e10861f1499dbb1534ead70ff677Q.jpg" alt="img"></p>
<h3 id="Margin-Learning-Embedded-Prediction-for-Video-Anomaly-Detection-with-A-Few-Anomalies"><a href="#Margin-Learning-Embedded-Prediction-for-Video-Anomaly-Detection-with-A-Few-Anomalies" class="headerlink" title="Margin Learning Embedded Prediction for Video Anomaly Detection with A Few Anomalies"></a>Margin Learning Embedded Prediction for Video Anomaly Detection with A Few Anomalies</h3><p><strong>来源：</strong>IJCAI2019</p>
<p><strong>创新点：</strong>提出了Margin Learning Em- bedded Prediction (MLEP) framework for open-set supervised anomaly detection。所提出的网络有三个特征，第一，将每一帧的特征顺序输入到ConvLSTM中，更好地编码时间和空间信息。第二，把margin learning 嵌入网络结构中。有助于观测到和未观测到的异常的检测。第三，能够通过帧级别和视频级别的异常标记处理异常检测。</p>
<p><strong>贡献：</strong>1.设计了MLEP来进行open-set supervised 异常检测。2.设计了一个预测框架对预测正常行为有利。3.网络可通过帧级别以及视频级别的异常标记处理异常检测。</p>
<p><strong>针对问题：</strong>unet有利于异常行为的预测；卷积编码器没有足够的能力编码运动信息进而预测正常帧；卷积lstm用历史运动信息，可能会预测异常行为。（说实话，这些理由都好牵强…）</p>
<p><strong>网络结构：</strong><br><img src="https://ae01.alicdn.com/kf/H3479b38018a6479f88ad590934ec1649L.jpg" alt="img"></p>
<p><img src="https://ae01.alicdn.com/kf/H8ec91df4835d4f02ab8c900c57e7caa5M.jpg" alt="img"><br>其中的margin learning模块的loss是triplet loss，灵感来自Person re- identification by multi-channel parts-based cnn with im- proved triplet loss function. 和A unified embedding for face recog- nition and clustering. </p>
<p><img src="https://ae01.alicdn.com/kf/Hf4649cb3fac0482bb45e18ec5e1ff2ebH.jpg" alt="img"><br>整个的loss就是两个loss加起来。<br><img src="https://ae01.alicdn.com/kf/H332f813492e644e488e6ec3b89a535f0G.jpg" alt="img"><br><strong>训练阶段：</strong>对于frame-level的标记，随机选择一个anchor，一个正样本和一个负样本来训练。对于video-level的标记，首先仅用正常数据训练一个基于预测的异常检测网络，这时不加triplet loss ，即λ=0。然后我们用训练好的模型预测正常和异常数据的正常分数。最后我们用sampled 片段来重新训练整个网络。测试阶段：仍然用PSNR来判断。越高表示越有可能是正常的。 </p>
<p><strong>实验细节：</strong>所有的帧resize成224X224。一个video snippet的长度是4。把测试集中的异常数据分成K折，每折仅包含一些异常事件，而不是全部的异常事件。K=10。训练的时候，把其中一折放进训练集中，然后把剩下的作为测试集。由此保证了测试集必须包含训练集没有包含的异常事件，测试集可能包含训练集中观察到的异常事件类型。为什么自己的预测的方法好也和别的方法进行了对比。用cyclegan的3个卷积层和6个残差块作为编码器，解码器用三个反卷积层。</p>
<p><strong>数据集：</strong>Avenue 和shanghaitech。不用ucf crime是因为正常和异常数据的比例是均衡的，另外摄像头的角度变化，对于预测来说不理想。另外他是一个closed-set supervised异常检测，所以不进行比较。</p>
<p><strong>疑惑的地方：</strong>4.2节imbalanced video classfication methods。</p>
<a id="more"></a>
<h3 id="Memorizing-Normality-to-Detect-Anomaly-Memory-augmented-Deep-Autoencoder-for-Unsupervised-Anomaly-Detection"><a href="#Memorizing-Normality-to-Detect-Anomaly-Memory-augmented-Deep-Autoencoder-for-Unsupervised-Anomaly-Detection" class="headerlink" title="Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection"></a>Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</h3><p><strong>链接</strong>：<a href="https://github.com/donggong1/memae-anomaly-detection" target="_blank" rel="noopener">code</a></p>
<p><strong>来源：</strong>ICCV2019</p>
<p><strong>创新点：</strong>提出用一个memory module来augment 自编码器，叫做MemAE。MemAE首先从encoder获得编码，然后在训练阶段，记忆内容是更新的，测试时，学习到的记忆固定，从正常数据中选择一些记录的记忆得到重建。</p>
<p><strong>针对问题：</strong>深度自编码器可能会很好地重建异常行为。比如，一些异常和正常的训练数据有相同的compositional patterns 或者decoder解码一些异常编码的时候太厉害了。</p>
<p><strong>关键词：</strong> attention based memory addressing</p>
<p><img src="https://ae01.alicdn.com/kf/H9238e8c430f543479d198412b9d711dbJ.jpg" alt=""></p>
<h3 id="Graph-Convolutional-Label-Noise-Cleaner-Train-a-Plug-and-play-Action-Classifier-for-Anomaly-Detection"><a href="#Graph-Convolutional-Label-Noise-Cleaner-Train-a-Plug-and-play-Action-Classifier-for-Anomaly-Detection" class="headerlink" title="Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection"></a>Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection</h3><p><strong>链接：</strong><a href="https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection" target="_blank" rel="noopener">code</a></p>
<p><strong>来源：</strong>CVPR2019</p>
<p><strong>创新点：[首次用GCN来纠正视频分析领域的label noise]</strong> a supervised learning task under noisy labels。只要清除label noise，就可以直接将完全监督的动作分类器(fully supervised action classifiers)应用于弱监督的异常检测，并最大限度地利用这些完善的分类器。为此，设计了一个图卷积网络(graph convolutional network)来校正noisy labels。基于特征相似性和时间一致性(视频的两个特性)，网络将supervisory信号从高置信度的片段传播到低置信度的片段。通过这种方式，网络能够为动作分类器提供cleaned supervision。在测试阶段，我们只需要从动作分类器中获取片段预测，无需做任何后处理。</p>
<p><strong>针对问题：</strong>近年来，对新兴的二元分类范式进行了一些研究，训练数据包括异常和普通视频。只有视频级别的异常标签提供。之前的工作都把弱监督异常检测问题看做是多示例学习(multiple-instance learning)。我们换了角度，看作是noise labels下的监督学习任务。噪声标签指的是异常视频中正常片段的错误注释，因为标记“异常”的视频可能包含相当多的正常片段。因此，一旦清除了噪声标签，就可以直接训练完全监督的动作分类器。</p>
<p><strong>方法：</strong>两个阶段，清洁和分类。清洁阶段中，训练一个cleaner来校正classifier得到的预测噪声，并且提供了更少噪声的refined labels。在分类阶段，action classifier使用cleaned labels重新训练并且产生更可靠的预测。cleaner的主要想法是通过高可信度预测的噪声来消除低可信度预测的噪声。设计了一个GCN来建立高可信度片段和低可信度片段之间的关系。在图中，片段被抽象成顶点 (vertexes) ，异常信息通过边 (edges) 传播。测试的时候，我们不需要cleaner，而是直接获得训练snippet-wise的异常结果。</p>
<p>对两种类型的主流动作分类器进行了大量实验：C3D和TSN。</p>
<p>（我们的label noise cleaner 的目标是在高可信度注释的监督下，在图（整个视频）中对节点（视频片段）进行分类。）</p>
<p>Y=0(只包含正常片段的negative bag)是noiseless。而Y=1是noisy，因为部分是异常的。这就叫做one-sided label noise。</p>
<p><strong>数据集：</strong>UCF-Crime Shanghai Tech UCSD-Peds</p>
<p><img src="https://ae01.alicdn.com/kf/HTB1GzbSdA5E3KVjSZFCq6zuzXXaQ.jpg" alt="图片"></p>
<p><strong>灵感来源：</strong>[Sivan Sabato and Naftali Tishby. Multi-instance learning with any hypothesis class. Journal of Machine Learning Research, 13(1):2999–3039, Oct. 2012.] MIL任务可以被视为在one-sided label noise下学习。</p>
<p>​         <img src="https://ae01.alicdn.com/kf/He2d73e0096074fd2b4f8063626527f33e.jpg" alt="img">       </p>
<h3 id="Learning-Regularity-in-Skeleton-Trajectories-for-Anomaly-Detection-in-Videos"><a href="#Learning-Regularity-in-Skeleton-Trajectories-for-Anomaly-Detection-in-Videos" class="headerlink" title="Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos"></a>Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos</h3><p><strong>链接：</strong><a href="https://github.com/RomeroBarata/skeleton_based_anomaly_detection" target="_blank" rel="noopener">code</a></p>
<p><strong>来源：</strong>CVPR2019</p>
<p><strong>创新点：</strong>用了dynamic skeleton features来建模人运动的正常模式。把skeletal movements分解成global body movement和local body posture。</p>
<p>(Message-Passing Encoder-Decoder Recurrent Neural Network)</p>
<p><strong>针对问题：</strong>1<strong>.</strong>现在的方法基于像素的外观特征和运动特征，然而基于像素的特征是高维非结构化信号，对噪声敏感，它掩盖了关于场景的重要信息；另外，这些特征中呈现的冗余的信息增加了对它们进行训练的模型的负担。2.另外一个现在方法的限制是由于视觉特征和事件真实含义之间存在语义鸿沟，缺乏可解释性。</p>
<p><strong>相关工作：</strong>1传统的用one-class分类的方法在处理具有各种异常类型的大规模数据的时候会获得suboptimal performance。2.基于intensity特征对外观噪声敏感。因此liu【cvpr2018】的工作用了预测的方法，但是光流提取成本高并且远离事件的语义性。</p>
<p><strong>方法：</strong>【在现实的监控视频中，人体骨骼的尺度在很大程度上取决于它们的位置和动作。 对于近场中的骨架，观察到的运动主要受局部因素的影响。 同时，对于远场中的骨架，运动主要受全局运动的影响，而局部变形则大多被忽略。因此进行了分解。】</p>
<p><img src="https://ae01.alicdn.com/kf/H7ed4edce4f85450fb58d79625736dcac5.jpg" alt="图片"></p>
<p>设置了一个附着在人体上的规范参考框架（称为局部框架）。全局分量被定义为原始图像帧内的局部框架中心的绝对位置，它基于骨架边界框的中心。 局部分量定义为从原始运动中减去全局分量后的残差。 它表示骨架关节相对于边界框的相对位置。 由于深度缺失，仅xy坐标不能很好地表示场景中的实际位置。 但是，骨架边界框的大小与场景中骨架的深度相关。 为了弥补这个差距，我们用骨架边界框的宽度和高度fg =（xg，yg，w，h）来扩充全局分量，并使用它们来规范化局部分量。</p>
<p>网络由两个 recurrent encoder-decoder network分支构成， 该模型的每个分支都具有单编码器 - 双解码器架构，具有三个RNN：编码器，重构解码器和预测解码器。网络是和[Unsupervised learning of video representations using LSTMs的工作相似。但是不同点在于，所提出的MPED-RNN不仅通过跨分支消息传递机制对每个单独组件的动态进行建模，还对它们之间的相互依赖性进行建模。用GRU代替LSTM。</p>
<p>输入是长度为T的skeleton segment，然后对于每个时间步长t，骨架ft分解成局部的和全局两个部分，分别送到局部和全局的编码器。</p>
<p><img src="https://ae01.alicdn.com/kf/H14f2ef3986bf42d1a74ba466bde56b9dZ.jpg" alt="img"></p>
<p>损失函数：</p>
<p>​         <img src="https://ae01.alicdn.com/kf/H12ff9c27c01240bda3b1cca75ad4fa805.jpg" alt="img">       </p>
<p>果然！作者用了AlphaPose来检测skeleton。</p>
<h2 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h2><h3 id="real-world-anomaly-detection-in-surveillance-videos"><a href="#real-world-anomaly-detection-in-surveillance-videos" class="headerlink" title="real-world anomaly detection in surveillance videos"></a>real-world anomaly detection in surveillance videos</h3><p><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/89215767.jpg" alt="图片"></p>
<p><strong>链接：</strong><a href="https://github.com/WaqasSultani/AnomalyDetectionCVPR2018" target="_blank" rel="noopener">code</a></p>
<p><strong>来源：</strong>CVPR2018</p>
<p><strong>使用的方法</strong>：MIL（multiple instance learning）多示例学习 </p>
<p><strong>方法步骤：</strong></p>
<p>（1）     positive(某一部分包含异常)，negative（不包含异常）视频。把positive和negative视频分别分成固定数量的segments。每个视频表示为一个包，每个temporal segment表示包里的一个instance。</p>
<p>（2）     对video segments提取C3D features。</p>
<p>（3）     用一个novel ranking loss function（positive bag和negative bag中，在最高分数的instances之间计算ranking loss）来训练一个全连接神经网络。</p>
<p>简言之就是数据处理、提特征【提取到的特征应该是时空特征吧】、训练网络、通过得分预测是否异常。</p>
<p><strong>创新点：</strong>同时利用正常和异常的视频来学习异常行为。不需要标记训练视频中的异常segments or clips（非常浪费时间），而是利用弱标记（weakly labeled）的训练视频，通过deep multiple instance ranking framework来学习异常。视频标记（异常或正常）是video-level，而不是clip-level的。我们把正常和异常的视频看作是bags，把video segments看作是instances。【采用MIL的方法引入到异常检测中来】</p>
<p>另外，对ranking loss function引入了<strong>sparsity</strong>和<strong>temporal smoothness constraints</strong> 来在训练中更好的定位异常。</p>
<p>（有新的数据集）</p>
<p><strong>关键词：</strong>weakly-supervised learning，MIL</p>
<p><strong>针对问题：</strong>1<strong>.</strong>其他的方法都是假设偏离正常的行为就是异常。但是这样假设是有问题的，因为把所有可能的正常行为考虑进去是不太可能的。2.正常和异常之间的界限是模糊的。在现实场景中，是否异常可能和条件的不同有关。</p>
<p><strong>Baseline methods：</strong>C3D,TCNN（这两种方法在数据集上的效果很差，证明提出来的数据集非常challenging）</p>
<p><strong>比较：</strong>主要和Learning temporal regularity in video sequences和Abnormal event detection at 150 fps in matlab的方法比较。</p>
<p>【the first to formulate the video anomaly detection problem in the context of MIL】</p>
<p><strong>个人感想与总结：</strong>（采用了什么方法，达到了什么效果，还有什么不太好的地方可以改进）作者采用MIL方法，同时利用正常和异常的视频，使用提出的deep MIL ranking loss来进行异常检测。【把异常检测作为一个回归问题】</p>
<h3 id="Future-Frame-Prediction-for-anomaly-detection-a-new-baseline"><a href="#Future-Frame-Prediction-for-anomaly-detection-a-new-baseline" class="headerlink" title="Future Frame Prediction for anomaly detection-a new baseline"></a>Future Frame Prediction for anomaly detection-a new baseline</h3><p><strong>链接：</strong><a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018" target="_blank" rel="noopener">code</a></p>
<p><strong>来源</strong>：CVPR2018</p>
<p><strong>创新点：</strong>在视频预测框架中解决异常检测问题。除了加spatial（<strong>Appearance</strong>）约束还加了temporal（<strong>motion</strong>）约束（光流）。也用到了GAN。<br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/34727984.jpg" alt="图片"></p>
<p><strong>细节 ：1.作者为什么用U-net？</strong>因为现在的工作进行帧预测或者图像生成的通常包含两个模块，一个编码器能够通过逐渐地降低spatial resolution来提取特征，一个解码器能够通过增加spatial resolution来恢复帧。这样的结构有梯度消失问题和信息不平衡。U-net可以抑制梯度消失的问题。</p>
<p><strong>2.</strong>Intensity 为了保证在RGB空间所有像素的相似性，gradient可以锐化产生的图。</p>
<p><strong>评价别人的工作：</strong>Learning temporal regularity in video sequences【16年】。Abnormal event detection at 150 FPS in MATLAB【13年】。Anomaly detection in crowded scenes【10年】。Of all these work, the idea of <strong>feature reconstruction for normal training data</strong> is a commonly used strategy. </p>
<p>Sparse reconstruction cost for abnormal event detection【11年】。Abnormal event detection at 150 FPS in MATLAB【13年】。hand-crafted features。</p>
<p><strong>思考：</strong>我觉得作者有漏洞的地方：1.【假设正常的事件可以被很好的预测。】可是就像之前作者说自编码器那种重建的思想假设正常的事件可以以较小的误差被重建出来，但是深度神经网络的容量很高，异常事件不一定有更大的重建误差。那么在这里，正常的事件可以被很好的预测，异常的事件就不能被很好的预测吗？</p>
<p>2.t帧高的PSNR表示这帧很有可能是正常的。人为设置阈值来判断是正常还是异常帧。</p>
<p>3.没有像素级的检测</p>
<p>4.作者自己说 there exsits some uncertainties in normal events</p>
<p>5.对于作者用的gap，计算的是正常帧的平均分数和异常帧的平均分数之间的gap。【这个平均分数是不是有一点点问题，就是有没有一些正常帧其实有很小的psnr，异常帧有很大的psnr，平均是不是抹去了一些差别..重要吗。会造成一定程度的漏检吧】。而且最后加constraint的多少，看到的gap不是差很多，就差一点，不过auc还是提高了一些的。</p>
<p>6.在和conv-AE对比的时候，可以看到，ped1场景和avenue数据集并没有比conv-AE好太多。而且这个是平均之后的结果，真的有变好吗？</p>
<p>7.在最后作者用toy dataset来评估效果的时候，出现了有时正常事件的运动方向也不确定的情况。【其实就是正常的事件有时也不能很好的预测呀】</p>
<h3 id="Adversarially-Learned-One-Class-Classifier-for-Novelty-Detection"><a href="#Adversarially-Learned-One-Class-Classifier-for-Novelty-Detection" class="headerlink" title="Adversarially Learned One-Class Classifier for Novelty Detection"></a>Adversarially Learned One-Class Classifier for Novelty Detection</h3><p><strong>链接：</strong><a href="https://github.com/khalooei/ALOCC-CVPR2018" target="_blank" rel="noopener">code</a></p>
<p><strong>来源</strong>：CVPR2018</p>
<p><strong>创新点：【enhance inlier，distort outlier】</strong>(1)提出来一个端到端的结构进行one-class classification。(2)几乎其他的所有基于GAN的方法在训练之后或者抛弃了生成器或者抛弃了判别器。但我们的方法更有效，<strong>能够从两个训练的模块受益</strong>。(3)用在图片或视频中。</p>
<p><strong>方法：</strong></p>
<p><img src="https://ae01.alicdn.com/kf/Hdac2389e6ad14d2587238c7c40f8bd9ff.jpg" alt="img"></p>
<h2 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h2><h3 id="Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge"><a href="#Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge" class="headerlink" title="Joint detection and recounting of abnormal events by learning deep generic knowledge"></a>Joint detection and recounting of abnormal events by learning deep generic knowledge</h3><p><strong>来源</strong>：ICCV2017</p>
<p><strong>创新点：</strong>把检测和描述视频中的异常事件联合起来。Recounting of abnormal events,就是解释为什么他们是异常的。我们把一个generic CNN model和environment-dependent anomaly detection融合起来。</p>
<p>（异常检测是有场景依赖性的）【动作理解动作识别的方法能不能用上？】</p>
<p><strong>关键词：</strong>anomaly detector</p>
<p><strong>方法：</strong>based on multi-task Fast R-CNN</p>
<ol>
<li>用大量带标签的数据集【不是anomaly detection的dataset】来学习multi-task Fast R-CNN，学习到generic model。这样提取出deep features 和visual concept classification scores（同时提出的）。</li>
<li>对每个环境在这些特征和分数上学习到anomaly detectors，建模了目标环境的正常行为并且预测测试样本的异常分数。anomaly detectors和classification scores分别用来做异常行为检测和描述。{anomaly detectors有几种，NN OC-SVM KDE}</li>
<li>之后就是用以上两个学到的模型做异常检测和描述。分为四个步骤：a) <strong>detect object proposal</strong>. b) <strong>extract features</strong>.这一步由multi-task Fast R-CNN从所有的object proposal同时提语义特征和分类分数。c) <strong>classify normal/abnormal</strong>.将anomaly detector用到proposal的语义特征计算出每个proposal的异常分数，高于设定阈值的被确定为异常行为的源域。d) <strong>recount abnormal events.</strong> 异常行为的三种类型（objects,action, attributes）的visual concepts通过分类分数预测。</li>
</ol>
<p><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/86054775.jpg" alt="图片">方法简言之就是提semantic特征和分类分数，特征用来判异常/正常，分类分数用来做描述，<strong>不需要使用motion features。</strong></p>
<p><strong>疑惑的地方：（1）</strong>The anomaly scores of each predicted concept are computed by the anomaly detector for classification scores to recount the evidence of anomaly detection.<strong>（2）（3.1）</strong>The bounding box regression was not used because it depends on the class to detect, which is not determined in abnormal event detection.</p>
<p><strong>关于数据集：</strong>只选取USCD ped2和avenue进行验证，因为ped1的分辨率比较低，所以不用。关于avenue的像素级的标记有些扯淡（比较主观），比如在扔包的异常事件中，包仅仅被标记为异常，因此仅在frame-level进行评估。另外，由于avenue把moving objects看做异常，而该paper研究static objects，因此评估除去了22个clips中的5个。</p>
<p><strong>结果：</strong>这个paper在<strong>avenue</strong>数据集上的auc达到<strong>89.2，ped2</strong>数据集上的auc达到90.8。FRCN的semantic feature总比HOG和SDAE特征表现好，并且不管用什么anomaly detector都比HOG和SDAE好。</p>
<p><img src="https://ae01.alicdn.com/kf/HTB1bl_1dBKw3KVjSZTE763uRpXaB.png" alt="img"></p>
<h3 id="unmasking-the-abnormal-events-in-video"><a href="#unmasking-the-abnormal-events-in-video" class="headerlink" title="unmasking the abnormal events in video"></a>unmasking the abnormal events in video</h3><p><strong>来源</strong>：ICCV2017</p>
<p><strong>创新点：</strong>不需要training sequences，我们的网络基于unmasking，是之前用来在文本文件中做授权认证的。<br>【the first work to apply unmasking for a computer vision task】<br>作者和【6 （2016）A Discriminative Framework for Anomaly Detection in Large Videos】还有一些监督的方法进行比较。<strong>【6】是主要借鉴的思想。</strong><br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/51723764.jpg" alt="图片"></p>
<p>提特征，训练分类器</p>
<p><strong>unmask是怎么用的：</strong>We retain the training accuracy of the classifier and repeat the training process by eliminating some of the best features.这个过程就叫做unmasking。如图中所示，After extracting motion or appearance features (step D), we apply unmasking (steps E to G) by training a classifier and removing the highly weighted features for a number of k loops.</p>
<p>【The unmasking technique [12] is based on testing the degradation rate of the cross-validation accuracy of learned models, as the best features are iteratively dropped from the learning process.We modify the original unmasking technique by considering the training accuracy instead of the cross-validation accuracy, in order to use this approach for online abnormal event detection in video.】 </p>
<p><strong>相关工作：</strong>很多方法不是完全无监督的。在此之前，唯一<strong>不用任何训练数据</strong>进行异常事件检测的是【 A Discriminative Framework for Anomaly Detection in Large Videos 2016】。作者的方法和这个很类似。但是作者的方法可以<strong>在线</strong>处理。作者的方法相当于在这个上面进行了改进。</p>
<p><strong>一些说明：</strong>对10x10x5的立方块算3D gradient feature，用的是【 A Discriminative Framework for Anomaly Detection in Large Videos 2016】和【 Abnormal Event Detection at 150 FPS in MATLAB. 2013】的方法【<a href="https://github.com/alliedel/anomalyframework_python" target="_blank" rel="noopener">https://github.com/alliedel/anomalyframework_python</a>】来计算<strong>运动特征</strong>，并且不用PCA。对于<strong>外观特征</strong>，用的是VGG-f，用这个考虑的是实时性所以没有用比较深的CNN；在这儿也不对CNN进行fine-tuning，因为本文的方法不能用任何训练数据，所以只是用预训练的CNN提特征；并且，去掉全连接层，将conv5的结果作为外观特征。</p>
<p>对于评估指标，<strong>EER</strong>在真实的异常检测中可能是具有误导性的，所以不用这个。</p>
<p><strong>结论中提到：</strong>采用了<strong>融合运动和外观特征的方法</strong>，但是没有看到大量的改善，需要进一步改进融合的方法。比如，可以用方法来在一个相关任务如action recognition上训练无监督的深度特征，然后用这些特征来同时表示运动和外观信息。</p>
<p><strong>疑惑的地方：1.</strong>【related work】As the authors want to build an approach independent of temporal ordering, they create shuffles of the data by permuting the frames before running each instance of the change detection. <strong>2.【3】</strong>2x2 spatial bins.什么是bin，为什么不直接写成4。<strong>3.【3.2】</strong>为什么假设前w帧标记为正常，后w帧标记为异常，然后训练一个分类器。这样假设有什么用？并且为什么分类器的准确率高后w帧就是异常，低的话后w帧就是正常。<strong>我觉得分类器的准确率高是异常低是正常可以这样解释：</strong>因为分类器分的准确的标志应该是将某两个特征区分度很多的类分开，如果前w是正常，后w是异常，那么分类器的准确率此时应该高，反之应该低。那么前面的假设也可以说的通了，就是相当于一个起始条件吧。</p>
<p><strong>题外话：</strong>【这些作者在17年的时候还写了一篇论文：Deep Appearance Features for Abnormal Behavior Detection in Video，前面的提特征方法相同。】</p>
<h3 id="abnormal-event-detection-in-videos-using-generative-adversarial-nets"><a href="#abnormal-event-detection-in-videos-using-generative-adversarial-nets" class="headerlink" title="abnormal event detection in videos using generative adversarial nets"></a>abnormal event detection in videos using generative adversarial nets</h3><p><strong>来源</strong>：ICIP2017</p>
<p><strong>方法：</strong>用正常的帧和对应的光流图来训练GAN，来学习正常场景的internal  representation。在测试的时候把真实的数据和GAN产生的外观和运动表示比较，通过计算local 不同来检测异常区域。<br>从raw-pixel frames<strong>产生光流图</strong>。</p>
<h3 id="a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework"><a href="#a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework" class="headerlink" title="a revisit of sparse coding based anomaly detection in stacked RNN framework"></a>a revisit of sparse coding based anomaly detection in stacked RNN framework</h3><p><strong>链接</strong>：<a href="https://github.com/StevenLiuWen/sRNN_TSC_Anomaly_Detection" target="_blank" rel="noopener">code</a></p>
<p><strong>来源</strong>：ICCV2017</p>
<p><strong>摘要：</strong>提出了TSC（<strong>Temporally-coherent</strong> sparse coding），enforce 相似的相邻帧用相似的重建系数编码。之后用srnn映射TSC,方便了参数优化加速了异常预测。<strong>用sRNN同时学习所有参数</strong>，能够避免TSC的non-trivial的超参数选择。另外用浅层的sRNN，重建稀疏系数可以在前向传播中推断出来，节约了计算成本。</p>
<p><strong>创新点：</strong>（1）提了TSC，可映射到sRNN方便了参数优化，加速了异常预测。（2）提了新数据集。作者这个新数据集的特点是不是特意设计异常事件，而是用在不同的spots安装的摄像头采集多种角度。</p>
<p>【为什么提出TSC？因为基于稀疏编码的异常检测方法不考虑相邻帧之间的temporal coherence。相似的特征也可能被编为不同的稀疏编码，丢失了位置信息。为了保留相邻帧之间的相似性，提出了TSC。】主要比较的</p>
<p><strong>baseline的缺点：1.</strong>基于词典学习的方法的sparse coefficients的优化非常耗时间。另外这些方法主要是基于人工特征的，对于视频表示可能不是最佳的。<strong>2.</strong>2016conv-AE基于3D ConvNet，但是之前的工作表明用双流网络分别提取外观和运动信息是视频中特征的提取的一个better solution。 而且conv-AE的输入是video cube，cube中的正常/异常帧可能会影响彼此的分类，因此需要在所有的帧上对数据集进行中心采样，计算代价大。【作者主要参考或者说关注的三篇论文：2013matlab，2010MPPCA，2011online】</p>
<p><strong>行文逻辑：</strong>相关工作（2）。方法（3）：【什么是基于稀疏编码的异常检测（3.1）；他有什么优势和缺点（即为什么用TSC），什么是TSC，如何进行优化（3.2）；如何用sRNN解读TSC（3.3）；如何用sRNN学习参数（3.4）；多种尺度采样的多个patch（3.5）；如何测试（3.6）】。我们的数据集（4）。实验（5）：【设置参数和指标（5.1），用仿真数据集测试（5.2），实际数据集测试（5.3），不同超参数的影响（5.4），运行时间（5.5）。】</p>
<p><strong>方法：</strong>学习能够编码外观上的正常行为的字典，之后，为了提高在相邻帧的预测的平滑性，加上了一个temporally-coherent term。（作者说，有意思的是得到的TSC的公式可以看成是一个特殊的sRNN）。</p>
<p>稀疏编码的目标函数：         <img src="https://ae01.alicdn.com/kf/H2059b341a83b497d80fb758cb78d7827f.jpg" alt="img">      </p>
<p> 第一项对应重建误差，第二项对应sparsity项，lambda平衡了sparsity和重建误差。</p>
<p>TSC的目标函数：</p>
<p>​         <img src="https://ae01.alicdn.com/kf/H1174a34af2d44b7a90ef024c208eaf3fC.jpg" alt="img">       </p>
<p><strong>实验细节：</strong>我认为首先是通过UCF101数据集用ConvNet预训练提取空间特征(没有用运动特征，作者认为不能帮助异常预测)，得到特征图，然后 partition the feature map into increasingly finer regions: 1×1, 2×2, and 4×4。然后最大池化。之后对这些不同尺度的特征学习同一个词典。</p>
<p>测试阶段：将对应于时间t的每块的特征喂给空间sRNN,通过一次前向传播得到αt，就可以计算出对应xt的重建误差，然后选择这帧的所有块的最大重建误差作为帧级别的重建误差，然后做归一化得到每帧的regularity score。</p>
<p>​         <img src="https://ae01.alicdn.com/kf/H906777bcca3743abbd727ade2508833ei.jpg" alt="img">       </p>
<p><strong>评估：</strong>作者有个<strong>挺好的</strong>想法，先用一个Synthesized Dataset评估自己的方法对于外观的突然变化导致的异常的表现如何。这个数据集是这样做的：从MINIST里面随意找两个数字，然后把他们放在225x225尺寸的黑背景中。然后在之后的19帧里，这两个数字随意的横向纵向运动。训练的时候用了10000个序列，对于每个测试的序列，5个连续的帧由随意插入的3x3白色的小方块随意的遮挡。测试集一共有3000个序列。如下图所示：<img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/24681967.jpg" alt="图片"></p>
<p><strong>挑选数据集的一些考虑：</strong>不用subway是因为有不同的真实标记。uscd ped1更通常用在像素级别的异常检测中，本文做的是帧级别。</p>
<h3 id="Remembering-History-With-Convolutional-LSTM-for-Anomaly-Detection"><a href="#Remembering-History-With-Convolutional-LSTM-for-Anomaly-Detection" class="headerlink" title="Remembering History With Convolutional LSTM for Anomaly Detection"></a>Remembering History With Convolutional LSTM for Anomaly Detection</h3><p><strong>链接</strong>：<a href="https://github.com/zachluo/convlstm_anomaly_detection" target="_blank" rel="noopener">code</a></p>
<p><strong>来源</strong>：ICME2017—IEEE International Conference on Multimedia and Expo</p>
<p><strong>摘要：</strong>用CNN对每一帧进行appearance encoding，用ConvLSTM来记忆过去的帧对应于运动信息。然后把cnn和convlstm和自编码器整合起来，称为ConvLSTM-AE学习正常的外观和运动信息。</p>
<p><strong>baseline的缺点：</strong>Learning temporal regularity in video sequences, in CVPR, 2016。<strong>而3D卷积不能很好的encode motion</strong>。作者的全文的关注点，或者说改进比较都是在这篇上的。</p>
<p><strong>实验细节：</strong>T’越大，表示更多的信息被记住了。所以对于有频繁变化的场景，我们可以用一个更小的T‘来保证更高的准确率。</p>
<p><strong>疑惑的地方：</strong>1.Learning temporal regularity in video sequences, in CVPR, 2016。In order to get a frame level anomaly prediction, it has to do the anomaly detection for multiple video clips and interpolate the degree of anomaly for each frame, which is time-consuming.<strong>2.（3.3节）</strong>In other words,we enforce the network to forget all history information every T’ frames to improve the anomaly detection accuracy。每T‘帧为单位忘掉所有的历史信息。</p>
<p><strong>网络结构：</strong></p>
<p>​         <img src="https://ae01.alicdn.com/kf/HTB1r_YIckxz61VjSZFt761DSVXal.png" alt="img">       </p>
<p><strong>公式：</strong></p>
<p>目标函数：</p>
<p>​         <img src="https://ae01.alicdn.com/kf/HTB1GOj2dBCw3KVjSZFl763JkFXaH.png" alt="img">       </p>
<p>重建误差：</p>
<p>​         <img src="https://ae01.alicdn.com/kf/H2df84d908df84b04a7bf162ef1856159s.jpg" alt="img">       </p>
<p><strong>神奇之处：</strong>作者说，和2016那个convae的异常检测方法不同，我们对不同的数据集分别进行训练 ，因为异常的定义不同。所以2016那个是训练出一个通用的模型吗？</p>
<h2 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h2><h3 id="Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection"><a href="#Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection" class="headerlink" title="Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection"></a>Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</h3><p>【the first work to employ the existing CNN models for motion representation in crowd analysis】<br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/95069565.jpg" alt="图片"></p>
<p><strong>来源</strong>：WACV 2018</p>
<p><strong>创新点：</strong>随着时间跟踪CNN特征的变化。通过将semantic information（从已有的CNN中得到）和low-level optical-flow结合来measure local abnormality 。不需要fine-tuning阶段。</p>
<p>Track the changes in the CNN features <strong>across time.</strong></p>
<p>（1）     引入了一个新的Binary Quantization Layer</p>
<p>（2）     提出了一个Temporal CNN Pattern measure 来表示人群中的运动。</p>
<p>（无监督的方法比监督方法在异常检测上更好，因为标注的主观性和训练数据少）</p>
<p><strong>方法步骤：</strong></p>
<p>​         <img src="https://ae01.alicdn.com/kf/HTB1s82VdBiE3KVjSZFM762QhVXa1.png" alt="img">       </p>
<p><strong>1.从输入的视频帧序列中提取CNN-based binary maps.</strong>具体来说是所有的帧输入到一个FCN，把一个binary layer 插在FCN的顶部为了把高维的特征图量化成压缩的二值模式。这个binary layer是一个卷积层，其中的权重是用一个external hashing method来初始化的。对于每个对应于FCN的感受野的patch，binary layer产生binary patterns，叫做binary map。输出的binary maps保留了最初帧的空间关系。         <img src="https://ae01.alicdn.com/kf/H7ad08d98bc414fb1a0f2b35dbcfe33578.jpg" alt="img">       </p>
<p>（其中的FCN层用的是Alexnet）</p>
<p><strong>2.用提到的CNN-based binary maps来计算Temporal CNN Pattern值。</strong>先根据binary maps来计算histograms。然后根据这些histograms计算TCP。【TCP measure 是用来表示人群的运动的motion representation】</p>
<p>{实验中TCP measure的分数来衡量是否异常，公式如下}</p>
<p>​         <img src="https://ae01.alicdn.com/kf/H21567749ae354cfbbd51b1705529c265Z.jpg" alt="img">       </p>
<p><strong>3.将TCP值和低层次的运动特征（光流）来找到refined motion segments。</strong></p>
<p><strong>细节：</strong>Binary Quantization Layer——–为什么用这个层，因为聚类高维的特征图需要很大的成本，另外就是需要事先知道聚类中心，用hashing方法聚类高维特征得到小的binary codes是一种解决方法，24位的binary code可以address 2的24次方聚类中心，另外binary map可以简单表示为三通道的RGB图。我感觉这层的实现就是<img src="https://ae01.alicdn.com/kf/H120a89f6db754bcb8163c676c55c4380V.jpg" alt="img"> ，再通过sigmoid函数，最后通过阈值0.5编码为0或者1。  </p>
<p>Iterative Quantization Hashing（ITQ）——-是所用的hashing方法，训练这个ITQ是所提方法中的唯一训练成本，只需要在训练数据的子集中做一次，用从ITQ学到的weights来建立BQL层。</p>
<p><strong>关键词：</strong>BFCN、TCP</p>
<h3 id="learning-temporal-regularity-in-video-sequences"><a href="#learning-temporal-regularity-in-video-sequences" class="headerlink" title="learning temporal regularity in video sequences"></a>learning temporal regularity in video sequences</h3><p><strong>来源</strong>：CVPR2016</p>
<p><strong>创新点：</strong>学习正常行为模式用非常有限的监督。两种方法：第一种用传统的人工时空局部特征，并在这些特征上面学习一个全连接的自编码器，但是这些特征可能对于学习正常不是最优的；第二种建立一个fully convolutional feed-forward autoencoder来学习局部特征和分类，是一个端到端的框架。<br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/98412371.jpg" alt="图片">第一种Learning Motions on Handcrafted Features。首先用HOG和HOF作为时空appearance feature 描述子。为了提取HOG和HOF特征以及轨迹信息，用的是improved trajectory（IT）features。编码器的输入是204维的HOG+HOF特征。目标函数：</p>
<p><img src="https://ae01.alicdn.com/kf/H42606ae38757456da44b5699d7370daa3.jpg" alt="img">       </p>
<p><strong>xi是特征。</strong></p>
<p>第二种是全卷积自编码器，输入是temporal cuboid。<strong>【因为自编码器的参数数量太大，所以需要大量的数据。】</strong>因此做了这样的事：用不同的<strong>skipping</strong> strides连接帧来建立T-sized input cuboid。目标函数：         <img src="https://ae01.alicdn.com/kf/HTB1WxYWdA9E3KVjSZFG76319XXaT.png" alt="img">       </p>
<p><strong>Xi是第i个cuboid。</strong></p>
<p><strong>baseline的缺点：</strong> “Abnormal event detection at 150 fps in matlab,” in ICCV, 2013； “Online detection of unusual events in videos via dynamic sparse coding,” in CVPR,2011； “Sparse Reconstruction Cost for Abnormal Event Detection,” in CVPR, 2011。稀疏编码的优化计算代价大。词袋不能保留单词的时空结构并且需要单词数的先验信息。</p>
<p><strong>有意思的地方：</strong>1.4.4节predicting the Regular Past and the Future。给中间的帧，能预测near过去的和未来的帧。（预测过去有什么用？）</p>
<p><strong>2.</strong>用了最大池化，空间信息就丢失了，所以在反卷积网络中用了unpooling。</p>
<p><strong>3</strong>.作者在4.1节画了这样的曲线：蓝色是表示在单一的数据集上训练的，红色表示在所有的数据集上训练的，黄色表示在除了要测试的数据集上训练的(足以表明transfer的能力)         <img src="https://ae01.alicdn.com/kf/H5edf427c496f4978b538af92baba9a4fD.jpg" alt="img">       </p>
<p><strong>疑惑的地方：1.（3.1.1）</strong>ReLU is not suitable for a network that has large receptive fields for each neuron as the sum of the inputs to a neuron can become very large.<strong>2.（3.2.1）</strong>The learned filters in the deconvolutional layers serve as bases to reconstruct the shape of an input motion cuboid. As we stack the convolutional layers at the beginning of the network, we stack the deconvolutional layers to capture different levels of shape details for building an autoencoder. The filters in early layers of convolutional and the later layers of deconvolutional layers tend to capture specific motion signature of input video frames while high level motion abstractions are encoded in the filters in later layers.（这段话的用意在哪里）<strong>3.（4.3）</strong>这节都不知道在说啥…</p>
<h3 id="A-discriminative-framework-for-anomaly-detection-in-large-videos"><a href="#A-discriminative-framework-for-anomaly-detection-in-large-videos" class="headerlink" title="A discriminative framework for anomaly detection in large videos"></a>A discriminative framework for anomaly detection in large videos</h3><p><strong>链接</strong>：<a href="https://github.com/alliedel/anomalyframework_ECCV2016" target="_blank" rel="noopener">code</a></p>
<p><strong>来源</strong>：ECCV2016</p>
<p><strong>创新点：</strong>异常的分数是独立于时间顺序的，不需要分开的训练序列。</p>
<h2 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h2><h3 id="crowd-motion-monitoring-using-tracklet-based-commotion-measure"><a href="#crowd-motion-monitoring-using-tracklet-based-commotion-measure" class="headerlink" title="crowd motion monitoring using tracklet-based commotion measure"></a>crowd motion monitoring using tracklet-based commotion measure</h3><p><strong>来源</strong>：ICIP2015</p>
<p><strong>贡献：1.</strong>提出用Motion Pattern来在magnitude和orientation上表示每一帧的tracklet的statistics。<strong>2.</strong>提出Tracklet Binary Code representation在空间和时间上建模一个异常点在其对应轨迹上的运动。<strong>3.</strong>我们提出了一个新的unsupervised measure来评估像素、帧和视频层级的群体场景的commotion。</p>
<p><strong>方法：1.</strong>tracklet extraction。先用SIFT算法检测异常点，再用跟踪的方法跟踪 L+1 帧。下图的(a)。</p>
<p><strong>2.</strong>Motion Pattern。用空间坐标表示对应的异常点，进而计算magnitude和orientation。之后画出来binary polar histogram(只有0、1二值)。把经过vectorized polar histogram叫做motion pattern。下图的(b)。</p>
<p><strong>3.</strong>Tracklet Binary Codes。把上一步的所有motion pattern连接起来计算一个tracklet histogram。H就是Tracklet Binary Codes。下图的(c)。<img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/85776732.jpg" alt="图片"></p>
<p><strong>细节：</strong>在做frame-level的对比实验时，由于和比较的方法HOT（有监督的），没有直接的可比性，所以把视频序列分成了两个子集，A和B。训练和测试两遍，分别是A或者B训练，B或者A测试。和HOT相比，两种方法都比较好，但是我们的方法更好因为我们是无监督的。</p>
<p><strong>疑惑的地方：2中的</strong>commotion measuring的那部分以及<strong>3中的</strong>video-level部分。</p>
<h3 id="Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression"><a href="#Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression" class="headerlink" title="Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression"></a>Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression</h3><p><strong>来源</strong>：CVPR2015</p>
<p><strong>创新点：</strong>通过层级框架来检测<strong>局部和全局</strong>的异常<strong>，</strong>通过层级特征表示和GPR（高斯过程回归）。为了同时检测局部异常和全局异常，我们提出了从训练视频中提取normal interactions 的问题(??)，即有效地找到附近稀疏时空兴趣点的频繁几何关系。用GPR建立并建模了interaction templates的codebook。另外提出了一个新的计算observed interaction的likelihood的inference方法。<br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/35454189.jpg" alt="图片"></p>
<h3 id="Learning-Deep-Representations-of-Appearance-and-Motion-for-Anomalous-Event-Detection"><a href="#Learning-Deep-Representations-of-Appearance-and-Motion-for-Anomalous-Event-Detection" class="headerlink" title="Learning Deep Representations of Appearance and Motion for Anomalous Event Detection"></a>Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</h3><p><strong>来源</strong>：BMVC2015</p>
<p><strong>创新点：</strong>提出了Appearance and Motion DeepNet(AMDN)来自动学习特征表示。stacked denoising autoencoders用来分别学习外观和运动特征，以及联合表示。作者是第一个用无监督的深度学习框架来对视频异常检测自动construct discriminative representations 。设计了两次fusion。第一次叫做pixel-level early fusion，第二次叫做late fusion。</p>
<p><img src="https://ae01.alicdn.com/kf/HTB1J7k_aLBj_uVjSZFp7630SXXay.png" alt="img">       </p>
<p><strong>细节：1.</strong>训练AMDN用两个步骤，pretraining和fine-tuning。<strong>2.</strong>本文用了前景分割！</p>
<p><strong>总结：</strong>简单来说，就是SDAE+one-class SVMs</p>
<p><strong>疑惑的地方：2.2.1节</strong> One-class SVM Modeling  <strong>2.2.2节</strong> Late Fusion for Anomaly Detection</p>
<h2 id="2014"><a href="#2014" class="headerlink" title="2014"></a>2014</h2><h3 id="Anomaly-Detection-and-Localization-in-Crowded-Scenes"><a href="#Anomaly-Detection-and-Localization-in-Crowded-Scenes" class="headerlink" title="Anomaly Detection and Localization in Crowded Scenes"></a>Anomaly Detection and Localization in Crowded Scenes</h3><p><strong>来源</strong>：PAMI</p>
<p>考虑异常行为的检测和定位。提出了同时检测时空异常的detector。</p>
<p>Temporal normalcy用MDT(mixtures of dynamic textures)建模，spatial normalcy由基于MDT的一个discriminant saliency 检测器来检测。</p>
<p>考虑了外观和动态，时间和空间和多种空间规模。提出了USCD数据集。数据集的相关介绍可以看看这篇的6.1.这伙人在cvpr2010anomaly detection in crowded scenes出现过。</p>
<p> 【就是同一篇论文吧。】</p>
<h2 id="2013"><a href="#2013" class="headerlink" title="2013"></a>2013</h2><h3 id="abnormal-event-detection-at-150-fps-in-matlab"><a href="#abnormal-event-detection-at-150-fps-in-matlab" class="headerlink" title="abnormal event detection at 150 fps in matlab"></a>abnormal event detection at 150 fps in matlab</h3><p><strong>链接：</strong><a href="https://github.com/kpandey008/Abnormal-Event-Detection" target="_blank" rel="noopener">non-official code</a></p>
<p><strong>来源</strong>：ICCV2013</p>
<p><strong>里程碑：</strong>avenue数据集是他们弄的</p>
<p><strong>引言：</strong>影响高效率的一个阻碍是建立稀疏表示的inherently intensive computation 。</p>
<p><strong>优势：</strong>快~每秒140-150帧在平常的电脑上。有效地将原来的复杂问题转化为只涉及少量无代价的小尺度最小二乘优化步骤，从而保证了较短的运行时间。</p>
<p><strong>方法：</strong>Sparse combination learning。和子空间聚类subspace clustering有关系但是又和传统的方法大不相同。子空间聚类的方法的聚类数量k是提前知道或者固定的,我们的方法用允许的表示误差来建立组合,误差上限是显式表现的,具有统计意义.</p>
<p><img src="https://puui.qpic.cn/fans_admin/0/3_1655376438_1561086415091/0" alt="img">       </p>
<p><strong>稀疏组合学习有两个目标：一是有效的表示，即找到K个基底组合，有较小的重建误差。二是让组合的总数K足够小。因为K大的话会让重建误差总是接近0，对于异常的事件也是这样。这两个目标是矛盾的。</strong></p>
<p>训练的时候用了一个maximum representation的策略，自动寻找K但是不让重建误差大幅度增加。实际上对于每个训练特征的误差t都有一个上限。我们的方法以迭代的方式执行。在每个pass中，我们只更新<strong>一个</strong>组合，使它尽可能多地表示训练数据。这个过程可以快速找到编码重要和最常见特性的主要组合。不能很好地表示此组合的其余训练块特征将被送到下一轮以收集剩余的最大共性。</p>
<p><strong>其他的方法：</strong>降低字典的大小【 Sparse reconstruction costs for abnormal event detection. In CVPR 2011】和采用快的稀疏编码solvers【 Online detection of unusual events in videos via dynamic sparse coding. In CVPR, 2011】，但是他们仍然不够快。</p>
<p><strong>细节：</strong>每帧resize成3个不同的scale*<em>(20x20,30x40,120x160)，每种scale的frame分成很多小块（10x10的不重叠小块），一共是208个子块(4+12+12</em>16=208)，看图5就知道啦。之后连续5帧的对应的regions堆叠起来组成时空块对于时空快计算3D gradient features 和【 Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models.CVPR2009 】一样。这些特征在视频序列中是根据它们的空间坐标分别处理的。只有在视频帧中相同空间位置的特征才会被一起用于训练和测试。</p>
<p><strong>稀疏组合的检验：</strong>一共150个不同的video，收集了一共208*150groups的cube features。每个group的组合数表示为K。如图4所示：组合数是10就足够用了。图5中很多region用1个组合就够了因为他们是静态的。</p>
<p>​         <img src="https://ae01.alicdn.com/kf/HTB12Or5dBKw3KVjSZTE763uRpXaN.png" alt="img">                <img src="https://ae01.alicdn.com/kf/HTB1FIn0dEuF3KVjSZK9762VtXXaF.png" alt="img">       </p>
<p> <strong>疑惑的地方：1.</strong>  <strong>2.3节</strong>update那里。<strong>2.</strong>table2和table3中的MISC是什么意思。</p>
<h2 id="2011"><a href="#2011" class="headerlink" title="2011"></a>2011</h2><h3 id="Video-Parsing-for-abnormality-detection"><a href="#Video-Parsing-for-abnormality-detection" class="headerlink" title="Video Parsing for abnormality detection"></a>Video Parsing for abnormality detection</h3><p><strong>来源</strong>：ICCV2011</p>
<p><strong>创新点：</strong>Parse video frames by establishing a set of hypotheses that jointly explain all the foreground while, at same time, trying to find normal training samples that explain the hypotheses.</p>
<p><strong>关键词：</strong>object  hypotheses<img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/31332758.jpg" alt="图片"></p>
<h3 id="Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection"><a href="#Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection" class="headerlink" title="Sparse Reconstruction Cost for Abnormal Event Detection"></a>Sparse Reconstruction Cost for Abnormal Event Detection</h3><p><strong>来源</strong>：CVPR2011</p>
<p><strong>摘要：</strong>引入了sparse reconstruction cost。我们的方法提供了一个unified solution来同时检测local abnormal events和global abnormal events。（什么是全局异常呢？就是整个场景是异常的，即使individual local behavior can be normal，什么是局部异常呢？就是local behavior is different from its spatio-temporal neighborhoods.）<br><strong>引言：</strong>稀疏表示能够表示高维度的sample。<br><strong>贡献：</strong>  <strong>1.</strong>support an efficient and robust dstimation of SRC<br><strong>2.</strong>方便地处理LAE和GAE异常。<br><strong>3.</strong>通过逐步更新字典，我们的方法能够支持在线的异常检测。<br><strong>细节：</strong>USCD ped1 数据集处理方法——–把每帧分成了7x7的local patches，有4像素的重叠。用了Type C basis（spatio-temporal basis），dimension 7x16=102.<br>subway数据集处理方法———把帧从512x384大小resize成了320x240大小，并把新的视频帧分成了15x15local patches，有6像素的重叠，用了Type B basis（temporal basis），dimension 16x5=80？？<br><strong>方法：</strong><br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/42528664.jpg" alt="图片"><br>绿色或红色的点是正常或异常的测试样本。representatives（深蓝色的点）的optimal subset通过redundant training features(浅蓝色的点)作为basis来构成正常的字典。深蓝色点的半径表示权重，越大表示越正常。异常检测就是measure 测试样本（绿点或红点）在深蓝色点上的稀疏重建成本。</p>
<h3 id="Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding"><a href="#Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding" class="headerlink" title="Online Detection of Unusual Events in Videos via Dynamic Sparse Coding"></a>Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</h3><p><strong>来源</strong>：CVPR2011</p>
<p><strong>创新点：</strong>We propose a fully <strong>unsupervised dynamic sparse coding approach</strong> for detecting unusual events in videos based on <strong>online</strong> sparse reconstructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases.<br><strong>误检的情况：</strong>Subway Exit数据集里面，出现了小孩误检为异常，一个人停在出口并且回头看也误检。<br><strong>相比前人来说成功的地方：</strong>our method not only detects abnormalities in a fine scale, but also unusual events caused by irregular interactions between people</p>
<h2 id="2010"><a href="#2010" class="headerlink" title="2010"></a>2010</h2><h3 id="Anomaly-Detection-in-Crowded-Scenes"><a href="#Anomaly-Detection-in-Crowded-Scenes" class="headerlink" title="Anomaly Detection in Crowded Scenes"></a>Anomaly Detection in Crowded Scenes</h3><p><strong>来源</strong>：CVPR2010</p>
<p>MDT模型<br>时间异常检测：[23]背景帧差法。GMM  MDT<br>空间异常检测：center surround saliency with the MDT</p>
<h3 id="Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-detection-in-crowded-scenes"><a href="#Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-detection-in-crowded-scenes" class="headerlink" title="Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes"></a>Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</h3><p><strong>来源</strong>：CVPR2010</p>
<p>特殊的粒子轨迹的应用<br>引入了chaotic dynamics </p>
<h2 id="2009"><a href="#2009" class="headerlink" title="2009"></a>2009</h2><h3 id="Abnormal-crowd-behavior-detection-using-social-force-model"><a href="#Abnormal-crowd-behavior-detection-using-social-force-model" class="headerlink" title="Abnormal crowd behavior detection using social force model"></a>Abnormal crowd behavior detection using social force model</h3><p><strong>来源</strong>：CVPR2009</p>
<p>社会力模型<br>Bag of words方法来分类异常和正常<br>这个方法比基于纯光流的方法好。<br><img src="http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/12979627.jpg" alt="图片"></p>
<h3 id="Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates"><a href="#Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates" class="headerlink" title="Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates"></a>Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates</h3><p><strong>来源</strong>：CVPR2009</p>
<p><strong>创新点：</strong>提出了一个空时MRF模型。为了学习每个local node的正常行为模式，用Mixture of Probabilistic Principal Component  Analyzers(MPPCA) 来capture光流的分布。另外，模型参数可以在新的观测进来的时候updated incrementally。<br><strong>方法：</strong>We extract optical flow features at each frame, use MPPCA to identify the typical patterns, and construct a space-time MRF to enable inference at each local site.<br><strong>作者说自己的优势：</strong>1.可以在local和global context检测异常活动。比单纯是local的方法好，local的方法fails to detect abnormal activities with irregular temporal orderings，并且local的方法对于光流参数很敏感导致高的false alarm rate.比单纯是global的方法好，global的方法fails to detect abnormal activity happens within a region so small,这个region在全局的场景中简单的被视为可以忽略的噪声。并且global的方法在拥挤的环境中会产生false alarm。<br><strong>对前人的方法做了什么改进：</strong>用了08年Robust Real-Time Unusual Event Detection Using Multiple…的subway数据集的gt，但是capture 更微小的异常，比如“no payment”和“loitering”.<br><strong>误检或者漏检的情况：</strong>entrance gate数据集中1.走的慢的人。2.对于far-filed area，产生了false alarm，因为光流对于far-filel area是不靠谱的。3.走的很快的人。4.没刷卡的人。exit gate数据集中”from right exit to left exit”</p>
<h2 id="2008"><a href="#2008" class="headerlink" title="2008"></a>2008</h2><h3 id="Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors"><a href="#Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors" class="headerlink" title="Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors"></a>Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</h3><p><strong>来源</strong>：PAMI</p>
<p><strong>方法：</strong>local-monitors-based 。<br>通过multiple, local, low-level feature monitors来监视不寻常的事件。每个monitor是从视频流提取local low-level observation的object。这个observation可以是在monitor的位置的现在的光流方向，或者是local flow的magnitude。<br><strong>异常检测需要什么：</strong>1.对于给定的视频流的tuning 算法应该简单快速。2.算法应该adaptive，适应环境的变换。3.short learning period。4.低成本。5.predictable performance。<br><strong>局限性：</strong>不能检测loitering person或者在进入安检的时候不刷卡。总结里面说，局限性是the lack of <strong>sequential</strong> monitoring.<br><strong>术语：</strong>aperture problem孔径问题<a href="https://blog.csdn.net/hankai1024/article/details/23433157" target="_blank" rel="noopener">https://blog.csdn.net/hankai1024/article/details/23433157</a>；SSD error matrix<br><strong>评价：</strong>2009年Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates评价：focus attention on individual local activities,where typical flow directions and speeds are measured on a grid in the video frame. While efficient and simple to implement, <strong>such an approach fails to model temporal relationships between motions.</strong></p>
<p><strong>思考：</strong>为什么作者用了这个方法，有什么优缺点。</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/anomaly-detection/" rel="tag"># anomaly detection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/11/异常行为检测文献新论文跟进/" rel="next" title="异常行为检测文献新论文跟进">
                <i class="fa fa-chevron-left"></i> 异常行为检测文献新论文跟进
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/14/Next主题优化/" rel="prev" title="Next主题的简单优化(一)">
                Next主题的简单优化(一) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.png"
                alt="Song Yu" />
            
              <p class="site-author-name" itemprop="name">Song Yu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/songfish" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:songyu_727@163.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://runninggump.github.io/" title="important to me" target="_blank">important to me</a>
                  </li>
                
              </ul>
          <div id="days"></div>
<script>
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("05/20/2018 12:06:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="已运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>

            </div>
          

          
            
          
          <div id="days"></div>
<script>
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("05/20/2018 12:06:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="已运行"+daysold+"天"+hrsold+"小时"+minsold+"分"+seconds+"秒";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#2019"><span class="nav-number">1.</span> <span class="nav-text">2019</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Latent-Space-Autoregression-for-Novelty-Detection"><span class="nav-number">1.1.</span> <span class="nav-text">Latent Space Autoregression for Novelty Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Margin-Learning-Embedded-Prediction-for-Video-Anomaly-Detection-with-A-Few-Anomalies"><span class="nav-number">1.2.</span> <span class="nav-text">Margin Learning Embedded Prediction for Video Anomaly Detection with A Few Anomalies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memorizing-Normality-to-Detect-Anomaly-Memory-augmented-Deep-Autoencoder-for-Unsupervised-Anomaly-Detection"><span class="nav-number">1.3.</span> <span class="nav-text">Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph-Convolutional-Label-Noise-Cleaner-Train-a-Plug-and-play-Action-Classifier-for-Anomaly-Detection"><span class="nav-number">1.4.</span> <span class="nav-text">Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Regularity-in-Skeleton-Trajectories-for-Anomaly-Detection-in-Videos"><span class="nav-number">1.5.</span> <span class="nav-text">Learning Regularity in Skeleton Trajectories for Anomaly Detection in Videos</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2018"><span class="nav-number">2.</span> <span class="nav-text">2018</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#real-world-anomaly-detection-in-surveillance-videos"><span class="nav-number">2.1.</span> <span class="nav-text">real-world anomaly detection in surveillance videos</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Future-Frame-Prediction-for-anomaly-detection-a-new-baseline"><span class="nav-number">2.2.</span> <span class="nav-text">Future Frame Prediction for anomaly detection-a new baseline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarially-Learned-One-Class-Classifier-for-Novelty-Detection"><span class="nav-number">2.3.</span> <span class="nav-text">Adversarially Learned One-Class Classifier for Novelty Detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2017"><span class="nav-number">3.</span> <span class="nav-text">2017</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge"><span class="nav-number">3.1.</span> <span class="nav-text">Joint detection and recounting of abnormal events by learning deep generic knowledge</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unmasking-the-abnormal-events-in-video"><span class="nav-number">3.2.</span> <span class="nav-text">unmasking the abnormal events in video</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#abnormal-event-detection-in-videos-using-generative-adversarial-nets"><span class="nav-number">3.3.</span> <span class="nav-text">abnormal event detection in videos using generative adversarial nets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework"><span class="nav-number">3.4.</span> <span class="nav-text">a revisit of sparse coding based anomaly detection in stacked RNN framework</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Remembering-History-With-Convolutional-LSTM-for-Anomaly-Detection"><span class="nav-number">3.5.</span> <span class="nav-text">Remembering History With Convolutional LSTM for Anomaly Detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2016"><span class="nav-number">4.</span> <span class="nav-text">2016</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection"><span class="nav-number">4.1.</span> <span class="nav-text">Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learning-temporal-regularity-in-video-sequences"><span class="nav-number">4.2.</span> <span class="nav-text">learning temporal regularity in video sequences</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-discriminative-framework-for-anomaly-detection-in-large-videos"><span class="nav-number">4.3.</span> <span class="nav-text">A discriminative framework for anomaly detection in large videos</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2015"><span class="nav-number">5.</span> <span class="nav-text">2015</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#crowd-motion-monitoring-using-tracklet-based-commotion-measure"><span class="nav-number">5.1.</span> <span class="nav-text">crowd motion monitoring using tracklet-based commotion measure</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression"><span class="nav-number">5.2.</span> <span class="nav-text">Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Deep-Representations-of-Appearance-and-Motion-for-Anomalous-Event-Detection"><span class="nav-number">5.3.</span> <span class="nav-text">Learning Deep Representations of Appearance and Motion for Anomalous Event Detection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2014"><span class="nav-number">6.</span> <span class="nav-text">2014</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anomaly-Detection-and-Localization-in-Crowded-Scenes"><span class="nav-number">6.1.</span> <span class="nav-text">Anomaly Detection and Localization in Crowded Scenes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2013"><span class="nav-number">7.</span> <span class="nav-text">2013</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#abnormal-event-detection-at-150-fps-in-matlab"><span class="nav-number">7.1.</span> <span class="nav-text">abnormal event detection at 150 fps in matlab</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2011"><span class="nav-number">8.</span> <span class="nav-text">2011</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Video-Parsing-for-abnormality-detection"><span class="nav-number">8.1.</span> <span class="nav-text">Video Parsing for abnormality detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection"><span class="nav-number">8.2.</span> <span class="nav-text">Sparse Reconstruction Cost for Abnormal Event Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding"><span class="nav-number">8.3.</span> <span class="nav-text">Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2010"><span class="nav-number">9.</span> <span class="nav-text">2010</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anomaly-Detection-in-Crowded-Scenes"><span class="nav-number">9.1.</span> <span class="nav-text">Anomaly Detection in Crowded Scenes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-detection-in-crowded-scenes"><span class="nav-number">9.2.</span> <span class="nav-text">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2009"><span class="nav-number">10.</span> <span class="nav-text">2009</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abnormal-crowd-behavior-detection-using-social-force-model"><span class="nav-number">10.1.</span> <span class="nav-text">Abnormal crowd behavior detection using social force model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates"><span class="nav-number">10.2.</span> <span class="nav-text">Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2008"><span class="nav-number">11.</span> <span class="nav-text">2008</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors"><span class="nav-number">11.1.</span> <span class="nav-text">Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Song Yu</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.3.0</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="Total Views">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



	






  





  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'TjbP3X04F2EXxf8C3IT1q4wn-gzGzoHsz',
        appKey: '3enScwbjsIRXMoaHpe7sdQn4',
        placeholder: 'Just go go',
        avatar:'monsterid',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("TjbP3X04F2EXxf8C3IT1q4wn-gzGzoHsz", "3enScwbjsIRXMoaHpe7sdQn4");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            
            counter.save(null, {
              success: function(counter) {
                
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.get('time'));
                
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            
              var newcounter = new Counter();
              /* Set ACL */
              var acl = new AV.ACL();
              acl.setPublicReadAccess(true);
              acl.setPublicWriteAccess(true);
              newcounter.setACL(acl);
              /* End Set ACL */
              newcounter.set("title", title);
              newcounter.set("url", url);
              newcounter.set("time", 1);
              newcounter.save(null, {
                success: function(newcounter) {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
                },
                error: function(newcounter, error) {
                  console.log('Failed to create');
                }
              });
            
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  

  
  

  

  

  

  

  

</body>
</html>
