{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/hexo-theme-next-master/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/RedFish_16x.ico","path":"images/RedFish_16x.ico","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/RedFish_32x.ico","path":"images/RedFish_32x.ico","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/images/header.png","path":"images/header.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1}],"Cache":[{"_id":"themes/hexo-theme-next-master/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.eslintrc.json","hash":"cc5f297f0322672fe3f684f823bc4659e4a54c41","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.gitignore","hash":"a18c2e83bb20991b899b58e6aeadcb87dd8aa16e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.stickler.yml","hash":"b7939095038cbdc4883fc10950e163a60a643b43","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.travis.yml","hash":"3d1dc928c4a97933e64379cfde749dedf62f252c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/LICENSE.md","hash":"fc7227c508af3351120181cbf2f9b99dc41f063e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/README.md","hash":"807c28ad6473b221101251d244aa08e2a61b0d60","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/_config.yml","hash":"493aacc74faf723bbab8ba1f0329d5106ad0ae89","modified":1534317931207},{"_id":"themes/hexo-theme-next-master/bower.json","hash":"a8c832da6aad5245052aed7ff26c246f85d68c6c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/crowdin.yml","hash":"e026078448c77dcdd9ef50256bb6635a8f83dca6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/gulpfile.coffee","hash":"48d2f9fa88a4210308fc41cc7d3f6d53989f71b7","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/package.json","hash":"11a0b27f92da8abf1efbea6e7a0af4271d7bff9e","modified":1526470537000},{"_id":"source/_posts/.title.md.swp","hash":"76905734c171e10bda855de54821a6d502fc01e9","modified":1526794875072},{"_id":"source/_posts/Next主题优化.md","hash":"9a8045700ff39b3a5a967dc70bb3a7a83e00ae92","modified":1541920689146},{"_id":"source/_posts/git学习.md","hash":"7d9fb325a16852df533f426ed6f98b527c197450","modified":1533566999732},{"_id":"source/_posts/学习使用TensorFlow来识别交通标志.md","hash":"03f2fc3a5a78cf5e3805f7963150f287916a7d90","modified":1536808414378},{"_id":"source/_posts/异常行为检测文献整理-顶会顶刊.md","hash":"4be4c5f760b2aa6856362e3a545de7f617100e68","modified":1541934311241},{"_id":"source/_posts/异常行为检测文献新论文跟进.md","hash":"6173776705108a9e396af87035aebed0c25caebe","modified":1541934187379},{"_id":"source/about/index.md","hash":"7279e323135b48f391750fdca50ec221dd4beec1","modified":1533611518657},{"_id":"source/categories/index.md","hash":"764585423003ac397c83a109c7540012e0541a4c","modified":1533436636601},{"_id":"source/tags/index.md","hash":"c6f7e39e3ce70118042b2250c82a899171d7f636","modified":1533436476638},{"_id":"source/title/index.md","hash":"d4764f5cdd4cceb4251a0b30a7aa894edbc6adbb","modified":1526793309172},{"_id":"themes/hexo-theme-next-master/.github/CODE_OF_CONDUCT.md","hash":"b63696d41f022525e40d7e7870c3785b6bc7536b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.github/CONTRIBUTING.md","hash":"f846118d7fc68c053df47b24e1f661241645373f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.github/ISSUE_TEMPLATE.md","hash":"00c25366764e6b9ccb40b877c60dc13b2916bbf7","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.github/PULL_REQUEST_TEMPLATE.md","hash":"7abbb4c8a29b2c14e576a00f53dbc0b4f5669c13","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/.github/stale.yml","hash":"fd0856f6745db8bd0228079ccb92a662830cc4fb","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/ALGOLIA-SEARCH.md","hash":"141e989844d0b5ae2e09fb162a280715afb39b0d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/AUTHORS.md","hash":"7b24be2891167bdedb9284a682c2344ec63e50b5","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/DATA-FILES.md","hash":"8e1962dd3e1b700169b3ae5bba43992f100651ce","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/INSTALLATION.md","hash":"2bbdd6c1751b2b42ce9b9335da420c6026a483e9","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"120750c03ec30ccaa470b113bbe39f3d423c67f0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/LICENSE","hash":"fe607fe22fc9308f6434b892a7f2d2c5514b8f0d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/MATH.md","hash":"0ae4258950de01a457ea8123a8d13ec6db496e53","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/UPDATE-FROM-5.1.X.md","hash":"ad57c168d12ba01cf144a1ea0627b2ffd1847d3e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/de.yml","hash":"fb478c5040a4e58a4c1ad5fb52a91e5983d65a3a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/default.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/en.yml","hash":"c540c3a0d7db2d4239293c8783881962640b6c34","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/fr.yml","hash":"0162a85ae4175e66882a9ead1249fedb89200467","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/id.yml","hash":"e7fb582e117a0785036dcdbb853a6551263d6aa6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/it.yml","hash":"62ef41d0a9a3816939cb4d93a524e6930ab9c517","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/ja.yml","hash":"5f8e54c666393d1ca2e257f6b1e3b4116f6657d8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/ko.yml","hash":"fae155018ae0efdf68669b2c7dd3f959c2e45cc9","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/nl.yml","hash":"bb9ce8adfa5ee94bc6b5fac6ad24ba4605d180d3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/pt-BR.yml","hash":"bfc80c8a363fa2e8dde38ea2bc85cd19e15ab653","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/pt.yml","hash":"3cb51937d13ff12fcce747f972ccb664840a9ef3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/ru.yml","hash":"db0644e738d2306ac38567aa183ca3e859a3980f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/tr.yml","hash":"c5f0c20743b1dd52ccb256050b1397d023e6bcd9","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/vi.yml","hash":"8da921dd8335dd676efce31bf75fdd4af7ce6448","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/zh-CN.yml","hash":"fbbf3a0b664ae8e927c700b0a813692b94345156","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/zh-HK.yml","hash":"7903b96912c605e630fb695534012501b2fad805","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/languages/zh-TW.yml","hash":"6e6d2cd8f4244cb1b349b94904cb4770935acefd","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_layout.swig","hash":"09e8a6bfe5aa901c66d314601c872e57f05509e8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/archive.swig","hash":"2b6450c6b6d2bcbcd123ad9f59922a5e323d77a5","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/category.swig","hash":"5d955284a42f802a48560b4452c80906a5d1da02","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/index.swig","hash":"53300ca42c00cba050bc98b0a3f2d888d71829b1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/page.swig","hash":"79040bae5ec14291441b33eea341a24a7c0e9f93","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/post.swig","hash":"e7458f896ac33086d9427979f0f963475b43338e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/schedule.swig","hash":"3e9cba5313bf3b98a38ccb6ef78b56ffa11d66ee","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/tag.swig","hash":"ba402ce8fd55e80b240e019e8d8c48949b194373","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/helpers.js","hash":"392cda207757d4c055b53492a98f81386379fc4f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/merge-configs.js","hash":"33afe97284d34542015d358a720823feeebef120","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/ru/DATA-FILES.md","hash":"d6d20f60f77a76c77f8e65d0c9adbd79d0274557","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/ru/INSTALLATION.md","hash":"6c5d69e94961c793da156217ecf1179e868d7ba1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/ru/README.md","hash":"712d9a9a557c54dd6638adfb0e1d2bb345b60756","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/ru/UPDATE-FROM-5.1.X.md","hash":"b1dd18d9b890b21718883ea1832e7e02a773104a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"6855402e2ef59aae307e8bd2a990647d3a605eb8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"a45a791b49954331390d548ac34169d573ea5922","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/CONTRIBUTING.md","hash":"44e4fb7ce2eca20dfa98cdd1700b50d6def4086f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/DATA-FILES.md","hash":"f3eec572a7d83542e2710a7404082014aaa1a5e7","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/INSTALLATION.md","hash":"b19a6e0ae96eb7c756fb5b1ba03934c7f9cbb3c3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"24cf2618d164440b047bb9396263de83bee5b993","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/MATH.md","hash":"e03607b608db4aa7d46f6726827c51ac16623339","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/README.md","hash":"84d349fda6b9973c81a9ad4677db9d9ee1828506","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"c1ba919f70efe87a39e6217883e1625af0b2c23c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_custom/head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_custom/sidebar.swig","hash":"bb6929c1215c62665d1def8608e86abf070f70ba","modified":1533433672792},{"_id":"themes/hexo-theme-next-master/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_macro/post-copyright.swig","hash":"05e67c50a4f3a20fad879ed61b890de8ca6ba4ea","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_macro/post-related.swig","hash":"08fe30ce8909b920540231e36c97e28cfbce62b6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_macro/post.swig","hash":"686e60ede86547bdd7bc34c3629e4c9dbd134a21","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_macro/reward.swig","hash":"bd5778d509c51f4b1d8da3a2bc35462929f08c75","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_macro/sidebar.swig","hash":"7fcea470c04728fa8ad7214600bb6998330aa75b","modified":1533433890208},{"_id":"themes/hexo-theme-next-master/layout/_macro/wechat-subscriber.swig","hash":"a9e1346b83cf99e06bed59a53fc069279751e52a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/.comments.swig.swp","hash":"8247eef2e982a513191c7807c0c9441dc2d73208","modified":1533380414104},{"_id":"themes/hexo-theme-next-master/layout/_partials/breadcrumb.swig","hash":"6994d891e064f10607bce23f6e2997db7994010e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/comments.swig","hash":"5df32b286a8265ba82a4ef5e1439ff34751545ad","modified":1533381064456},{"_id":"themes/hexo-theme-next-master/layout/_partials/footer.swig","hash":"1ae77b6a369f83c9986408f2ab448090e37cd2dc","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/page-header.swig","hash":"1aaf32bed57b976c4c1913fd801be34d4838cc72","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/pagination.swig","hash":"dbe321bcf3cf45917cc11a3e3f50d8572bac2c70","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/boostrap.swig","hash":"0a0129e926c27fffc6e7ef87fe370016bc7a4564","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/commons.swig","hash":"6fc63d5da49cb6157b8792f39c7305b55a0d1593","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/noscript.swig","hash":"ac3ad2c0eccdf16edaa48816d111aaf51200a54b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/vendors.swig","hash":"e0bdc723d1dc858b41fd66e44e2786e6519f259f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/bookmark.swig","hash":"60001c8e08b21bf3a7afaf029839e1455340e95d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/copy-code.swig","hash":"a8ab2035654dd06d94faf11a35750529e922d719","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/exturl.swig","hash":"f532ce257fca6108e84b8f35329c53f272c2ce84","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/github-banner.swig","hash":"cabd9640dc3027a0b3ac06f5ebce777e50754065","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/needsharebutton.swig","hash":"927f19160ae14e7030df306fc7114ba777476282","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/pangu.swig","hash":"6b75c5fd76ae7cf0a7b04024510bd5221607eab3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/rating.swig","hash":"fc93b1a7e6aed0dddb1f3910142b48d8ab61174e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/scroll-cookie.swig","hash":"b0ca46e0d1ff4c08cb0a3a8c1994f20d0260cef9","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/button.js","hash":"5a61c2da25970a4981fbd65f4a57c5e85db4dcda","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/center-quote.js","hash":"db70a841e7c1708f95ca97b44413b526b267fa9b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/exturl.js","hash":"2b3a4dc15dea33972c0b6d46a1483dabbf06fb5b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/full-image.js","hash":"a98fc19a90924f2368e1982f8c449cbc09df8439","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/group-pictures.js","hash":"1b97b1b5364945b8ab3e50813bef84273055234f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/include-raw.js","hash":"b7600f6b868d8f4f7032126242d9738cd1e6ad71","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/label.js","hash":"621004f2836040b12c4e8fef77e62cf22c561297","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/lazy-image.js","hash":"460e5e1f305847dcd4bcab9da2038a85f0a1c273","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/note.js","hash":"4975d4433e11161b2e9a5744b7287c2d667b3c76","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/scripts/tags/tabs.js","hash":"5786545d51c38e8ca38d1bfc7dd9e946fc70a316","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/main.styl","hash":"c26ca6e7b5bd910b9046d6722c8e00be672890e0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/RedFish_16x.ico","hash":"69d9ffaa4a78ca18c9117a2c649f2077966d3335","modified":1533459287128},{"_id":"themes/hexo-theme-next-master/source/images/RedFish_32x.ico","hash":"d363f00c80ba667edfcc8e68a2a87b80f6cb2764","modified":1533459344845},{"_id":"themes/hexo-theme-next-master/source/images/algolia_logo.svg","hash":"ec119560b382b2624e00144ae01c137186e91621","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/logo.svg","hash":"d29cacbae1bdc4bbccb542107ee0524fe55ad6de","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/images/header.png","hash":"c15abb9dbeb1c8a57059762b796c8893ae9402e1","modified":1527385765834},{"_id":"themes/hexo-theme-next-master/layout/_macro/menu/menu-badge.swig","hash":"65c5e585982dae7ae1542cada71858b4ea1f73d6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_macro/menu/menu-item.swig","hash":"d1b73c926109145e52605929b75914cc8b60fb89","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/head/head-unique.swig","hash":"a7e376b087ae77f2e2a61ba6af81cde5af693174","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/head/head.swig","hash":"00bf33b3c557b8f7e9faf49b226ea6ff7df5cda0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/header/brand.swig","hash":"fd780171713aada5eb4f4ffed8e714617c8ae6be","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/header/index.swig","hash":"2082f5077551123e695e8afec471c9c44b436acb","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/header/menu.swig","hash":"3db735d0cd2d449edf2674310ac1e7c0043cb357","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/header/sub-menu.swig","hash":"88b4b6051592d26bff59788acb76346ce4e398c2","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/search/index.swig","hash":"a33b29ccbdc2248aedff23b04e0627f435824406","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_partials/share/jiathis.swig","hash":"048fd5e98149469f8c28c21ba3561a7a67952c9b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/pages/post-details.swig","hash":"cc865af4a3cb6d25a0be171b7fc919ade306bb50","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/schemes/gemini.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_scripts/schemes/pisces.swig","hash":"ea03fe9c98ddcfcc0ecfdbe5a2b622f9cde3b3a1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/analytics-with-widget.swig","hash":"98df9d72e37dd071e882f2d5623c9d817815b139","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/busuanzi-counter.swig","hash":"67f0cb55e6702c492e99a9f697827629da036a0c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/facebook-sdk.swig","hash":"a234c5cd1f75ca5731e814d0dbb92fdcf9240d1b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/firestore.swig","hash":"94b26dfbcd1cf2eb87dd9752d58213338926af27","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/google-analytics.swig","hash":"beb53371c035b62e1a2c7bb76c63afbb595fe6e5","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/index.swig","hash":"5e9bb24c750b49513d9a65799e832f07410002ac","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/lean-analytics.swig","hash":"cee047575ae324398025423696b760db64d04e6f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/disqus.swig","hash":"8878241797f8494a70968756c57cacdfc77b61c7","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/gitment.swig","hash":"fe8177e4698df764e470354b6acde8292a3515e0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/hypercomments.swig","hash":"6df755ef6f735a91751d6dd8273ccb213c84967d","modified":1533380265662},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/index.swig","hash":"40e3cacbd5fa5f2948d0179eff6dd88053e8648e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/livere.swig","hash":"8a2e393d2e49f7bf560766d8a07cd461bf3fce4f","modified":1533380963838},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/valine.swig","hash":"c0eb6123464d745ac5324ce6deac8ded601f432f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/comments/youyan.swig","hash":"42f62695029834d45934705c619035733762309e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/math/index.swig","hash":"a6fc00ec7f5642aabd66aa1cf51c6acc5b10e012","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/math/katex.swig","hash":"97dbc2035bcb5aa7eafb80a4202dc827cce34983","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/math/mathjax.swig","hash":"9b9ff4cc6d5474ab03f09835a2be80e0dba9fe89","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/search/localsearch.swig","hash":"b15e10abe85b4270860a56c970b559baa258b2a8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_mixins/Gemini.styl","hash":"2aa5b7166a85a8aa34b17792ae4f58a5a96df6cc","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_mixins/Pisces.styl","hash":"2640a54fa63bdd4c547eab7ce2fc1192cf0ccec8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_mixins/base.styl","hash":"81ca13d6d0beff8b1a4b542a51e3b0fb68f08efd","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_variables/Gemini.styl","hash":"7a2706304465b9e673d5561b715e7c72a238437c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_variables/Mist.styl","hash":"be087dcc060e8179f7e7f60ab4feb65817bd3d9f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_variables/Pisces.styl","hash":"32392d213f5d05bc26b2dc452f2fc6fea9d44f6d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_variables/base.styl","hash":"cfb03ec629f13883509eac66e561e9dba562333f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/bootstrap.js","hash":"40de94fd18fcbd67a327d63b0d1e242a08aa5404","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/motion.js","hash":"50e57f8acb6924c6999cdcc664ddd3f0730d2061","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/post-details.js","hash":"d1333fb588d4521b4d1e9c69aef06e0ad1bf0b12","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/utils.js","hash":"4284c67ea1435de2acd523f6d48c0d073fd1ad03","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/search/algolia-search/assets.swig","hash":"6958a97fde63e03983ec2394a4f8e408860fb42b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/back-to-top-sidebar.styl","hash":"4719ce717962663c5c33ef97b1119a0b3a4ecdc3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/back-to-top.styl","hash":"31050fc7a25784805b4843550151c93bfa55c9c8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/buttons.styl","hash":"7e509c7c28c59f905b847304dd3d14d94b6f3b8e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pagination.styl","hash":"c5d48863f332ff8ce7c88dec2c893f709d7331d3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/scaffolding/base.styl","hash":"18309b68ff33163a6f76a39437e618bb6ed411f8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/scaffolding/mobile.styl","hash":"47a46583a1f3731157a3f53f80ed1ed5e2753e8e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/scaffolding/scaffolding.styl","hash":"a280a583b7615e939aaddbf778f5c108ef8a2a6c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/scaffolding/tables.styl","hash":"0810e7c43d6c8adc8434a8fa66eabe0436ab8178","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Gemini/index.styl","hash":"f362fbc791dafb378807cabbc58abf03e097af6d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/_base.styl","hash":"0bef9f0dc134215bc4d0984ba3a16a1a0b6f87ec","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/_menu.styl","hash":"f43c821ea272f80703862260b140932fe4aa0e1f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/_posts-expanded.styl","hash":"2212511ae14258d93bec57993c0385e5ffbb382b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/index.styl","hash":"5e12572b18846250e016a872a738026478ceef37","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Muse/_menu.styl","hash":"35f093fe4c1861661ac1542d6e8ea5a9bbfeb659","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Muse/index.styl","hash":"d5e8ea6336bc2e237d501ed0d5bbcbbfe296c832","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/_layout.styl","hash":"ba1842dbeb97e46c6c4d2ae0e7a2ca6d610ada67","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/_menu.styl","hash":"05a5abf02e84ba8f639b6f9533418359f0ae4ecb","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/_sidebar.styl","hash":"41f9cdafa00e256561c50ae0b97ab7fcd7c1d6a2","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/_sub-menu.styl","hash":"ffa870c3fa37a48b01dc6f967e66f5df508d02bf","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Pisces/index.styl","hash":"5779cc8086b1cfde9bc4f1afdd85223bdc45f0a0","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/js/src/schemes/pisces.js","hash":"8050a5b2683d1d77238c5762b6bd89c543daed6e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/header/github-banner.styl","hash":"ee37e6c465b9b2a7e39175fccfcbed14f2db039b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/footer/footer.styl","hash":"39dee82d481dd9d44e33658960ec63e47cd0a715","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/header/header.styl","hash":"7cc3f36222494c9a1325c5347d7eb9ae53755a32","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/highlight/highlight.styl","hash":"17b95828f9db7f131ec0361a8c0e89b0b5c9bff5","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pages/archive.styl","hash":"f5aa2ba3bfffc15475e7e72a55b5c9d18609fdf5","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pages/breadcrumb.styl","hash":"7dd9a0378ccff3e4a2003f486b1a34e74c20dac6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pages/pages.styl","hash":"fb451dc4cc0355b57849c27d3eb110c73562f794","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-button.styl","hash":"e72a89e0f421444453e149ba32c77a64bd8e44e8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-collapse.styl","hash":"0f7f522cc6bfb3401d5afd62b0fcdf48bb2d604b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-expand.styl","hash":"ca89b167d368eac50a4f808fa53ba67e69cbef94","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-meta.styl","hash":"417f05ff12a2aaca6ceeac8b7e7eb26e9440c4c3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-reading_progress.styl","hash":"f4e9f870baa56eae423a123062f00e24cc780be1","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-reward.styl","hash":"36332c8a91f089f545f3c3e8ea90d08aa4d6e60c","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-rtl.styl","hash":"017074ef58166e2d69c53bb7590a0e7a8947a1ed","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-title.styl","hash":"c0ac49fadd33ca4a9a0a04d5ff2ac6560d0ecd9e","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post-widgets.styl","hash":"e4055a0d2cd2b0ad9dc55928e2f3e7bd4e499da3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/post/post.styl","hash":"8bf095377d28881f63a30bd7db97526829103bf2","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"35c0350096921dd8e2222ec41b6c17a4ea6b44f2","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-author.styl","hash":"bbe0d111f6451fc04e52719fd538bd0753ec17f9","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"89dd4f8b1f1cce3ad46cf2256038472712387d02","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"efa5e5022e205b52786ce495d4879f5e7b8f84b2","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"4427ed3250483ed5b7baad74fa93474bd1eda729","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"f7784aba0c1cd20d824c918c120012d57a5eaa2a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/sidebar.styl","hash":"43bc58daa8d35d5d515dc787ceb21dd77633fe49","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/full-image.styl","hash":"5d15cc8bbefe44c77a9b9f96bf04a6033a4b35b8","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/label.styl","hash":"4a457d265d62f287c63d48764ce45d9bcfc9ec5a","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/note-modern.styl","hash":"ee7528900578ef4753effe05b346381c40de5499","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/note.styl","hash":"32c9156bea5bac9e9ad0b4c08ffbca8b3d9aac4b","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/tabs.styl","hash":"4ab5deed8c3b0c338212380f678f8382672e1bcb","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/tags/tags.styl","hash":"ead0d0f2321dc71505788c7f689f92257cf14947","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/algolia-search.styl","hash":"10e9bb3392826a5a8f4cabfc14c6d81645f33fe6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/gitment.styl","hash":"34935b40237c074be5f5e8818c14ccfd802b7439","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/needsharebutton.styl","hash":"a5e3e6b4b4b814a9fe40b34d784fed67d6d977fa","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/related-posts.styl","hash":"76937db9702053d772f6758d9cea4088c2a6e2a3","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_common/components/third-party/third-party.styl","hash":"1c06be422bc41fd35e5c7948cdea2c09961207f6","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1526470537000},{"_id":"themes/hexo-theme-next-master/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1526470537000}],"Category":[{"name":"一些摸索","_id":"cjofrrc220004k8cyo1hakb2e"},{"name":"tensorflow","_id":"cjofrrc2l000ek8cygr1p5cta"},{"name":"paper","_id":"cjofrrc2n000jk8cyn81reohn"}],"Data":[],"Page":[{"title":"","date":"2018-05-27T01:54:16.000Z","comments":0,"_content":"![](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/41202540.jpg)\n\n<center>不停地学习，不断地成长。</center>","source":"about/index.md","raw":"---\ntitle: \ndate: 2018-05-27 09:54:16\ncomments: false\n---\n![](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/41202540.jpg)\n\n<center>不停地学习，不断地成长。</center>","updated":"2018-08-07T03:11:58.657Z","path":"about/index.html","layout":"page","_id":"cjofrrc1z0001k8cytnh1rtn5","content":"<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/41202540.jpg\" alt=\"\"></p>\n<center>不停地学习，不断地成长。</center>","site":{"data":{}},"excerpt":"","more":"<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/41202540.jpg\" alt=\"\"></p>\n<center>不停地学习，不断地成长。</center>"},{"title":"","date":"2018-05-27T01:52:04.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: \ndate: 2018-05-27 09:52:04\ntype: \"categories\"\ncomments: false\n---\n","updated":"2018-08-05T02:37:16.601Z","path":"categories/index.html","layout":"page","_id":"cjofrrc210003k8cybfzfw2id","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"","date":"2018-05-27T01:53:37.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle:\ndate: 2018-05-27 09:53:37\ntype: \"tags\"\ncomments: false\n---\n","updated":"2018-08-05T02:34:36.638Z","path":"tags/index.html","layout":"page","_id":"cjofrrc2d0007k8cyjk5417m8","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"title","date":"2018-05-20T05:15:09.000Z","_content":"","source":"title/index.md","raw":"---\ntitle: title\ndate: 2018-05-20 13:15:09\n---\n","updated":"2018-05-20T05:15:09.172Z","path":"title/index.html","comments":1,"layout":"page","_id":"cjofrrc2f0009k8cyov5yf09q","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Next主题的简单优化(一)","date":"2018-08-05T09:13:51.000Z","comments":1,"_content":"\n## 前言\n\n之前匆匆忙忙建站，没有加评论、搜索、数据统计与分析、搜索功能等等，这些功能对于搭建博客也是很重要的。参考了很多大佬的博客，受益匪浅，以下是我的一些摸索。\n\n实际上[Next主题的官方文档](https://theme-next.iissnan.com/getting-started.html)非常详细了，建议多查看。\n\nNext主题版本：Muse v6.3.0\n\n## 评论系统\n\n一开始按照[Next主题的官方文档](https://theme-next.iissnan.com/getting-started.html)配置[来必力](https://livere.com/)评论系统，但是后来发现来必力加载速度有点慢。于是转用基于[LeanCloud](https://leancloud.cn/dashboard/login.html#/signup)的评论系统Valine，Valine也是有[官方文档](https://valine.js.org/)的（看官方文档可是个好习惯）。\n\n<!--more-->\n\n**简要步骤如下：**\n\n**1.获取APP ID和APP Key**。首先在[LeanCloud](https://leancloud.cn/dashboard/login.html#/signup)注册自己的账号。进入[控制台](https://leancloud.cn/dashboard/applist.html#/apps)创建应用。应用创建好以后，进入刚创建的应用，选择`设置`>`应用Key`，就能看到`APP ID`和`APP Key`了：\n\n![img](https://ws1.sinaimg.cn/large/006qRazegy1fkwo6w2b6uj30xe0etjt4.jpg)\n\n**2.设置安全域名 :**\n\n![è®¾ç½®å®å¨åå](https://ws1.sinaimg.cn/large/006qRazegy1fkxqmddfh1j30qd0go40h.jpg)\n\n**3.修改`主题配置文件`中的Valine部分 :**\n\n（未开邮件提醒​​）\n\n文件位置：`themes/next/_config.yml`\n\n```yaml\n# Valine.\n# You can get your appid and appkey from https://leancloud.cn\n# more info please open https://valine.js.org\nvaline:\n  enable: true\n  appid: your APP ID\n  appkey: your Key\n  notify: false # mail notifier , https://github.com/xCss/Valine/wiki\n  verify: false # Verification code\n  placeholder: Just go go # comment box placeholder\n  avatar: monsterid # gravatar style\n  guest_info: nick,mail,link # custom comment header\n  pageSize: 10 # pagination size\n```\n\n**4.如需取消某个页面/文章 的评论，在 md 文件的 [front-matter ](https://hexo.io/docs/front-matter.html)中增加 `comments: false`。**\n\n## 数据统计与分析\n\n### 文章阅读量统计\n\n1.仍然使用LeanCloud。按下图创建`Class`，`Class`名称必须为`Counter`。\n\n![Selection_003](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/35529984.jpg)\n\n2.修改`主题配置文件`中的`leancloud_visitors`配置项：\n\n```yaml\nleancloud_visitors:\n  enable: true\n  app_id: \n  app_key: \n```\n\n### 博客访问量统计\n\n用的是`不蒜子统计`，修改`主题配置文件`中的`busuanzi_count`的配置项，当`enable: true`时，代表开启全局开关。\n\n```yaml\n# Show Views/Visitors of the website/page with busuanzi.\n# Get more information on http://ibruce.info/2015/04/04/busuanzi/\nbusuanzi_count:\n  enable: true\n  total_visitors: true\n  total_visitors_icon: user\n  total_views: true\n  total_views_icon: eye\n  post_views: false\n  post_views_icon: eye\n```\n\n## 博客图标\n\n网站的默认图标不是特别好看，因此换成了现在的小鱼。\n\n**修改方法：**\n\n1.到这个神奇的网站[EasyIcon](http://www.easyicon.net/)找心仪的图标，下载`32PX`和`16PX`的`ICO`格式，并把它们放在`/themes/next/source/images`里。\n\n\n\n![](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/95085113.jpg)\n\n2.修改`主题配置文件`中的`favicon`配置项，其中`small`对应`16px`的图标路径，`medium`对应`32px`的图标路径。\n\n```yaml\nfavicon:\n  small: /images/favicon-16x16.ico\n  medium: /images/favicon-32x32.ico\n  apple_touch_icon: /images/apple-touch-icon-next.png\n  safari_pinned_tab: /images/logo.svg\n  #android_manifest: /images/manifest.json\n  #ms_browserconfig: /images/browserconfig.xml\n```\n\n## 博客运行时间\n\n来源[reuixiy](https://reuixiy.github.io/)的[博客](https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#%E5%A5%BD%E7%8E%A9%E7%9A%84%E5%86%99%E4%BD%9C%E6%A0%B7%E5%BC%8F)。\n\n文件位置：`themes/next/layout/_custom/sidebar.swig`（其中的`BirthDay`改成自己的）\n\n```html\n<div id=\"days\"></div>\n<script>\nfunction show_date_time(){\nwindow.setTimeout(\"show_date_time()\", 1000);\nBirthDay=new Date(\"05/20/2018 15:13:14\");\ntoday=new Date();\ntimeold=(today.getTime()-BirthDay.getTime());\nsectimeold=timeold/1000\nsecondsold=Math.floor(sectimeold);\nmsPerDay=24*60*60*1000\ne_daysold=timeold/msPerDay\ndaysold=Math.floor(e_daysold);\ne_hrsold=(e_daysold-daysold)*24;\nhrsold=setzero(Math.floor(e_hrsold));\ne_minsold=(e_hrsold-hrsold)*60;\nminsold=setzero(Math.floor((e_hrsold-hrsold)*60));\nseconds=setzero(Math.floor((e_minsold-minsold)*60));\ndocument.getElementById('days').innerHTML=\"已运行\"+daysold+\"天\"+hrsold+\"小时\"+minsold+\"分\"+seconds+\"秒\";\n}\nfunction setzero(i){\nif (i<10)\n{i=\"0\" + i};\nreturn i;\n}\nshow_date_time();\n</script>\n```\n\n文件位置：`themes/next/layout/_macro/sidebar.swig`  (其中加上带加号的那句)\n\n```html\n {# Blogroll #}\n        {% if theme.links %}\n          <div class=\"links-of-blogroll motion-element {{ \"links-of-blogroll-\" + theme.links_layout | default('inline') }}\">\n            <div class=\"links-of-blogroll-title\">\n              <i class=\"fa  fa-fw fa-{{ theme.links_icon | default('globe') | lower }}\"></i>\n              {{ theme.links_title }}&nbsp;\n              <i class=\"fa  fa-fw fa-{{ theme.links_icon | default('globe') | lower }}\"></i>\n            </div>\n            <ul class=\"links-of-blogroll-list\">\n              {% for name, link in theme.links %}\n                <li class=\"links-of-blogroll-item\">\n                  <a href=\"{{ link }}\" title=\"{{ name }}\" target=\"_blank\">{{ name }}</a>\n                </li>\n              {% endfor %}\n            </ul>\n+        {% include '../_custom/sidebar.swig' %} \n          </div>\n         {% endif %}\n\n```\n\n## 搜索功能\n\n文件位置：`themes/next/_config.yml`\n\n```yaml\n# Local search\n# Dependencies: https://github.com/theme-next/hexo-generator-searchdb\nlocal_search:\n  enable: ture\n```\n\n安装插件\n\n~~~shell\n$ npm install hexo-generator-search --save\n~~~\n\n但是我在安装插件的时候一直报错\n\n~~~shell\nnpm ERR! path /home/song/hexo/test/node_modules/babylon\nnpm ERR! code ENOENT\nnpm ERR! errno -2\nnpm ERR! syscall access\nnpm ERR! enoent ENOENT: no such file or directory, access '/home/song/hexo/test/node_modules/babylon'\nnpm ERR! enoent This is related to npm not being able to find a file.\nnpm ERR! enoent \n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /home/song/.npm/_logs/2018-11-11T06_59_34_564Z-debug.log\n~~~\n\n[解决办法](https://blog.csdn.net/h416756139/article/details/50812109)：\n\n~~~shell\n$ npm install -g cnpm --registry=http://registry.npm.taobao.org\n$ cnpm install hexo-generator-search --save\n~~~\n\n\n\n## 预告\n\n1.关于更新主题\n\n2.关于如何推广博客\n\n3.评论邮件提醒\n","source":"_posts/Next主题优化.md","raw":"---\ntitle: Next主题的简单优化(一)\ndate: 2018-08-05 17:13:51\ntags: hexo\ncomments: true\ncategories: 一些摸索\n---\n\n## 前言\n\n之前匆匆忙忙建站，没有加评论、搜索、数据统计与分析、搜索功能等等，这些功能对于搭建博客也是很重要的。参考了很多大佬的博客，受益匪浅，以下是我的一些摸索。\n\n实际上[Next主题的官方文档](https://theme-next.iissnan.com/getting-started.html)非常详细了，建议多查看。\n\nNext主题版本：Muse v6.3.0\n\n## 评论系统\n\n一开始按照[Next主题的官方文档](https://theme-next.iissnan.com/getting-started.html)配置[来必力](https://livere.com/)评论系统，但是后来发现来必力加载速度有点慢。于是转用基于[LeanCloud](https://leancloud.cn/dashboard/login.html#/signup)的评论系统Valine，Valine也是有[官方文档](https://valine.js.org/)的（看官方文档可是个好习惯）。\n\n<!--more-->\n\n**简要步骤如下：**\n\n**1.获取APP ID和APP Key**。首先在[LeanCloud](https://leancloud.cn/dashboard/login.html#/signup)注册自己的账号。进入[控制台](https://leancloud.cn/dashboard/applist.html#/apps)创建应用。应用创建好以后，进入刚创建的应用，选择`设置`>`应用Key`，就能看到`APP ID`和`APP Key`了：\n\n![img](https://ws1.sinaimg.cn/large/006qRazegy1fkwo6w2b6uj30xe0etjt4.jpg)\n\n**2.设置安全域名 :**\n\n![è®¾ç½®å®å¨åå](https://ws1.sinaimg.cn/large/006qRazegy1fkxqmddfh1j30qd0go40h.jpg)\n\n**3.修改`主题配置文件`中的Valine部分 :**\n\n（未开邮件提醒​​）\n\n文件位置：`themes/next/_config.yml`\n\n```yaml\n# Valine.\n# You can get your appid and appkey from https://leancloud.cn\n# more info please open https://valine.js.org\nvaline:\n  enable: true\n  appid: your APP ID\n  appkey: your Key\n  notify: false # mail notifier , https://github.com/xCss/Valine/wiki\n  verify: false # Verification code\n  placeholder: Just go go # comment box placeholder\n  avatar: monsterid # gravatar style\n  guest_info: nick,mail,link # custom comment header\n  pageSize: 10 # pagination size\n```\n\n**4.如需取消某个页面/文章 的评论，在 md 文件的 [front-matter ](https://hexo.io/docs/front-matter.html)中增加 `comments: false`。**\n\n## 数据统计与分析\n\n### 文章阅读量统计\n\n1.仍然使用LeanCloud。按下图创建`Class`，`Class`名称必须为`Counter`。\n\n![Selection_003](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/35529984.jpg)\n\n2.修改`主题配置文件`中的`leancloud_visitors`配置项：\n\n```yaml\nleancloud_visitors:\n  enable: true\n  app_id: \n  app_key: \n```\n\n### 博客访问量统计\n\n用的是`不蒜子统计`，修改`主题配置文件`中的`busuanzi_count`的配置项，当`enable: true`时，代表开启全局开关。\n\n```yaml\n# Show Views/Visitors of the website/page with busuanzi.\n# Get more information on http://ibruce.info/2015/04/04/busuanzi/\nbusuanzi_count:\n  enable: true\n  total_visitors: true\n  total_visitors_icon: user\n  total_views: true\n  total_views_icon: eye\n  post_views: false\n  post_views_icon: eye\n```\n\n## 博客图标\n\n网站的默认图标不是特别好看，因此换成了现在的小鱼。\n\n**修改方法：**\n\n1.到这个神奇的网站[EasyIcon](http://www.easyicon.net/)找心仪的图标，下载`32PX`和`16PX`的`ICO`格式，并把它们放在`/themes/next/source/images`里。\n\n\n\n![](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/95085113.jpg)\n\n2.修改`主题配置文件`中的`favicon`配置项，其中`small`对应`16px`的图标路径，`medium`对应`32px`的图标路径。\n\n```yaml\nfavicon:\n  small: /images/favicon-16x16.ico\n  medium: /images/favicon-32x32.ico\n  apple_touch_icon: /images/apple-touch-icon-next.png\n  safari_pinned_tab: /images/logo.svg\n  #android_manifest: /images/manifest.json\n  #ms_browserconfig: /images/browserconfig.xml\n```\n\n## 博客运行时间\n\n来源[reuixiy](https://reuixiy.github.io/)的[博客](https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#%E5%A5%BD%E7%8E%A9%E7%9A%84%E5%86%99%E4%BD%9C%E6%A0%B7%E5%BC%8F)。\n\n文件位置：`themes/next/layout/_custom/sidebar.swig`（其中的`BirthDay`改成自己的）\n\n```html\n<div id=\"days\"></div>\n<script>\nfunction show_date_time(){\nwindow.setTimeout(\"show_date_time()\", 1000);\nBirthDay=new Date(\"05/20/2018 15:13:14\");\ntoday=new Date();\ntimeold=(today.getTime()-BirthDay.getTime());\nsectimeold=timeold/1000\nsecondsold=Math.floor(sectimeold);\nmsPerDay=24*60*60*1000\ne_daysold=timeold/msPerDay\ndaysold=Math.floor(e_daysold);\ne_hrsold=(e_daysold-daysold)*24;\nhrsold=setzero(Math.floor(e_hrsold));\ne_minsold=(e_hrsold-hrsold)*60;\nminsold=setzero(Math.floor((e_hrsold-hrsold)*60));\nseconds=setzero(Math.floor((e_minsold-minsold)*60));\ndocument.getElementById('days').innerHTML=\"已运行\"+daysold+\"天\"+hrsold+\"小时\"+minsold+\"分\"+seconds+\"秒\";\n}\nfunction setzero(i){\nif (i<10)\n{i=\"0\" + i};\nreturn i;\n}\nshow_date_time();\n</script>\n```\n\n文件位置：`themes/next/layout/_macro/sidebar.swig`  (其中加上带加号的那句)\n\n```html\n {# Blogroll #}\n        {% if theme.links %}\n          <div class=\"links-of-blogroll motion-element {{ \"links-of-blogroll-\" + theme.links_layout | default('inline') }}\">\n            <div class=\"links-of-blogroll-title\">\n              <i class=\"fa  fa-fw fa-{{ theme.links_icon | default('globe') | lower }}\"></i>\n              {{ theme.links_title }}&nbsp;\n              <i class=\"fa  fa-fw fa-{{ theme.links_icon | default('globe') | lower }}\"></i>\n            </div>\n            <ul class=\"links-of-blogroll-list\">\n              {% for name, link in theme.links %}\n                <li class=\"links-of-blogroll-item\">\n                  <a href=\"{{ link }}\" title=\"{{ name }}\" target=\"_blank\">{{ name }}</a>\n                </li>\n              {% endfor %}\n            </ul>\n+        {% include '../_custom/sidebar.swig' %} \n          </div>\n         {% endif %}\n\n```\n\n## 搜索功能\n\n文件位置：`themes/next/_config.yml`\n\n```yaml\n# Local search\n# Dependencies: https://github.com/theme-next/hexo-generator-searchdb\nlocal_search:\n  enable: ture\n```\n\n安装插件\n\n~~~shell\n$ npm install hexo-generator-search --save\n~~~\n\n但是我在安装插件的时候一直报错\n\n~~~shell\nnpm ERR! path /home/song/hexo/test/node_modules/babylon\nnpm ERR! code ENOENT\nnpm ERR! errno -2\nnpm ERR! syscall access\nnpm ERR! enoent ENOENT: no such file or directory, access '/home/song/hexo/test/node_modules/babylon'\nnpm ERR! enoent This is related to npm not being able to find a file.\nnpm ERR! enoent \n\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     /home/song/.npm/_logs/2018-11-11T06_59_34_564Z-debug.log\n~~~\n\n[解决办法](https://blog.csdn.net/h416756139/article/details/50812109)：\n\n~~~shell\n$ npm install -g cnpm --registry=http://registry.npm.taobao.org\n$ cnpm install hexo-generator-search --save\n~~~\n\n\n\n## 预告\n\n1.关于更新主题\n\n2.关于如何推广博客\n\n3.评论邮件提醒\n","slug":"Next主题优化","published":1,"updated":"2018-11-11T07:18:09.146Z","layout":"post","photos":[],"link":"","_id":"cjofrrc1s0000k8cynaaj2fft","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>之前匆匆忙忙建站，没有加评论、搜索、数据统计与分析、搜索功能等等，这些功能对于搭建博客也是很重要的。参考了很多大佬的博客，受益匪浅，以下是我的一些摸索。</p>\n<p>实际上<a href=\"https://theme-next.iissnan.com/getting-started.html\" target=\"_blank\" rel=\"noopener\">Next主题的官方文档</a>非常详细了，建议多查看。</p>\n<p>Next主题版本：Muse v6.3.0</p>\n<h2 id=\"评论系统\"><a href=\"#评论系统\" class=\"headerlink\" title=\"评论系统\"></a>评论系统</h2><p>一开始按照<a href=\"https://theme-next.iissnan.com/getting-started.html\" target=\"_blank\" rel=\"noopener\">Next主题的官方文档</a>配置<a href=\"https://livere.com/\" target=\"_blank\" rel=\"noopener\">来必力</a>评论系统，但是后来发现来必力加载速度有点慢。于是转用基于<a href=\"https://leancloud.cn/dashboard/login.html#/signup\" target=\"_blank\" rel=\"noopener\">LeanCloud</a>的评论系统Valine，Valine也是有<a href=\"https://valine.js.org/\" target=\"_blank\" rel=\"noopener\">官方文档</a>的（看官方文档可是个好习惯）。</p>\n<a id=\"more\"></a>\n<p><strong>简要步骤如下：</strong></p>\n<p><strong>1.获取APP ID和APP Key</strong>。首先在<a href=\"https://leancloud.cn/dashboard/login.html#/signup\" target=\"_blank\" rel=\"noopener\">LeanCloud</a>注册自己的账号。进入<a href=\"https://leancloud.cn/dashboard/applist.html#/apps\" target=\"_blank\" rel=\"noopener\">控制台</a>创建应用。应用创建好以后，进入刚创建的应用，选择<code>设置</code>&gt;<code>应用Key</code>，就能看到<code>APP ID</code>和<code>APP Key</code>了：</p>\n<p><img src=\"https://ws1.sinaimg.cn/large/006qRazegy1fkwo6w2b6uj30xe0etjt4.jpg\" alt=\"img\"></p>\n<p><strong>2.设置安全域名 :</strong></p>\n<p><img src=\"https://ws1.sinaimg.cn/large/006qRazegy1fkxqmddfh1j30qd0go40h.jpg\" alt=\"è®¾ç½®å®å¨åå\"></p>\n<p><strong>3.修改<code>主题配置文件</code>中的Valine部分 :</strong></p>\n<p>（未开邮件提醒​​）</p>\n<p>文件位置：<code>themes/next/_config.yml</code></p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Valine.</span></span><br><span class=\"line\"><span class=\"comment\"># You can get your appid and appkey from https://leancloud.cn</span></span><br><span class=\"line\"><span class=\"comment\"># more info please open https://valine.js.org</span></span><br><span class=\"line\"><span class=\"attr\">valine:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  appid:</span> <span class=\"string\">your</span> <span class=\"string\">APP</span> <span class=\"string\">ID</span></span><br><span class=\"line\"><span class=\"attr\">  appkey:</span> <span class=\"string\">your</span> <span class=\"string\">Key</span></span><br><span class=\"line\"><span class=\"attr\">  notify:</span> <span class=\"literal\">false</span> <span class=\"comment\"># mail notifier , https://github.com/xCss/Valine/wiki</span></span><br><span class=\"line\"><span class=\"attr\">  verify:</span> <span class=\"literal\">false</span> <span class=\"comment\"># Verification code</span></span><br><span class=\"line\"><span class=\"attr\">  placeholder:</span> <span class=\"string\">Just</span> <span class=\"string\">go</span> <span class=\"string\">go</span> <span class=\"comment\"># comment box placeholder</span></span><br><span class=\"line\"><span class=\"attr\">  avatar:</span> <span class=\"string\">monsterid</span> <span class=\"comment\"># gravatar style</span></span><br><span class=\"line\"><span class=\"attr\">  guest_info:</span> <span class=\"string\">nick,mail,link</span> <span class=\"comment\"># custom comment header</span></span><br><span class=\"line\"><span class=\"attr\">  pageSize:</span> <span class=\"number\">10</span> <span class=\"comment\"># pagination size</span></span><br></pre></td></tr></table></figure>\n<p><strong>4.如需取消某个页面/文章 的评论，在 md 文件的 <a href=\"https://hexo.io/docs/front-matter.html\" target=\"_blank\" rel=\"noopener\">front-matter </a>中增加 <code>comments: false</code>。</strong></p>\n<h2 id=\"数据统计与分析\"><a href=\"#数据统计与分析\" class=\"headerlink\" title=\"数据统计与分析\"></a>数据统计与分析</h2><h3 id=\"文章阅读量统计\"><a href=\"#文章阅读量统计\" class=\"headerlink\" title=\"文章阅读量统计\"></a>文章阅读量统计</h3><p>1.仍然使用LeanCloud。按下图创建<code>Class</code>，<code>Class</code>名称必须为<code>Counter</code>。</p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/35529984.jpg\" alt=\"Selection_003\"></p>\n<p>2.修改<code>主题配置文件</code>中的<code>leancloud_visitors</code>配置项：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">leancloud_visitors:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  app_id:</span> </span><br><span class=\"line\"><span class=\"attr\">  app_key:</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"博客访问量统计\"><a href=\"#博客访问量统计\" class=\"headerlink\" title=\"博客访问量统计\"></a>博客访问量统计</h3><p>用的是<code>不蒜子统计</code>，修改<code>主题配置文件</code>中的<code>busuanzi_count</code>的配置项，当<code>enable: true</code>时，代表开启全局开关。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Show Views/Visitors of the website/page with busuanzi.</span></span><br><span class=\"line\"><span class=\"comment\"># Get more information on http://ibruce.info/2015/04/04/busuanzi/</span></span><br><span class=\"line\"><span class=\"attr\">busuanzi_count:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  total_visitors:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  total_visitors_icon:</span> <span class=\"string\">user</span></span><br><span class=\"line\"><span class=\"attr\">  total_views:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  total_views_icon:</span> <span class=\"string\">eye</span></span><br><span class=\"line\"><span class=\"attr\">  post_views:</span> <span class=\"literal\">false</span></span><br><span class=\"line\"><span class=\"attr\">  post_views_icon:</span> <span class=\"string\">eye</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"博客图标\"><a href=\"#博客图标\" class=\"headerlink\" title=\"博客图标\"></a>博客图标</h2><p>网站的默认图标不是特别好看，因此换成了现在的小鱼。</p>\n<p><strong>修改方法：</strong></p>\n<p>1.到这个神奇的网站<a href=\"http://www.easyicon.net/\" target=\"_blank\" rel=\"noopener\">EasyIcon</a>找心仪的图标，下载<code>32PX</code>和<code>16PX</code>的<code>ICO</code>格式，并把它们放在<code>/themes/next/source/images</code>里。</p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/95085113.jpg\" alt=\"\"></p>\n<p>2.修改<code>主题配置文件</code>中的<code>favicon</code>配置项，其中<code>small</code>对应<code>16px</code>的图标路径，<code>medium</code>对应<code>32px</code>的图标路径。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">favicon:</span></span><br><span class=\"line\"><span class=\"attr\">  small:</span> <span class=\"string\">/images/favicon-16x16.ico</span></span><br><span class=\"line\"><span class=\"attr\">  medium:</span> <span class=\"string\">/images/favicon-32x32.ico</span></span><br><span class=\"line\"><span class=\"attr\">  apple_touch_icon:</span> <span class=\"string\">/images/apple-touch-icon-next.png</span></span><br><span class=\"line\"><span class=\"attr\">  safari_pinned_tab:</span> <span class=\"string\">/images/logo.svg</span></span><br><span class=\"line\">  <span class=\"comment\">#android_manifest: /images/manifest.json</span></span><br><span class=\"line\">  <span class=\"comment\">#ms_browserconfig: /images/browserconfig.xml</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"博客运行时间\"><a href=\"#博客运行时间\" class=\"headerlink\" title=\"博客运行时间\"></a>博客运行时间</h2><p>来源<a href=\"https://reuixiy.github.io/\" target=\"_blank\" rel=\"noopener\">reuixiy</a>的<a href=\"https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#%E5%A5%BD%E7%8E%A9%E7%9A%84%E5%86%99%E4%BD%9C%E6%A0%B7%E5%BC%8F\" target=\"_blank\" rel=\"noopener\">博客</a>。</p>\n<p>文件位置：<code>themes/next/layout/_custom/sidebar.swig</code>（其中的<code>BirthDay</code>改成自己的）</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">\"days\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span>&gt;</span><span class=\"undefined\"></span></span><br><span class=\"line\"><span class=\"undefined\">function show_date_time()&#123;</span></span><br><span class=\"line\"><span class=\"undefined\">window.setTimeout(\"show_date_time()\", 1000);</span></span><br><span class=\"line\"><span class=\"undefined\">BirthDay=new Date(\"05/20/2018 15:13:14\");</span></span><br><span class=\"line\"><span class=\"undefined\">today=new Date();</span></span><br><span class=\"line\"><span class=\"undefined\">timeold=(today.getTime()-BirthDay.getTime());</span></span><br><span class=\"line\"><span class=\"undefined\">sectimeold=timeold/1000</span></span><br><span class=\"line\"><span class=\"undefined\">secondsold=Math.floor(sectimeold);</span></span><br><span class=\"line\"><span class=\"undefined\">msPerDay=24*60*60*1000</span></span><br><span class=\"line\"><span class=\"undefined\">e_daysold=timeold/msPerDay</span></span><br><span class=\"line\"><span class=\"undefined\">daysold=Math.floor(e_daysold);</span></span><br><span class=\"line\"><span class=\"undefined\">e_hrsold=(e_daysold-daysold)*24;</span></span><br><span class=\"line\"><span class=\"undefined\">hrsold=setzero(Math.floor(e_hrsold));</span></span><br><span class=\"line\"><span class=\"undefined\">e_minsold=(e_hrsold-hrsold)*60;</span></span><br><span class=\"line\"><span class=\"undefined\">minsold=setzero(Math.floor((e_hrsold-hrsold)*60));</span></span><br><span class=\"line\"><span class=\"undefined\">seconds=setzero(Math.floor((e_minsold-minsold)*60));</span></span><br><span class=\"line\"><span class=\"undefined\">document.getElementById('days').innerHTML=\"已运行\"+daysold+\"天\"+hrsold+\"小时\"+minsold+\"分\"+seconds+\"秒\";</span></span><br><span class=\"line\"><span class=\"undefined\">&#125;</span></span><br><span class=\"line\"><span class=\"undefined\">function setzero(i)&#123;</span></span><br><span class=\"line\"><span class=\"undefined\">if (i&lt;10)</span></span><br><span class=\"line\"><span class=\"undefined\">&#123;i=\"0\" + i&#125;;</span></span><br><span class=\"line\"><span class=\"undefined\">return i;</span></span><br><span class=\"line\"><span class=\"undefined\">&#125;</span></span><br><span class=\"line\"><span class=\"undefined\">show_date_time();</span></span><br><span class=\"line\"><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>文件位置：<code>themes/next/layout/_macro/sidebar.swig</code>  (其中加上带加号的那句)</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &#123;# Blogroll #&#125;</span><br><span class=\"line\">        &#123;% if theme.links %&#125;</span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll motion-element &#123;&#123; \"</span><span class=\"attr\">links-of-blogroll-</span>\" + <span class=\"attr\">theme.links_layout</span> | <span class=\"attr\">default</span>('<span class=\"attr\">inline</span>') &#125;&#125;\"&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll-title\"</span>&gt;</span></span><br><span class=\"line\">              <span class=\"tag\">&lt;<span class=\"name\">i</span> <span class=\"attr\">class</span>=<span class=\"string\">\"fa  fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">i</span>&gt;</span></span><br><span class=\"line\">              &#123;&#123; theme.links_title &#125;&#125;&amp;nbsp;</span><br><span class=\"line\">              <span class=\"tag\">&lt;<span class=\"name\">i</span> <span class=\"attr\">class</span>=<span class=\"string\">\"fa  fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">i</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ul</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll-list\"</span>&gt;</span></span><br><span class=\"line\">              &#123;% for name, link in theme.links %&#125;</span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">li</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll-item\"</span>&gt;</span></span><br><span class=\"line\">                  <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"&#123;&#123; link &#125;&#125;\"</span> <span class=\"attr\">title</span>=<span class=\"string\">\"&#123;&#123; name &#125;&#125;\"</span> <span class=\"attr\">target</span>=<span class=\"string\">\"_blank\"</span>&gt;</span>&#123;&#123; name &#125;&#125;<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">              &#123;% endfor %&#125;</span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">ul</span>&gt;</span></span><br><span class=\"line\">+        &#123;% include '../_custom/sidebar.swig' %&#125; </span><br><span class=\"line\">          <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">         &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"搜索功能\"><a href=\"#搜索功能\" class=\"headerlink\" title=\"搜索功能\"></a>搜索功能</h2><p>文件位置：<code>themes/next/_config.yml</code></p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Local search</span></span><br><span class=\"line\"><span class=\"comment\"># Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span></span><br><span class=\"line\"><span class=\"attr\">local_search:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"string\">ture</span></span><br></pre></td></tr></table></figure>\n<p>安装插件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure>\n<p>但是我在安装插件的时候一直报错</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm ERR! path /home/song/hexo/test/node_modules/babylon</span><br><span class=\"line\">npm ERR! code ENOENT</span><br><span class=\"line\">npm ERR! errno -2</span><br><span class=\"line\">npm ERR! syscall access</span><br><span class=\"line\">npm ERR! enoent ENOENT: no such file or directory, access '/home/song/hexo/test/node_modules/babylon'</span><br><span class=\"line\">npm ERR! enoent This is related to npm not being able to find a file.</span><br><span class=\"line\">npm ERR! enoent </span><br><span class=\"line\"></span><br><span class=\"line\">npm ERR! A complete log of this run can be found in:</span><br><span class=\"line\">npm ERR!     /home/song/.npm/_logs/2018-11-11T06_59_34_564Z-debug.log</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://blog.csdn.net/h416756139/article/details/50812109\" target=\"_blank\" rel=\"noopener\">解决办法</a>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> npm install -g cnpm --registry=http://registry.npm.taobao.org</span><br><span class=\"line\"><span class=\"meta\">$</span> cnpm install hexo-generator-search --save</span><br></pre></td></tr></table></figure>\n<h2 id=\"预告\"><a href=\"#预告\" class=\"headerlink\" title=\"预告\"></a>预告</h2><p>1.关于更新主题</p>\n<p>2.关于如何推广博客</p>\n<p>3.评论邮件提醒</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>之前匆匆忙忙建站，没有加评论、搜索、数据统计与分析、搜索功能等等，这些功能对于搭建博客也是很重要的。参考了很多大佬的博客，受益匪浅，以下是我的一些摸索。</p>\n<p>实际上<a href=\"https://theme-next.iissnan.com/getting-started.html\" target=\"_blank\" rel=\"noopener\">Next主题的官方文档</a>非常详细了，建议多查看。</p>\n<p>Next主题版本：Muse v6.3.0</p>\n<h2 id=\"评论系统\"><a href=\"#评论系统\" class=\"headerlink\" title=\"评论系统\"></a>评论系统</h2><p>一开始按照<a href=\"https://theme-next.iissnan.com/getting-started.html\" target=\"_blank\" rel=\"noopener\">Next主题的官方文档</a>配置<a href=\"https://livere.com/\" target=\"_blank\" rel=\"noopener\">来必力</a>评论系统，但是后来发现来必力加载速度有点慢。于是转用基于<a href=\"https://leancloud.cn/dashboard/login.html#/signup\" target=\"_blank\" rel=\"noopener\">LeanCloud</a>的评论系统Valine，Valine也是有<a href=\"https://valine.js.org/\" target=\"_blank\" rel=\"noopener\">官方文档</a>的（看官方文档可是个好习惯）。</p>","more":"<p><strong>简要步骤如下：</strong></p>\n<p><strong>1.获取APP ID和APP Key</strong>。首先在<a href=\"https://leancloud.cn/dashboard/login.html#/signup\" target=\"_blank\" rel=\"noopener\">LeanCloud</a>注册自己的账号。进入<a href=\"https://leancloud.cn/dashboard/applist.html#/apps\" target=\"_blank\" rel=\"noopener\">控制台</a>创建应用。应用创建好以后，进入刚创建的应用，选择<code>设置</code>&gt;<code>应用Key</code>，就能看到<code>APP ID</code>和<code>APP Key</code>了：</p>\n<p><img src=\"https://ws1.sinaimg.cn/large/006qRazegy1fkwo6w2b6uj30xe0etjt4.jpg\" alt=\"img\"></p>\n<p><strong>2.设置安全域名 :</strong></p>\n<p><img src=\"https://ws1.sinaimg.cn/large/006qRazegy1fkxqmddfh1j30qd0go40h.jpg\" alt=\"è®¾ç½®å®å¨åå\"></p>\n<p><strong>3.修改<code>主题配置文件</code>中的Valine部分 :</strong></p>\n<p>（未开邮件提醒​​）</p>\n<p>文件位置：<code>themes/next/_config.yml</code></p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Valine.</span></span><br><span class=\"line\"><span class=\"comment\"># You can get your appid and appkey from https://leancloud.cn</span></span><br><span class=\"line\"><span class=\"comment\"># more info please open https://valine.js.org</span></span><br><span class=\"line\"><span class=\"attr\">valine:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  appid:</span> <span class=\"string\">your</span> <span class=\"string\">APP</span> <span class=\"string\">ID</span></span><br><span class=\"line\"><span class=\"attr\">  appkey:</span> <span class=\"string\">your</span> <span class=\"string\">Key</span></span><br><span class=\"line\"><span class=\"attr\">  notify:</span> <span class=\"literal\">false</span> <span class=\"comment\"># mail notifier , https://github.com/xCss/Valine/wiki</span></span><br><span class=\"line\"><span class=\"attr\">  verify:</span> <span class=\"literal\">false</span> <span class=\"comment\"># Verification code</span></span><br><span class=\"line\"><span class=\"attr\">  placeholder:</span> <span class=\"string\">Just</span> <span class=\"string\">go</span> <span class=\"string\">go</span> <span class=\"comment\"># comment box placeholder</span></span><br><span class=\"line\"><span class=\"attr\">  avatar:</span> <span class=\"string\">monsterid</span> <span class=\"comment\"># gravatar style</span></span><br><span class=\"line\"><span class=\"attr\">  guest_info:</span> <span class=\"string\">nick,mail,link</span> <span class=\"comment\"># custom comment header</span></span><br><span class=\"line\"><span class=\"attr\">  pageSize:</span> <span class=\"number\">10</span> <span class=\"comment\"># pagination size</span></span><br></pre></td></tr></table></figure>\n<p><strong>4.如需取消某个页面/文章 的评论，在 md 文件的 <a href=\"https://hexo.io/docs/front-matter.html\" target=\"_blank\" rel=\"noopener\">front-matter </a>中增加 <code>comments: false</code>。</strong></p>\n<h2 id=\"数据统计与分析\"><a href=\"#数据统计与分析\" class=\"headerlink\" title=\"数据统计与分析\"></a>数据统计与分析</h2><h3 id=\"文章阅读量统计\"><a href=\"#文章阅读量统计\" class=\"headerlink\" title=\"文章阅读量统计\"></a>文章阅读量统计</h3><p>1.仍然使用LeanCloud。按下图创建<code>Class</code>，<code>Class</code>名称必须为<code>Counter</code>。</p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/35529984.jpg\" alt=\"Selection_003\"></p>\n<p>2.修改<code>主题配置文件</code>中的<code>leancloud_visitors</code>配置项：</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">leancloud_visitors:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  app_id:</span> </span><br><span class=\"line\"><span class=\"attr\">  app_key:</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"博客访问量统计\"><a href=\"#博客访问量统计\" class=\"headerlink\" title=\"博客访问量统计\"></a>博客访问量统计</h3><p>用的是<code>不蒜子统计</code>，修改<code>主题配置文件</code>中的<code>busuanzi_count</code>的配置项，当<code>enable: true</code>时，代表开启全局开关。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Show Views/Visitors of the website/page with busuanzi.</span></span><br><span class=\"line\"><span class=\"comment\"># Get more information on http://ibruce.info/2015/04/04/busuanzi/</span></span><br><span class=\"line\"><span class=\"attr\">busuanzi_count:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  total_visitors:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  total_visitors_icon:</span> <span class=\"string\">user</span></span><br><span class=\"line\"><span class=\"attr\">  total_views:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  total_views_icon:</span> <span class=\"string\">eye</span></span><br><span class=\"line\"><span class=\"attr\">  post_views:</span> <span class=\"literal\">false</span></span><br><span class=\"line\"><span class=\"attr\">  post_views_icon:</span> <span class=\"string\">eye</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"博客图标\"><a href=\"#博客图标\" class=\"headerlink\" title=\"博客图标\"></a>博客图标</h2><p>网站的默认图标不是特别好看，因此换成了现在的小鱼。</p>\n<p><strong>修改方法：</strong></p>\n<p>1.到这个神奇的网站<a href=\"http://www.easyicon.net/\" target=\"_blank\" rel=\"noopener\">EasyIcon</a>找心仪的图标，下载<code>32PX</code>和<code>16PX</code>的<code>ICO</code>格式，并把它们放在<code>/themes/next/source/images</code>里。</p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-8-7/95085113.jpg\" alt=\"\"></p>\n<p>2.修改<code>主题配置文件</code>中的<code>favicon</code>配置项，其中<code>small</code>对应<code>16px</code>的图标路径，<code>medium</code>对应<code>32px</code>的图标路径。</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">favicon:</span></span><br><span class=\"line\"><span class=\"attr\">  small:</span> <span class=\"string\">/images/favicon-16x16.ico</span></span><br><span class=\"line\"><span class=\"attr\">  medium:</span> <span class=\"string\">/images/favicon-32x32.ico</span></span><br><span class=\"line\"><span class=\"attr\">  apple_touch_icon:</span> <span class=\"string\">/images/apple-touch-icon-next.png</span></span><br><span class=\"line\"><span class=\"attr\">  safari_pinned_tab:</span> <span class=\"string\">/images/logo.svg</span></span><br><span class=\"line\">  <span class=\"comment\">#android_manifest: /images/manifest.json</span></span><br><span class=\"line\">  <span class=\"comment\">#ms_browserconfig: /images/browserconfig.xml</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"博客运行时间\"><a href=\"#博客运行时间\" class=\"headerlink\" title=\"博客运行时间\"></a>博客运行时间</h2><p>来源<a href=\"https://reuixiy.github.io/\" target=\"_blank\" rel=\"noopener\">reuixiy</a>的<a href=\"https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html#%E5%A5%BD%E7%8E%A9%E7%9A%84%E5%86%99%E4%BD%9C%E6%A0%B7%E5%BC%8F\" target=\"_blank\" rel=\"noopener\">博客</a>。</p>\n<p>文件位置：<code>themes/next/layout/_custom/sidebar.swig</code>（其中的<code>BirthDay</code>改成自己的）</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">id</span>=<span class=\"string\">\"days\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span>&gt;</span><span class=\"undefined\"></span></span><br><span class=\"line\"><span class=\"undefined\">function show_date_time()&#123;</span></span><br><span class=\"line\"><span class=\"undefined\">window.setTimeout(\"show_date_time()\", 1000);</span></span><br><span class=\"line\"><span class=\"undefined\">BirthDay=new Date(\"05/20/2018 15:13:14\");</span></span><br><span class=\"line\"><span class=\"undefined\">today=new Date();</span></span><br><span class=\"line\"><span class=\"undefined\">timeold=(today.getTime()-BirthDay.getTime());</span></span><br><span class=\"line\"><span class=\"undefined\">sectimeold=timeold/1000</span></span><br><span class=\"line\"><span class=\"undefined\">secondsold=Math.floor(sectimeold);</span></span><br><span class=\"line\"><span class=\"undefined\">msPerDay=24*60*60*1000</span></span><br><span class=\"line\"><span class=\"undefined\">e_daysold=timeold/msPerDay</span></span><br><span class=\"line\"><span class=\"undefined\">daysold=Math.floor(e_daysold);</span></span><br><span class=\"line\"><span class=\"undefined\">e_hrsold=(e_daysold-daysold)*24;</span></span><br><span class=\"line\"><span class=\"undefined\">hrsold=setzero(Math.floor(e_hrsold));</span></span><br><span class=\"line\"><span class=\"undefined\">e_minsold=(e_hrsold-hrsold)*60;</span></span><br><span class=\"line\"><span class=\"undefined\">minsold=setzero(Math.floor((e_hrsold-hrsold)*60));</span></span><br><span class=\"line\"><span class=\"undefined\">seconds=setzero(Math.floor((e_minsold-minsold)*60));</span></span><br><span class=\"line\"><span class=\"undefined\">document.getElementById('days').innerHTML=\"已运行\"+daysold+\"天\"+hrsold+\"小时\"+minsold+\"分\"+seconds+\"秒\";</span></span><br><span class=\"line\"><span class=\"undefined\">&#125;</span></span><br><span class=\"line\"><span class=\"undefined\">function setzero(i)&#123;</span></span><br><span class=\"line\"><span class=\"undefined\">if (i&lt;10)</span></span><br><span class=\"line\"><span class=\"undefined\">&#123;i=\"0\" + i&#125;;</span></span><br><span class=\"line\"><span class=\"undefined\">return i;</span></span><br><span class=\"line\"><span class=\"undefined\">&#125;</span></span><br><span class=\"line\"><span class=\"undefined\">show_date_time();</span></span><br><span class=\"line\"><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>文件位置：<code>themes/next/layout/_macro/sidebar.swig</code>  (其中加上带加号的那句)</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"> &#123;# Blogroll #&#125;</span><br><span class=\"line\">        &#123;% if theme.links %&#125;</span><br><span class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll motion-element &#123;&#123; \"</span><span class=\"attr\">links-of-blogroll-</span>\" + <span class=\"attr\">theme.links_layout</span> | <span class=\"attr\">default</span>('<span class=\"attr\">inline</span>') &#125;&#125;\"&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll-title\"</span>&gt;</span></span><br><span class=\"line\">              <span class=\"tag\">&lt;<span class=\"name\">i</span> <span class=\"attr\">class</span>=<span class=\"string\">\"fa  fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">i</span>&gt;</span></span><br><span class=\"line\">              &#123;&#123; theme.links_title &#125;&#125;&amp;nbsp;</span><br><span class=\"line\">              <span class=\"tag\">&lt;<span class=\"name\">i</span> <span class=\"attr\">class</span>=<span class=\"string\">\"fa  fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;\"</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">i</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">ul</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll-list\"</span>&gt;</span></span><br><span class=\"line\">              &#123;% for name, link in theme.links %&#125;</span><br><span class=\"line\">                <span class=\"tag\">&lt;<span class=\"name\">li</span> <span class=\"attr\">class</span>=<span class=\"string\">\"links-of-blogroll-item\"</span>&gt;</span></span><br><span class=\"line\">                  <span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">\"&#123;&#123; link &#125;&#125;\"</span> <span class=\"attr\">title</span>=<span class=\"string\">\"&#123;&#123; name &#125;&#125;\"</span> <span class=\"attr\">target</span>=<span class=\"string\">\"_blank\"</span>&gt;</span>&#123;&#123; name &#125;&#125;<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">                <span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">              &#123;% endfor %&#125;</span><br><span class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">ul</span>&gt;</span></span><br><span class=\"line\">+        &#123;% include '../_custom/sidebar.swig' %&#125; </span><br><span class=\"line\">          <span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\">         &#123;% endif %&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"搜索功能\"><a href=\"#搜索功能\" class=\"headerlink\" title=\"搜索功能\"></a>搜索功能</h2><p>文件位置：<code>themes/next/_config.yml</code></p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Local search</span></span><br><span class=\"line\"><span class=\"comment\"># Dependencies: https://github.com/theme-next/hexo-generator-searchdb</span></span><br><span class=\"line\"><span class=\"attr\">local_search:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"string\">ture</span></span><br></pre></td></tr></table></figure>\n<p>安装插件</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure>\n<p>但是我在安装插件的时候一直报错</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm ERR! path /home/song/hexo/test/node_modules/babylon</span><br><span class=\"line\">npm ERR! code ENOENT</span><br><span class=\"line\">npm ERR! errno -2</span><br><span class=\"line\">npm ERR! syscall access</span><br><span class=\"line\">npm ERR! enoent ENOENT: no such file or directory, access '/home/song/hexo/test/node_modules/babylon'</span><br><span class=\"line\">npm ERR! enoent This is related to npm not being able to find a file.</span><br><span class=\"line\">npm ERR! enoent </span><br><span class=\"line\"></span><br><span class=\"line\">npm ERR! A complete log of this run can be found in:</span><br><span class=\"line\">npm ERR!     /home/song/.npm/_logs/2018-11-11T06_59_34_564Z-debug.log</span><br></pre></td></tr></table></figure>\n<p><a href=\"https://blog.csdn.net/h416756139/article/details/50812109\" target=\"_blank\" rel=\"noopener\">解决办法</a>：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> npm install -g cnpm --registry=http://registry.npm.taobao.org</span><br><span class=\"line\"><span class=\"meta\">$</span> cnpm install hexo-generator-search --save</span><br></pre></td></tr></table></figure>\n<h2 id=\"预告\"><a href=\"#预告\" class=\"headerlink\" title=\"预告\"></a>预告</h2><p>1.关于更新主题</p>\n<p>2.关于如何推广博客</p>\n<p>3.评论邮件提醒</p>"},{"title":"Git学习——使用Git将本地库的内容推送到Github","date":"2018-05-27T03:03:13.000Z","comments":1,"_content":"## 前言\n\n本文参考廖雪峰老师的教程，记录git将本地库的内容推送到Github远程仓库的整个流程。比如我要推送的是叫assignment1的文件夹，内容为斯坦福大学CS231n课程的第一个作业。\n## 流程\n\n1. 进入包含assignment1文件夹的目录，把当前目录变成Git可以管理的仓库:\n\n   ```shell\n   $ git init\n   ```\n   <!-- more -->\n\n2. 配置用户信息:\n\n   ```shell\n   $ git config --global user.name \"xxx\"\n   $ git config --global user.email \"xxx@xxx.com\"\n   ```\n\n3. 将文件添加到暂存区:\n\n   ```shell\n   $ git add assignment1\n   ```\n\n4. 提交文件到分支:\n\n   ```shell\n   $ git commit -m \"add assignment1\"\n   ```\n\n   双引号里面是本次提交的说明。输入说明对自己和对别人的阅读都很重要，所以建议写上。\n\n5. 在github上建立远程仓库，仓库的名字取为CS231n。\n\n6. 现在，在本地的仓库运行以下命令： \n\n   ```shell\n   $ git remote add origin git@github.com:xxx/CS231n.git\n   ```\n\n   这里的xxx替换成自己的Github用户名。\n\n7. 将本地库的所有内容推送到远程库上：\n\n   ```shell\n   $ git push -u origin master\n   ```\n\n8. 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令：\n\n   ```shell\n   $ git push origin master\n   ```\n\n9. 如果我们对本地库的文件进行了修改，提交到远程仓库只需要进行第3,4以及8步即可。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/git学习.md","raw":"---\ntitle: Git学习——使用Git将本地库的内容推送到Github\ndate: 2018-05-27 11:03:13\ntags: \n- git\ncategories:\n- 一些摸索\ncomments: true\n---\n## 前言\n\n本文参考廖雪峰老师的教程，记录git将本地库的内容推送到Github远程仓库的整个流程。比如我要推送的是叫assignment1的文件夹，内容为斯坦福大学CS231n课程的第一个作业。\n## 流程\n\n1. 进入包含assignment1文件夹的目录，把当前目录变成Git可以管理的仓库:\n\n   ```shell\n   $ git init\n   ```\n   <!-- more -->\n\n2. 配置用户信息:\n\n   ```shell\n   $ git config --global user.name \"xxx\"\n   $ git config --global user.email \"xxx@xxx.com\"\n   ```\n\n3. 将文件添加到暂存区:\n\n   ```shell\n   $ git add assignment1\n   ```\n\n4. 提交文件到分支:\n\n   ```shell\n   $ git commit -m \"add assignment1\"\n   ```\n\n   双引号里面是本次提交的说明。输入说明对自己和对别人的阅读都很重要，所以建议写上。\n\n5. 在github上建立远程仓库，仓库的名字取为CS231n。\n\n6. 现在，在本地的仓库运行以下命令： \n\n   ```shell\n   $ git remote add origin git@github.com:xxx/CS231n.git\n   ```\n\n   这里的xxx替换成自己的Github用户名。\n\n7. 将本地库的所有内容推送到远程库上：\n\n   ```shell\n   $ git push -u origin master\n   ```\n\n8. 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令：\n\n   ```shell\n   $ git push origin master\n   ```\n\n9. 如果我们对本地库的文件进行了修改，提交到远程仓库只需要进行第3,4以及8步即可。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"git学习","published":1,"updated":"2018-08-06T14:49:59.732Z","layout":"post","photos":[],"link":"","_id":"cjofrrc200002k8cyvs6exyag","content":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>本文参考廖雪峰老师的教程，记录git将本地库的内容推送到Github远程仓库的整个流程。比如我要推送的是叫assignment1的文件夹，内容为斯坦福大学CS231n课程的第一个作业。</p>\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><ol>\n<li><p>进入包含assignment1文件夹的目录，把当前目录变成Git可以管理的仓库:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git init</span><br></pre></td></tr></table></figure>\n<a id=\"more\"></a>\n</li>\n<li><p>配置用户信息:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git config --global user.name \"xxx\"</span><br><span class=\"line\"><span class=\"meta\">$</span> git config --global user.email \"xxx@xxx.com\"</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>将文件添加到暂存区:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git add assignment1</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>提交文件到分支:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git commit -m \"add assignment1\"</span><br></pre></td></tr></table></figure>\n<p>双引号里面是本次提交的说明。输入说明对自己和对别人的阅读都很重要，所以建议写上。</p>\n</li>\n<li><p>在github上建立远程仓库，仓库的名字取为CS231n。</p>\n</li>\n<li><p>现在，在本地的仓库运行以下命令： </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git remote add origin git@github.com:xxx/CS231n.git</span><br></pre></td></tr></table></figure>\n<p>这里的xxx替换成自己的Github用户名。</p>\n</li>\n<li><p>将本地库的所有内容推送到远程库上：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git push -u origin master</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git push origin master</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>如果我们对本地库的文件进行了修改，提交到远程仓库只需要进行第3,4以及8步即可。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>本文参考廖雪峰老师的教程，记录git将本地库的内容推送到Github远程仓库的整个流程。比如我要推送的是叫assignment1的文件夹，内容为斯坦福大学CS231n课程的第一个作业。</p>\n<h2 id=\"流程\"><a href=\"#流程\" class=\"headerlink\" title=\"流程\"></a>流程</h2><ol>\n<li><p>进入包含assignment1文件夹的目录，把当前目录变成Git可以管理的仓库:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git init</span><br></pre></td></tr></table></figure>","more":"</li>\n<li><p>配置用户信息:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git config --global user.name \"xxx\"</span><br><span class=\"line\"><span class=\"meta\">$</span> git config --global user.email \"xxx@xxx.com\"</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>将文件添加到暂存区:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git add assignment1</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>提交文件到分支:</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git commit -m \"add assignment1\"</span><br></pre></td></tr></table></figure>\n<p>双引号里面是本次提交的说明。输入说明对自己和对别人的阅读都很重要，所以建议写上。</p>\n</li>\n<li><p>在github上建立远程仓库，仓库的名字取为CS231n。</p>\n</li>\n<li><p>现在，在本地的仓库运行以下命令： </p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git remote add origin git@github.com:xxx/CS231n.git</span><br></pre></td></tr></table></figure>\n<p>这里的xxx替换成自己的Github用户名。</p>\n</li>\n<li><p>将本地库的所有内容推送到远程库上：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git push -u origin master</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span> git push origin master</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>如果我们对本地库的文件进行了修改，提交到远程仓库只需要进行第3,4以及8步即可。</p>\n</li>\n</ol>"},{"title":"学习使用TensorFlow来识别交通标志","date":"2018-09-13T02:50:14.000Z","comments":1,"_content":"\n# 前言\n\n本文参考https://juejin.im/entry/5a1637f2f265da432528f6ef  的文章和  https://github.com/waleedka/traffic-signs-tensorflow  的源代码。   \n\n 给定交通标志的图像，我们的模型应该能够知道它的类型。  \n 首先我们要导入需要的库。\n\n\n```python\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage import transform\nimport random\n```\n\n    /home/song/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n      from ._conv import register_converters as _register_converters\n\n   <!-- more -->\n## 1  加载数据和分析数据\n\n### 1.1 加载数据\n\n我们使用的是Belgian Traffic Sign Dataset。网址为http://btsd.ethz.ch/shareddata/  \n在这个网站可以下载到我们需要的数据集。你只需要下载BelgiumTS for Classification (cropped images):后面的两个数据集：  \n    \n    BelgiumTSC_Training (171.3MBytes)  \n    BelgiumTSC_Testing (76.5MBytes)  \n  我把这两个数据集分别放在了以下的路径：    \n    \n    /home/song/Downloads/BelgiumTSC_Training/Training  \n    /home/song/Downloads/BelgiumTSC_Testing/Testing  \n  Training目录包含具有从00000到00061的序列号的子目录。目录名称表示从0到61的标签，每个目录中的图像表示属于该标签的交通标志。 图像以不常见的.ppm格式保存，但幸运的是，这种格式在skimage库中得到了支持。\n\n\n```python\ndef load_data(data_dir):\n    # Get all subdirectories of data_dir. Each represents a label.\n    directories = [d for d in os.listdir(data_dir)\n                  if os.path.isdir(os.path.join(data_dir, d))]\n\n    # Loop through the label directories and collect the data in\n    # two lists, labels and images.\n    labels = []\n    images = []\n    for d in directories:\n        label_dir = os.path.join(data_dir, d)\n        file_names = [os.path.join(label_dir, f) \n                      for f in os.listdir(label_dir) \n                      if f.endswith(\".ppm\")]\n        for f in file_names:\n            images.append(data.imread(f))\n            labels.append(int(d))\n    return images, labels\n\nROOT_PATH = \"/home/song/Downloads/\"\ntrain_data_dir = os.path.join(ROOT_PATH, \"BelgiumTSC_Training/Training\")\ntest_data_dir = os.path.join(ROOT_PATH, \"BelgiumTSC_Testing/Testing\")\n\nimages, labels = load_data(train_data_dir)\n```\n\n### 1.2 分析数据\n\n我们可以看一下我们的训练集中有多少图片和标签：\n\n\n```python\nprint(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))\n```\n\n    Unique Labels: 62\n    Total Images: 4575\n\n这里的set很有意思，可以看一下这篇文章：http://www.voidcn.com/article/p-uekeyeby-hn.html  \n这里的set很有意思，可以看一下这篇文章：http://www.voidcn.com/article/p-uekeyeby-hn.html  \n在处理一系列数据时，如果需要剔除重复项，则通常采用set数据类型。本身labels里面是有很多重复的元素的，但set(labels)就剔除了重复项。可以通过print(labels)和print(set(labels))命令查看一下两者输出的有什么区别。  \n我们还可以通过画直方图来看一下数据的分布情况。\n\n\n```python\nplt.hist(labels,62)\nplt.show()\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/58270023.jpg)\n\n\n可以看出，该数据集中有的标签的分量比其它标签更重：标签 22、32、38 和 61 显然出类拔萃。这一点之后我们会更深入地了解。\n\n### 1.3 可视化数据\n\n#### 1.3.1 热身\n\n我们可以先随机地选取几个交通标志将其显示出来。我们还可以看一下图片的尺寸。我们还可以看一下图片的最小值和最大值，这是验证数据范围并及早发现错误的一个简单方法。其中的plt.axis('off')是为了不在图片上显示坐标尺，大家可以注释掉这句话看看如果去掉有什么不一样。\n\n\n```python\ntraffic_signs=[100,1050,3650,4000]\n\nfor i in range(len(traffic_signs)):\n    plt.subplot(1, 4, i+1)\n    plt.axis('off')\n    plt.imshow(images[traffic_signs[i]])\n    #plt.subplots_adjust(wspace=0.5)\n    plt.show()\n    print(\"shape: {0}, min: {1}, max: {2}\".format(images[traffic_signs[i]].shape, \n                                                  images[traffic_signs[i]].min(), \n                                                  images[traffic_signs[i]].max()))\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/91631184.jpg)\n\n\n    shape: (292, 290, 3), min: 0, max: 255\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94366993.jpg)\n\n\n    shape: (132, 139, 3), min: 4, max: 255\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/55563405.jpg)\n\n\n    shape: (146, 110, 3), min: 7, max: 255\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/66096551.jpg)\n\n\n    shape: (110, 105, 3), min: 3, max: 255\n\n大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32*32。\n大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32*32。\n\n#### 1.3.2 重调图片的大小\n\n\n```python\nimages32 = [transform.resize(image,(32,32)) for image in images]\n```\n\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n      warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n      warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n\n重新运行上面随机显示交通标志的代码。\n重新运行上面随机显示交通标志的代码。\n\n\n```python\ntraffic_signs=[100,1050,3650,4000]\n\nfor i in range(len(traffic_signs)):\n    plt.subplot(1, 4, i+1)\n    plt.axis('off')\n    plt.imshow(images32[traffic_signs[i]])\n    plt.subplots_adjust(wspace=0.5)\n    plt.show()\n    print(\"shape: {0}, min: {1}, max: {2}\".format(images32[traffic_signs[i]].shape, \n                                                  images32[traffic_signs[i]].min(), \n                                                  images32[traffic_signs[i]].max()))\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/65434369.jpg)\n\n\n    shape: (32, 32, 3), min: 0.0, max: 1.0\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/59269206.jpg)\n\n\n    shape: (32, 32, 3), min: 0.038373161764705975, max: 1.0\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/62497379.jpg)\n\n\n    shape: (32, 32, 3), min: 0.05559895833333348, max: 1.0\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/90866008.jpg)\n\n\n    shape: (32, 32, 3), min: 0.048665364583333495, max: 1.0\n\n从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。\n从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。\n\n#### 1.3.3 显示每一个标签下的第一张图片\n\n之前我们在直方图中看过62个标签的分布情况。现在我们尝试将每个标签下的第一张图片显示出来，另外还可以通过列表的count()方法来统计某个标签出现的次数，也就是能统计出有多少张图片对应该标签。我们可以定义一个函数，名为display_images_and_labels，你当然可以定义成别的名字，不过定义函数是为了之后可以方便地调用。以下分别显示出了未调整尺寸和已调整尺寸的交通标志图。\n\n\n```python\ndef display_images_and_labels(images, labels):\n    \"\"\"Display the first image of each label.\"\"\"\n    unique_labels = set(labels)\n    plt.figure(figsize=(15, 15))\n    i = 1\n    for label in unique_labels:\n        # Pick the first image for each label.\n        image = images[labels.index(label)]\n        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n        plt.axis('off')\n        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n        i += 1\n        plt.imshow(image)\n    \n\ndisplay_images_and_labels(images, labels)\ndisplay_images_and_labels(images32, labels)\n\n\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/19586062.jpg)\n\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/15441432.jpg)\n\n\n正如我们在直方图中看到的那样，具有标签 22、32、38 和 61 的交通标志要明显多得多。图中可以看到标签 22 有 375 个实例，标签 32 有 316 实例，标签 38 有 285 个实例，标签 61 有 282 个实例。\n\n#### 1.3.4 显示某一个标签下的交通标志\n\n看过每个标签下的第一张图片之后，我们可以将某一个标签下的图片展开显示出来，看看这个标签下的是否是同一类交通标志。我们不需要把该标签下的所有图片都显示出来，可以只展示24张，你可以更改为其他的数字，显示更多或者更少。我们这里选择标签为21的看一下，在之前的图片中可以看到，label 21对应于stop标志。\n\n\n```python\ndef display_label_images(images, label):\n    \"\"\"Display images of a specific label.\"\"\"\n    limit = 24  # show a max of 24 images\n    plt.figure(figsize=(15, 5))\n    i = 1\n\n    start = labels.index(label)\n    end = start + labels.count(label)\n    for image in images[start:end][:limit]:\n        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n        plt.axis('off')\n        i += 1\n        plt.imshow(image)\n\n\ndisplay_label_images(images32,21)\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/52152061.jpg)\n\n\n可以看出，label 21对应的前24张图片都是stop标志。不难推测，整个label 21对应的应都是stop标志。\n\n## 2 构建深度网络 \n\n### 2.1 构建TensorFlow图并训练\n\n首先，我们创建一个Graph对象。TensorFlow有一个默认的全局图，但是我们不建议使用它。设置全局变量通常太容易引入错误了，因此我们自己创建一个图。之后设置占位符来放图片和标签。注意这里参数x的维度是 [None, 32, 32, 3]，这四个参数分别表示 [批量大小，高度，宽度，通道] （通常缩写为 NHWC）。我们定义了一个全连接层，并使用了relu激活函数进行非线性操作。我们通过argmax()函数找到logits最大值对应的索引，也就是预测的标签了。之后定义loss函数，并选择合适的优化算法。这里选择Adam算法，因为它的收敛速度比一般的梯度下降算法更快。这个时候我们只刚刚构建图，并且描述了输入。我们定义的变量，比如，loss和predicted_labels，它们都不包含具体的数值。它们是我们接下来要执行的操作的引用。我们要创建会话才能开始训练。我这里把循环次数设置为301，并且如果i是10的倍数，就打印loss的值。\n\n\n```python\ng = tf.Graph()\n\nwith g.as_default():\n    # Initialize placeholders \n    x = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32,3])\n    y = tf.placeholder(dtype = tf.int32, shape = [None])\n\n    # Flatten the input data\n    images_flat = tf.contrib.layers.flatten(x)\n    #print(images_flat)\n    \n    # Fully connected layer \n    logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)\n\n     # Convert logits to label \n    predicted_labels = tf.argmax(logits, 1)\n    \n    # Define a loss function\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, \n                                logits = logits))\n\n    # Define an optimizer \n    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\n    print(\"images_flat: \", images_flat)\n    print(\"logits: \", logits)\n    print(\"loss: \", loss)\n    print(\"predicted_labels: \", predicted_labels)\n\n    sess=tf.Session(graph=g)\n    sess.run(tf.global_variables_initializer())\n    for i in range(301):\n        #print('EPOCH', i)\n        _,loss_value = sess.run([train_op, loss], feed_dict={x: images32, y: labels}) \n        if i % 10 == 0:\n            print(\"Loss: \", loss_value)\n        #print('DONE WITH EPOCH')\n```\n\n    images_flat:  Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 3072), dtype=float32)\n    logits:  Tensor(\"fully_connected/Relu:0\", shape=(?, 62), dtype=float32)\n    loss:  Tensor(\"Mean:0\", shape=(), dtype=float32)\n    predicted_labels:  Tensor(\"ArgMax:0\", shape=(?,), dtype=int64)\n    Loss:  4.181018\n    Loss:  3.0714655\n    Loss:  2.6622696\n    Loss:  2.4586942\n    Loss:  2.3419585\n    Loss:  2.2633858\n    Loss:  2.2044215\n    Loss:  2.157206\n    Loss:  2.1180305\n    Loss:  2.0847433\n    Loss:  2.0559382\n    Loss:  2.030667\n    Loss:  2.008251\n    Loss:  1.9882014\n    Loss:  1.9701369\n    Loss:  1.9537587\n    Loss:  1.938837\n    Loss:  1.9251733\n    Loss:  1.912607\n    Loss:  1.9010073\n    Loss:  1.8902632\n    Loss:  1.8802778\n    Loss:  1.8709714\n    Loss:  1.8622767\n    Loss:  1.8541412\n    Loss:  1.8465083\n    Loss:  1.8393359\n    Loss:  1.8325756\n    Loss:  1.8261962\n    Loss:  1.8201678\n    Loss:  1.8144621\n\n### 2.2使用模型\n### 2.2使用模型\n\n现在我们用sess.run()来使用我们训练好的模型，并随机取了训练集中的10个图片进行分类，并同时打印了真实的标签结果和预测结果。\n\n\n```python\n# Pick 10 random images\nsample_indexes = random.sample(range(len(images32)), 10)\nsample_images = [images32[i] for i in sample_indexes]\nsample_labels = [labels[i] for i in sample_indexes]\n\n# Run the \"predicted_labels\" op.\npredicted = sess.run([predicted_labels], \n                        feed_dict={x: sample_images})[0]\nprint(sample_labels)\nprint(predicted)\n```\n\n    [41, 39, 1, 53, 21, 22, 38, 48, 7, 53]\n    [41 39  1 53 21 22 40 47  7 53]\n    \n\n```python\n​```python\nfig=plt.figure(figsize=(10,10))\nfor i in range(len(sample_images)):\n    truth = sample_labels[i]\n    prediction = predicted[i]\n    plt.subplot(5,2,1+i)\n    plt.axis(\"off\")\n    color='green' if truth == prediction else 'red'\n    plt.text(40,10,\"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n             fontsize=12, color=color)\n    plt.imshow(sample_images[i])\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94071068.jpg)\n\n\n### 2.3评估模型\n\n以上，我们的模型只在训练集上是可以正常运行的，但是它对于其他的未知数据集的泛化能力如何呢？我们可以在测试集当中进行评估。我们还可以计算一下准确率。\n\n\n```python\ntest_images, test_labels = load_data(test_data_dir)\ntest_images32 = [transform.resize(image, (32, 32))\n                 for image in test_images]\ndisplay_images_and_labels(test_images32, test_labels)\n\n# Calculate how many matches we got.\npredicted = sess.run([predicted_labels], \n                        feed_dict={x: test_images32})[0]\nmatch_count = sum([int(y == y_) \n                   for y, y_ in zip(test_labels, predicted)])\naccuracy = match_count / len(test_labels)\nprint(\"Accuracy: {:.4f}\".format(accuracy))\n\n\n\n# Pick 10 random images\nsample_test_indexes = random.sample(range(len(test_images32)), 10)\nsample_test_images = [test_images32[i] for i in sample_test_indexes]\nsample_test_labels = [test_labels[i] for i in sample_test_indexes]\n\n# Run the \"predicted_labels\" op.\ntest_predicted = sess.run([predicted_labels], \n                        feed_dict={x: sample_test_images})[0]\nprint(sample_test_labels)\nprint(test_predicted)\n\nfig=plt.figure(figsize=(10,10))\nfor i in range(len(sample_test_images)):\n    truth = sample_test_labels[i]\n    prediction = test_predicted[i]\n    plt.subplot(5,2,1+i)\n    plt.axis(\"off\")\n    color='green' if truth == prediction else 'red'\n    plt.text(40,10,\"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n             fontsize=12, color=color)\n    plt.imshow(sample_test_images[i])\n```\n\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n      warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n      warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n    \n    \n    Accuracy: 0.5631\n    [38, 35, 19, 32, 32, 7, 13, 38, 18, 38]\n    [39  0 19 32 32  7 13 40 17 39]\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/67359700.jpg)\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/73349594.jpg)\n\n\n### 2.4关闭会话\n\n\n```python\nsess.close()\n```\n\n最后，记得关闭会话。\n","source":"_posts/学习使用TensorFlow来识别交通标志.md","raw":"---\ntitle: 学习使用TensorFlow来识别交通标志\ndate: 2018-09-13 10:50:14\ntags:\n- tensorflow\ncategories:\n- tensorflow\ncomments: true\n---\n\n# 前言\n\n本文参考https://juejin.im/entry/5a1637f2f265da432528f6ef  的文章和  https://github.com/waleedka/traffic-signs-tensorflow  的源代码。   \n\n 给定交通标志的图像，我们的模型应该能够知道它的类型。  \n 首先我们要导入需要的库。\n\n\n```python\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage import transform\nimport random\n```\n\n    /home/song/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n      from ._conv import register_converters as _register_converters\n\n   <!-- more -->\n## 1  加载数据和分析数据\n\n### 1.1 加载数据\n\n我们使用的是Belgian Traffic Sign Dataset。网址为http://btsd.ethz.ch/shareddata/  \n在这个网站可以下载到我们需要的数据集。你只需要下载BelgiumTS for Classification (cropped images):后面的两个数据集：  \n    \n    BelgiumTSC_Training (171.3MBytes)  \n    BelgiumTSC_Testing (76.5MBytes)  \n  我把这两个数据集分别放在了以下的路径：    \n    \n    /home/song/Downloads/BelgiumTSC_Training/Training  \n    /home/song/Downloads/BelgiumTSC_Testing/Testing  \n  Training目录包含具有从00000到00061的序列号的子目录。目录名称表示从0到61的标签，每个目录中的图像表示属于该标签的交通标志。 图像以不常见的.ppm格式保存，但幸运的是，这种格式在skimage库中得到了支持。\n\n\n```python\ndef load_data(data_dir):\n    # Get all subdirectories of data_dir. Each represents a label.\n    directories = [d for d in os.listdir(data_dir)\n                  if os.path.isdir(os.path.join(data_dir, d))]\n\n    # Loop through the label directories and collect the data in\n    # two lists, labels and images.\n    labels = []\n    images = []\n    for d in directories:\n        label_dir = os.path.join(data_dir, d)\n        file_names = [os.path.join(label_dir, f) \n                      for f in os.listdir(label_dir) \n                      if f.endswith(\".ppm\")]\n        for f in file_names:\n            images.append(data.imread(f))\n            labels.append(int(d))\n    return images, labels\n\nROOT_PATH = \"/home/song/Downloads/\"\ntrain_data_dir = os.path.join(ROOT_PATH, \"BelgiumTSC_Training/Training\")\ntest_data_dir = os.path.join(ROOT_PATH, \"BelgiumTSC_Testing/Testing\")\n\nimages, labels = load_data(train_data_dir)\n```\n\n### 1.2 分析数据\n\n我们可以看一下我们的训练集中有多少图片和标签：\n\n\n```python\nprint(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))\n```\n\n    Unique Labels: 62\n    Total Images: 4575\n\n这里的set很有意思，可以看一下这篇文章：http://www.voidcn.com/article/p-uekeyeby-hn.html  \n这里的set很有意思，可以看一下这篇文章：http://www.voidcn.com/article/p-uekeyeby-hn.html  \n在处理一系列数据时，如果需要剔除重复项，则通常采用set数据类型。本身labels里面是有很多重复的元素的，但set(labels)就剔除了重复项。可以通过print(labels)和print(set(labels))命令查看一下两者输出的有什么区别。  \n我们还可以通过画直方图来看一下数据的分布情况。\n\n\n```python\nplt.hist(labels,62)\nplt.show()\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/58270023.jpg)\n\n\n可以看出，该数据集中有的标签的分量比其它标签更重：标签 22、32、38 和 61 显然出类拔萃。这一点之后我们会更深入地了解。\n\n### 1.3 可视化数据\n\n#### 1.3.1 热身\n\n我们可以先随机地选取几个交通标志将其显示出来。我们还可以看一下图片的尺寸。我们还可以看一下图片的最小值和最大值，这是验证数据范围并及早发现错误的一个简单方法。其中的plt.axis('off')是为了不在图片上显示坐标尺，大家可以注释掉这句话看看如果去掉有什么不一样。\n\n\n```python\ntraffic_signs=[100,1050,3650,4000]\n\nfor i in range(len(traffic_signs)):\n    plt.subplot(1, 4, i+1)\n    plt.axis('off')\n    plt.imshow(images[traffic_signs[i]])\n    #plt.subplots_adjust(wspace=0.5)\n    plt.show()\n    print(\"shape: {0}, min: {1}, max: {2}\".format(images[traffic_signs[i]].shape, \n                                                  images[traffic_signs[i]].min(), \n                                                  images[traffic_signs[i]].max()))\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/91631184.jpg)\n\n\n    shape: (292, 290, 3), min: 0, max: 255\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94366993.jpg)\n\n\n    shape: (132, 139, 3), min: 4, max: 255\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/55563405.jpg)\n\n\n    shape: (146, 110, 3), min: 7, max: 255\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/66096551.jpg)\n\n\n    shape: (110, 105, 3), min: 3, max: 255\n\n大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32*32。\n大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32*32。\n\n#### 1.3.2 重调图片的大小\n\n\n```python\nimages32 = [transform.resize(image,(32,32)) for image in images]\n```\n\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n      warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n      warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n\n重新运行上面随机显示交通标志的代码。\n重新运行上面随机显示交通标志的代码。\n\n\n```python\ntraffic_signs=[100,1050,3650,4000]\n\nfor i in range(len(traffic_signs)):\n    plt.subplot(1, 4, i+1)\n    plt.axis('off')\n    plt.imshow(images32[traffic_signs[i]])\n    plt.subplots_adjust(wspace=0.5)\n    plt.show()\n    print(\"shape: {0}, min: {1}, max: {2}\".format(images32[traffic_signs[i]].shape, \n                                                  images32[traffic_signs[i]].min(), \n                                                  images32[traffic_signs[i]].max()))\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/65434369.jpg)\n\n\n    shape: (32, 32, 3), min: 0.0, max: 1.0\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/59269206.jpg)\n\n\n    shape: (32, 32, 3), min: 0.038373161764705975, max: 1.0\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/62497379.jpg)\n\n\n    shape: (32, 32, 3), min: 0.05559895833333348, max: 1.0\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/90866008.jpg)\n\n\n    shape: (32, 32, 3), min: 0.048665364583333495, max: 1.0\n\n从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。\n从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。\n\n#### 1.3.3 显示每一个标签下的第一张图片\n\n之前我们在直方图中看过62个标签的分布情况。现在我们尝试将每个标签下的第一张图片显示出来，另外还可以通过列表的count()方法来统计某个标签出现的次数，也就是能统计出有多少张图片对应该标签。我们可以定义一个函数，名为display_images_and_labels，你当然可以定义成别的名字，不过定义函数是为了之后可以方便地调用。以下分别显示出了未调整尺寸和已调整尺寸的交通标志图。\n\n\n```python\ndef display_images_and_labels(images, labels):\n    \"\"\"Display the first image of each label.\"\"\"\n    unique_labels = set(labels)\n    plt.figure(figsize=(15, 15))\n    i = 1\n    for label in unique_labels:\n        # Pick the first image for each label.\n        image = images[labels.index(label)]\n        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n        plt.axis('off')\n        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n        i += 1\n        plt.imshow(image)\n    \n\ndisplay_images_and_labels(images, labels)\ndisplay_images_and_labels(images32, labels)\n\n\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/19586062.jpg)\n\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/15441432.jpg)\n\n\n正如我们在直方图中看到的那样，具有标签 22、32、38 和 61 的交通标志要明显多得多。图中可以看到标签 22 有 375 个实例，标签 32 有 316 实例，标签 38 有 285 个实例，标签 61 有 282 个实例。\n\n#### 1.3.4 显示某一个标签下的交通标志\n\n看过每个标签下的第一张图片之后，我们可以将某一个标签下的图片展开显示出来，看看这个标签下的是否是同一类交通标志。我们不需要把该标签下的所有图片都显示出来，可以只展示24张，你可以更改为其他的数字，显示更多或者更少。我们这里选择标签为21的看一下，在之前的图片中可以看到，label 21对应于stop标志。\n\n\n```python\ndef display_label_images(images, label):\n    \"\"\"Display images of a specific label.\"\"\"\n    limit = 24  # show a max of 24 images\n    plt.figure(figsize=(15, 5))\n    i = 1\n\n    start = labels.index(label)\n    end = start + labels.count(label)\n    for image in images[start:end][:limit]:\n        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n        plt.axis('off')\n        i += 1\n        plt.imshow(image)\n\n\ndisplay_label_images(images32,21)\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/52152061.jpg)\n\n\n可以看出，label 21对应的前24张图片都是stop标志。不难推测，整个label 21对应的应都是stop标志。\n\n## 2 构建深度网络 \n\n### 2.1 构建TensorFlow图并训练\n\n首先，我们创建一个Graph对象。TensorFlow有一个默认的全局图，但是我们不建议使用它。设置全局变量通常太容易引入错误了，因此我们自己创建一个图。之后设置占位符来放图片和标签。注意这里参数x的维度是 [None, 32, 32, 3]，这四个参数分别表示 [批量大小，高度，宽度，通道] （通常缩写为 NHWC）。我们定义了一个全连接层，并使用了relu激活函数进行非线性操作。我们通过argmax()函数找到logits最大值对应的索引，也就是预测的标签了。之后定义loss函数，并选择合适的优化算法。这里选择Adam算法，因为它的收敛速度比一般的梯度下降算法更快。这个时候我们只刚刚构建图，并且描述了输入。我们定义的变量，比如，loss和predicted_labels，它们都不包含具体的数值。它们是我们接下来要执行的操作的引用。我们要创建会话才能开始训练。我这里把循环次数设置为301，并且如果i是10的倍数，就打印loss的值。\n\n\n```python\ng = tf.Graph()\n\nwith g.as_default():\n    # Initialize placeholders \n    x = tf.placeholder(dtype = tf.float32, shape = [None, 32, 32,3])\n    y = tf.placeholder(dtype = tf.int32, shape = [None])\n\n    # Flatten the input data\n    images_flat = tf.contrib.layers.flatten(x)\n    #print(images_flat)\n    \n    # Fully connected layer \n    logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)\n\n     # Convert logits to label \n    predicted_labels = tf.argmax(logits, 1)\n    \n    # Define a loss function\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, \n                                logits = logits))\n\n    # Define an optimizer \n    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\n    print(\"images_flat: \", images_flat)\n    print(\"logits: \", logits)\n    print(\"loss: \", loss)\n    print(\"predicted_labels: \", predicted_labels)\n\n    sess=tf.Session(graph=g)\n    sess.run(tf.global_variables_initializer())\n    for i in range(301):\n        #print('EPOCH', i)\n        _,loss_value = sess.run([train_op, loss], feed_dict={x: images32, y: labels}) \n        if i % 10 == 0:\n            print(\"Loss: \", loss_value)\n        #print('DONE WITH EPOCH')\n```\n\n    images_flat:  Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 3072), dtype=float32)\n    logits:  Tensor(\"fully_connected/Relu:0\", shape=(?, 62), dtype=float32)\n    loss:  Tensor(\"Mean:0\", shape=(), dtype=float32)\n    predicted_labels:  Tensor(\"ArgMax:0\", shape=(?,), dtype=int64)\n    Loss:  4.181018\n    Loss:  3.0714655\n    Loss:  2.6622696\n    Loss:  2.4586942\n    Loss:  2.3419585\n    Loss:  2.2633858\n    Loss:  2.2044215\n    Loss:  2.157206\n    Loss:  2.1180305\n    Loss:  2.0847433\n    Loss:  2.0559382\n    Loss:  2.030667\n    Loss:  2.008251\n    Loss:  1.9882014\n    Loss:  1.9701369\n    Loss:  1.9537587\n    Loss:  1.938837\n    Loss:  1.9251733\n    Loss:  1.912607\n    Loss:  1.9010073\n    Loss:  1.8902632\n    Loss:  1.8802778\n    Loss:  1.8709714\n    Loss:  1.8622767\n    Loss:  1.8541412\n    Loss:  1.8465083\n    Loss:  1.8393359\n    Loss:  1.8325756\n    Loss:  1.8261962\n    Loss:  1.8201678\n    Loss:  1.8144621\n\n### 2.2使用模型\n### 2.2使用模型\n\n现在我们用sess.run()来使用我们训练好的模型，并随机取了训练集中的10个图片进行分类，并同时打印了真实的标签结果和预测结果。\n\n\n```python\n# Pick 10 random images\nsample_indexes = random.sample(range(len(images32)), 10)\nsample_images = [images32[i] for i in sample_indexes]\nsample_labels = [labels[i] for i in sample_indexes]\n\n# Run the \"predicted_labels\" op.\npredicted = sess.run([predicted_labels], \n                        feed_dict={x: sample_images})[0]\nprint(sample_labels)\nprint(predicted)\n```\n\n    [41, 39, 1, 53, 21, 22, 38, 48, 7, 53]\n    [41 39  1 53 21 22 40 47  7 53]\n    \n\n```python\n​```python\nfig=plt.figure(figsize=(10,10))\nfor i in range(len(sample_images)):\n    truth = sample_labels[i]\n    prediction = predicted[i]\n    plt.subplot(5,2,1+i)\n    plt.axis(\"off\")\n    color='green' if truth == prediction else 'red'\n    plt.text(40,10,\"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n             fontsize=12, color=color)\n    plt.imshow(sample_images[i])\n```\n\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94071068.jpg)\n\n\n### 2.3评估模型\n\n以上，我们的模型只在训练集上是可以正常运行的，但是它对于其他的未知数据集的泛化能力如何呢？我们可以在测试集当中进行评估。我们还可以计算一下准确率。\n\n\n```python\ntest_images, test_labels = load_data(test_data_dir)\ntest_images32 = [transform.resize(image, (32, 32))\n                 for image in test_images]\ndisplay_images_and_labels(test_images32, test_labels)\n\n# Calculate how many matches we got.\npredicted = sess.run([predicted_labels], \n                        feed_dict={x: test_images32})[0]\nmatch_count = sum([int(y == y_) \n                   for y, y_ in zip(test_labels, predicted)])\naccuracy = match_count / len(test_labels)\nprint(\"Accuracy: {:.4f}\".format(accuracy))\n\n\n\n# Pick 10 random images\nsample_test_indexes = random.sample(range(len(test_images32)), 10)\nsample_test_images = [test_images32[i] for i in sample_test_indexes]\nsample_test_labels = [test_labels[i] for i in sample_test_indexes]\n\n# Run the \"predicted_labels\" op.\ntest_predicted = sess.run([predicted_labels], \n                        feed_dict={x: sample_test_images})[0]\nprint(sample_test_labels)\nprint(test_predicted)\n\nfig=plt.figure(figsize=(10,10))\nfor i in range(len(sample_test_images)):\n    truth = sample_test_labels[i]\n    prediction = test_predicted[i]\n    plt.subplot(5,2,1+i)\n    plt.axis(\"off\")\n    color='green' if truth == prediction else 'red'\n    plt.text(40,10,\"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n             fontsize=12, color=color)\n    plt.imshow(sample_test_images[i])\n```\n\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n      warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n    /home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n      warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n    \n    \n    Accuracy: 0.5631\n    [38, 35, 19, 32, 32, 7, 13, 38, 18, 38]\n    [39  0 19 32 32  7 13 40 17 39]\n    \n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/67359700.jpg)\n\n![png](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/73349594.jpg)\n\n\n### 2.4关闭会话\n\n\n```python\nsess.close()\n```\n\n最后，记得关闭会话。\n","slug":"学习使用TensorFlow来识别交通标志","published":1,"updated":"2018-09-13T03:13:34.378Z","layout":"post","photos":[],"link":"","_id":"cjofrrc2a0006k8cyiyu0syhx","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>本文参考<a href=\"https://juejin.im/entry/5a1637f2f265da432528f6ef\" target=\"_blank\" rel=\"noopener\">https://juejin.im/entry/5a1637f2f265da432528f6ef</a>  的文章和  <a href=\"https://github.com/waleedka/traffic-signs-tensorflow\" target=\"_blank\" rel=\"noopener\">https://github.com/waleedka/traffic-signs-tensorflow</a>  的源代码。   </p>\n<p> 给定交通标志的图像，我们的模型应该能够知道它的类型。<br> 首先我们要导入需要的库。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> skimage <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> skimage <span class=\"keyword\">import</span> transform</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br></pre></td></tr></table></figure>\n<pre><code>/home/song/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n</code></pre>   <a id=\"more\"></a>\n<h2 id=\"1-加载数据和分析数据\"><a href=\"#1-加载数据和分析数据\" class=\"headerlink\" title=\"1  加载数据和分析数据\"></a>1  加载数据和分析数据</h2><h3 id=\"1-1-加载数据\"><a href=\"#1-1-加载数据\" class=\"headerlink\" title=\"1.1 加载数据\"></a>1.1 加载数据</h3><p>我们使用的是Belgian Traffic Sign Dataset。网址为<a href=\"http://btsd.ethz.ch/shareddata/\" target=\"_blank\" rel=\"noopener\">http://btsd.ethz.ch/shareddata/</a><br>在这个网站可以下载到我们需要的数据集。你只需要下载BelgiumTS for Classification (cropped images):后面的两个数据集：  </p>\n<pre><code>BelgiumTSC_Training (171.3MBytes)  \nBelgiumTSC_Testing (76.5MBytes)  \n</code></pre><p>  我把这两个数据集分别放在了以下的路径：    </p>\n<pre><code>/home/song/Downloads/BelgiumTSC_Training/Training  \n/home/song/Downloads/BelgiumTSC_Testing/Testing  \n</code></pre><p>  Training目录包含具有从00000到00061的序列号的子目录。目录名称表示从0到61的标签，每个目录中的图像表示属于该标签的交通标志。 图像以不常见的.ppm格式保存，但幸运的是，这种格式在skimage库中得到了支持。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">(data_dir)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Get all subdirectories of data_dir. Each represents a label.</span></span><br><span class=\"line\">    directories = [d <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> os.listdir(data_dir)</span><br><span class=\"line\">                  <span class=\"keyword\">if</span> os.path.isdir(os.path.join(data_dir, d))]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop through the label directories and collect the data in</span></span><br><span class=\"line\">    <span class=\"comment\"># two lists, labels and images.</span></span><br><span class=\"line\">    labels = []</span><br><span class=\"line\">    images = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> directories:</span><br><span class=\"line\">        label_dir = os.path.join(data_dir, d)</span><br><span class=\"line\">        file_names = [os.path.join(label_dir, f) </span><br><span class=\"line\">                      <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> os.listdir(label_dir) </span><br><span class=\"line\">                      <span class=\"keyword\">if</span> f.endswith(<span class=\"string\">\".ppm\"</span>)]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> file_names:</span><br><span class=\"line\">            images.append(data.imread(f))</span><br><span class=\"line\">            labels.append(int(d))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> images, labels</span><br><span class=\"line\"></span><br><span class=\"line\">ROOT_PATH = <span class=\"string\">\"/home/song/Downloads/\"</span></span><br><span class=\"line\">train_data_dir = os.path.join(ROOT_PATH, <span class=\"string\">\"BelgiumTSC_Training/Training\"</span>)</span><br><span class=\"line\">test_data_dir = os.path.join(ROOT_PATH, <span class=\"string\">\"BelgiumTSC_Testing/Testing\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">images, labels = load_data(train_data_dir)</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-分析数据\"><a href=\"#1-2-分析数据\" class=\"headerlink\" title=\"1.2 分析数据\"></a>1.2 分析数据</h3><p>我们可以看一下我们的训练集中有多少图片和标签：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(<span class=\"string\">\"Unique Labels: &#123;0&#125;\\nTotal Images: &#123;1&#125;\"</span>.format(len(set(labels)), len(images)))</span><br></pre></td></tr></table></figure>\n<pre><code>Unique Labels: 62\nTotal Images: 4575\n</code></pre><p>这里的set很有意思，可以看一下这篇文章：<a href=\"http://www.voidcn.com/article/p-uekeyeby-hn.html\" target=\"_blank\" rel=\"noopener\">http://www.voidcn.com/article/p-uekeyeby-hn.html</a><br>这里的set很有意思，可以看一下这篇文章：<a href=\"http://www.voidcn.com/article/p-uekeyeby-hn.html\" target=\"_blank\" rel=\"noopener\">http://www.voidcn.com/article/p-uekeyeby-hn.html</a><br>在处理一系列数据时，如果需要剔除重复项，则通常采用set数据类型。本身labels里面是有很多重复的元素的，但set(labels)就剔除了重复项。可以通过print(labels)和print(set(labels))命令查看一下两者输出的有什么区别。<br>我们还可以通过画直方图来看一下数据的分布情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.hist(labels,<span class=\"number\">62</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/58270023.jpg\" alt=\"png\"></p>\n<p>可以看出，该数据集中有的标签的分量比其它标签更重：标签 22、32、38 和 61 显然出类拔萃。这一点之后我们会更深入地了解。</p>\n<h3 id=\"1-3-可视化数据\"><a href=\"#1-3-可视化数据\" class=\"headerlink\" title=\"1.3 可视化数据\"></a>1.3 可视化数据</h3><h4 id=\"1-3-1-热身\"><a href=\"#1-3-1-热身\" class=\"headerlink\" title=\"1.3.1 热身\"></a>1.3.1 热身</h4><p>我们可以先随机地选取几个交通标志将其显示出来。我们还可以看一下图片的尺寸。我们还可以看一下图片的最小值和最大值，这是验证数据范围并及早发现错误的一个简单方法。其中的plt.axis(‘off’)是为了不在图片上显示坐标尺，大家可以注释掉这句话看看如果去掉有什么不一样。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">traffic_signs=[<span class=\"number\">100</span>,<span class=\"number\">1050</span>,<span class=\"number\">3650</span>,<span class=\"number\">4000</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(traffic_signs)):</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">4</span>, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">    plt.imshow(images[traffic_signs[i]])</span><br><span class=\"line\">    <span class=\"comment\">#plt.subplots_adjust(wspace=0.5)</span></span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    print(<span class=\"string\">\"shape: &#123;0&#125;, min: &#123;1&#125;, max: &#123;2&#125;\"</span>.format(images[traffic_signs[i]].shape, </span><br><span class=\"line\">                                                  images[traffic_signs[i]].min(), </span><br><span class=\"line\">                                                  images[traffic_signs[i]].max()))</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/91631184.jpg\" alt=\"png\"></p>\n<pre><code>shape: (292, 290, 3), min: 0, max: 255\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94366993.jpg\" alt=\"png\"></p>\n<pre><code>shape: (132, 139, 3), min: 4, max: 255\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/55563405.jpg\" alt=\"png\"></p>\n<pre><code>shape: (146, 110, 3), min: 7, max: 255\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/66096551.jpg\" alt=\"png\"></p>\n<pre><code>shape: (110, 105, 3), min: 3, max: 255\n</code></pre><p>大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32<em>32。<br>大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32</em>32。</p>\n<h4 id=\"1-3-2-重调图片的大小\"><a href=\"#1-3-2-重调图片的大小\" class=\"headerlink\" title=\"1.3.2 重调图片的大小\"></a>1.3.2 重调图片的大小</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">images32 = [transform.resize(image,(<span class=\"number\">32</span>,<span class=\"number\">32</span>)) <span class=\"keyword\">for</span> image <span class=\"keyword\">in</span> images]</span><br></pre></td></tr></table></figure>\n<pre><code>/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in skimage 0.15.\n  warn(&quot;The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in &quot;\n/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;\n</code></pre><p>重新运行上面随机显示交通标志的代码。<br>重新运行上面随机显示交通标志的代码。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">traffic_signs=[<span class=\"number\">100</span>,<span class=\"number\">1050</span>,<span class=\"number\">3650</span>,<span class=\"number\">4000</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(traffic_signs)):</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">4</span>, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">    plt.imshow(images32[traffic_signs[i]])</span><br><span class=\"line\">    plt.subplots_adjust(wspace=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    print(<span class=\"string\">\"shape: &#123;0&#125;, min: &#123;1&#125;, max: &#123;2&#125;\"</span>.format(images32[traffic_signs[i]].shape, </span><br><span class=\"line\">                                                  images32[traffic_signs[i]].min(), </span><br><span class=\"line\">                                                  images32[traffic_signs[i]].max()))</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/65434369.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.0, max: 1.0\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/59269206.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.038373161764705975, max: 1.0\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/62497379.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.05559895833333348, max: 1.0\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/90866008.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.048665364583333495, max: 1.0\n</code></pre><p>从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。<br>从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。</p>\n<h4 id=\"1-3-3-显示每一个标签下的第一张图片\"><a href=\"#1-3-3-显示每一个标签下的第一张图片\" class=\"headerlink\" title=\"1.3.3 显示每一个标签下的第一张图片\"></a>1.3.3 显示每一个标签下的第一张图片</h4><p>之前我们在直方图中看过62个标签的分布情况。现在我们尝试将每个标签下的第一张图片显示出来，另外还可以通过列表的count()方法来统计某个标签出现的次数，也就是能统计出有多少张图片对应该标签。我们可以定义一个函数，名为display_images_and_labels，你当然可以定义成别的名字，不过定义函数是为了之后可以方便地调用。以下分别显示出了未调整尺寸和已调整尺寸的交通标志图。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">display_images_and_labels</span><span class=\"params\">(images, labels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Display the first image of each label.\"\"\"</span></span><br><span class=\"line\">    unique_labels = set(labels)</span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">15</span>, <span class=\"number\">15</span>))</span><br><span class=\"line\">    i = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> label <span class=\"keyword\">in</span> unique_labels:</span><br><span class=\"line\">        <span class=\"comment\"># Pick the first image for each label.</span></span><br><span class=\"line\">        image = images[labels.index(label)]</span><br><span class=\"line\">        plt.subplot(<span class=\"number\">8</span>, <span class=\"number\">8</span>, i)  <span class=\"comment\"># A grid of 8 rows x 8 columns</span></span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        plt.title(<span class=\"string\">\"Label &#123;0&#125; (&#123;1&#125;)\"</span>.format(label, labels.count(label)))</span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\">        plt.imshow(image)</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\">display_images_and_labels(images, labels)</span><br><span class=\"line\">display_images_and_labels(images32, labels)</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/19586062.jpg\" alt=\"png\"></p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/15441432.jpg\" alt=\"png\"></p>\n<p>正如我们在直方图中看到的那样，具有标签 22、32、38 和 61 的交通标志要明显多得多。图中可以看到标签 22 有 375 个实例，标签 32 有 316 实例，标签 38 有 285 个实例，标签 61 有 282 个实例。</p>\n<h4 id=\"1-3-4-显示某一个标签下的交通标志\"><a href=\"#1-3-4-显示某一个标签下的交通标志\" class=\"headerlink\" title=\"1.3.4 显示某一个标签下的交通标志\"></a>1.3.4 显示某一个标签下的交通标志</h4><p>看过每个标签下的第一张图片之后，我们可以将某一个标签下的图片展开显示出来，看看这个标签下的是否是同一类交通标志。我们不需要把该标签下的所有图片都显示出来，可以只展示24张，你可以更改为其他的数字，显示更多或者更少。我们这里选择标签为21的看一下，在之前的图片中可以看到，label 21对应于stop标志。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">display_label_images</span><span class=\"params\">(images, label)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Display images of a specific label.\"\"\"</span></span><br><span class=\"line\">    limit = <span class=\"number\">24</span>  <span class=\"comment\"># show a max of 24 images</span></span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">15</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">    i = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    start = labels.index(label)</span><br><span class=\"line\">    end = start + labels.count(label)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> image <span class=\"keyword\">in</span> images[start:end][:limit]:</span><br><span class=\"line\">        plt.subplot(<span class=\"number\">3</span>, <span class=\"number\">8</span>, i)  <span class=\"comment\"># 3 rows, 8 per row</span></span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\">        plt.imshow(image)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">display_label_images(images32,<span class=\"number\">21</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/52152061.jpg\" alt=\"png\"></p>\n<p>可以看出，label 21对应的前24张图片都是stop标志。不难推测，整个label 21对应的应都是stop标志。</p>\n<h2 id=\"2-构建深度网络\"><a href=\"#2-构建深度网络\" class=\"headerlink\" title=\"2 构建深度网络\"></a>2 构建深度网络</h2><h3 id=\"2-1-构建TensorFlow图并训练\"><a href=\"#2-1-构建TensorFlow图并训练\" class=\"headerlink\" title=\"2.1 构建TensorFlow图并训练\"></a>2.1 构建TensorFlow图并训练</h3><p>首先，我们创建一个Graph对象。TensorFlow有一个默认的全局图，但是我们不建议使用它。设置全局变量通常太容易引入错误了，因此我们自己创建一个图。之后设置占位符来放图片和标签。注意这里参数x的维度是 [None, 32, 32, 3]，这四个参数分别表示 [批量大小，高度，宽度，通道] （通常缩写为 NHWC）。我们定义了一个全连接层，并使用了relu激活函数进行非线性操作。我们通过argmax()函数找到logits最大值对应的索引，也就是预测的标签了。之后定义loss函数，并选择合适的优化算法。这里选择Adam算法，因为它的收敛速度比一般的梯度下降算法更快。这个时候我们只刚刚构建图，并且描述了输入。我们定义的变量，比如，loss和predicted_labels，它们都不包含具体的数值。它们是我们接下来要执行的操作的引用。我们要创建会话才能开始训练。我这里把循环次数设置为301，并且如果i是10的倍数，就打印loss的值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g = tf.Graph()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> g.as_default():</span><br><span class=\"line\">    <span class=\"comment\"># Initialize placeholders </span></span><br><span class=\"line\">    x = tf.placeholder(dtype = tf.float32, shape = [<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">    y = tf.placeholder(dtype = tf.int32, shape = [<span class=\"keyword\">None</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Flatten the input data</span></span><br><span class=\"line\">    images_flat = tf.contrib.layers.flatten(x)</span><br><span class=\"line\">    <span class=\"comment\">#print(images_flat)</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Fully connected layer </span></span><br><span class=\"line\">    logits = tf.contrib.layers.fully_connected(images_flat, <span class=\"number\">62</span>, tf.nn.relu)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># Convert logits to label </span></span><br><span class=\"line\">    predicted_labels = tf.argmax(logits, <span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Define a loss function</span></span><br><span class=\"line\">    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, </span><br><span class=\"line\">                                logits = logits))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Define an optimizer </span></span><br><span class=\"line\">    train_op = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>).minimize(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">\"images_flat: \"</span>, images_flat)</span><br><span class=\"line\">    print(<span class=\"string\">\"logits: \"</span>, logits)</span><br><span class=\"line\">    print(<span class=\"string\">\"loss: \"</span>, loss)</span><br><span class=\"line\">    print(<span class=\"string\">\"predicted_labels: \"</span>, predicted_labels)</span><br><span class=\"line\"></span><br><span class=\"line\">    sess=tf.Session(graph=g)</span><br><span class=\"line\">    sess.run(tf.global_variables_initializer())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">301</span>):</span><br><span class=\"line\">        <span class=\"comment\">#print('EPOCH', i)</span></span><br><span class=\"line\">        _,loss_value = sess.run([train_op, loss], feed_dict=&#123;x: images32, y: labels&#125;) </span><br><span class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Loss: \"</span>, loss_value)</span><br><span class=\"line\">        <span class=\"comment\">#print('DONE WITH EPOCH')</span></span><br></pre></td></tr></table></figure>\n<pre><code>images_flat:  Tensor(&quot;Flatten/flatten/Reshape:0&quot;, shape=(?, 3072), dtype=float32)\nlogits:  Tensor(&quot;fully_connected/Relu:0&quot;, shape=(?, 62), dtype=float32)\nloss:  Tensor(&quot;Mean:0&quot;, shape=(), dtype=float32)\npredicted_labels:  Tensor(&quot;ArgMax:0&quot;, shape=(?,), dtype=int64)\nLoss:  4.181018\nLoss:  3.0714655\nLoss:  2.6622696\nLoss:  2.4586942\nLoss:  2.3419585\nLoss:  2.2633858\nLoss:  2.2044215\nLoss:  2.157206\nLoss:  2.1180305\nLoss:  2.0847433\nLoss:  2.0559382\nLoss:  2.030667\nLoss:  2.008251\nLoss:  1.9882014\nLoss:  1.9701369\nLoss:  1.9537587\nLoss:  1.938837\nLoss:  1.9251733\nLoss:  1.912607\nLoss:  1.9010073\nLoss:  1.8902632\nLoss:  1.8802778\nLoss:  1.8709714\nLoss:  1.8622767\nLoss:  1.8541412\nLoss:  1.8465083\nLoss:  1.8393359\nLoss:  1.8325756\nLoss:  1.8261962\nLoss:  1.8201678\nLoss:  1.8144621\n</code></pre><h3 id=\"2-2使用模型\"><a href=\"#2-2使用模型\" class=\"headerlink\" title=\"2.2使用模型\"></a>2.2使用模型</h3><h3 id=\"2-2使用模型-1\"><a href=\"#2-2使用模型-1\" class=\"headerlink\" title=\"2.2使用模型\"></a>2.2使用模型</h3><p>现在我们用sess.run()来使用我们训练好的模型，并随机取了训练集中的10个图片进行分类，并同时打印了真实的标签结果和预测结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Pick 10 random images</span></span><br><span class=\"line\">sample_indexes = random.sample(range(len(images32)), <span class=\"number\">10</span>)</span><br><span class=\"line\">sample_images = [images32[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_indexes]</span><br><span class=\"line\">sample_labels = [labels[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_indexes]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Run the \"predicted_labels\" op.</span></span><br><span class=\"line\">predicted = sess.run([predicted_labels], </span><br><span class=\"line\">                        feed_dict=&#123;x: sample_images&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\">print(sample_labels)</span><br><span class=\"line\">print(predicted)</span><br></pre></td></tr></table></figure>\n<pre><code>[41, 39, 1, 53, 21, 22, 38, 48, 7, 53]\n[41 39  1 53 21 22 40 47  7 53]\n</code></pre><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">​```python</span><br><span class=\"line\">fig=plt.figure(figsize=(<span class=\"number\">10</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(sample_images)):</span><br><span class=\"line\">    truth = sample_labels[i]</span><br><span class=\"line\">    prediction = predicted[i]</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>+i)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">    color=<span class=\"string\">'green'</span> <span class=\"keyword\">if</span> truth == prediction <span class=\"keyword\">else</span> <span class=\"string\">'red'</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">40</span>,<span class=\"number\">10</span>,<span class=\"string\">\"Truth:        &#123;0&#125;\\nPrediction: &#123;1&#125;\"</span>.format(truth, prediction), </span><br><span class=\"line\">             fontsize=<span class=\"number\">12</span>, color=color)</span><br><span class=\"line\">    plt.imshow(sample_images[i])</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94071068.jpg\" alt=\"png\"></p>\n<h3 id=\"2-3评估模型\"><a href=\"#2-3评估模型\" class=\"headerlink\" title=\"2.3评估模型\"></a>2.3评估模型</h3><p>以上，我们的模型只在训练集上是可以正常运行的，但是它对于其他的未知数据集的泛化能力如何呢？我们可以在测试集当中进行评估。我们还可以计算一下准确率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test_images, test_labels = load_data(test_data_dir)</span><br><span class=\"line\">test_images32 = [transform.resize(image, (<span class=\"number\">32</span>, <span class=\"number\">32</span>))</span><br><span class=\"line\">                 <span class=\"keyword\">for</span> image <span class=\"keyword\">in</span> test_images]</span><br><span class=\"line\">display_images_and_labels(test_images32, test_labels)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Calculate how many matches we got.</span></span><br><span class=\"line\">predicted = sess.run([predicted_labels], </span><br><span class=\"line\">                        feed_dict=&#123;x: test_images32&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\">match_count = sum([int(y == y_) </span><br><span class=\"line\">                   <span class=\"keyword\">for</span> y, y_ <span class=\"keyword\">in</span> zip(test_labels, predicted)])</span><br><span class=\"line\">accuracy = match_count / len(test_labels)</span><br><span class=\"line\">print(<span class=\"string\">\"Accuracy: &#123;:.4f&#125;\"</span>.format(accuracy))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Pick 10 random images</span></span><br><span class=\"line\">sample_test_indexes = random.sample(range(len(test_images32)), <span class=\"number\">10</span>)</span><br><span class=\"line\">sample_test_images = [test_images32[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_test_indexes]</span><br><span class=\"line\">sample_test_labels = [test_labels[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_test_indexes]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Run the \"predicted_labels\" op.</span></span><br><span class=\"line\">test_predicted = sess.run([predicted_labels], </span><br><span class=\"line\">                        feed_dict=&#123;x: sample_test_images&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\">print(sample_test_labels)</span><br><span class=\"line\">print(test_predicted)</span><br><span class=\"line\"></span><br><span class=\"line\">fig=plt.figure(figsize=(<span class=\"number\">10</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(sample_test_images)):</span><br><span class=\"line\">    truth = sample_test_labels[i]</span><br><span class=\"line\">    prediction = test_predicted[i]</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>+i)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">    color=<span class=\"string\">'green'</span> <span class=\"keyword\">if</span> truth == prediction <span class=\"keyword\">else</span> <span class=\"string\">'red'</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">40</span>,<span class=\"number\">10</span>,<span class=\"string\">\"Truth:        &#123;0&#125;\\nPrediction: &#123;1&#125;\"</span>.format(truth, prediction), </span><br><span class=\"line\">             fontsize=<span class=\"number\">12</span>, color=color)</span><br><span class=\"line\">    plt.imshow(sample_test_images[i])</span><br></pre></td></tr></table></figure>\n<pre><code>/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in skimage 0.15.\n  warn(&quot;The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in &quot;\n/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;\n\n\nAccuracy: 0.5631\n[38, 35, 19, 32, 32, 7, 13, 38, 18, 38]\n[39  0 19 32 32  7 13 40 17 39]\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/67359700.jpg\" alt=\"png\"></p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/73349594.jpg\" alt=\"png\"></p>\n<h3 id=\"2-4关闭会话\"><a href=\"#2-4关闭会话\" class=\"headerlink\" title=\"2.4关闭会话\"></a>2.4关闭会话</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sess.close()</span><br></pre></td></tr></table></figure>\n<p>最后，记得关闭会话。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>本文参考<a href=\"https://juejin.im/entry/5a1637f2f265da432528f6ef\" target=\"_blank\" rel=\"noopener\">https://juejin.im/entry/5a1637f2f265da432528f6ef</a>  的文章和  <a href=\"https://github.com/waleedka/traffic-signs-tensorflow\" target=\"_blank\" rel=\"noopener\">https://github.com/waleedka/traffic-signs-tensorflow</a>  的源代码。   </p>\n<p> 给定交通标志的图像，我们的模型应该能够知道它的类型。<br> 首先我们要导入需要的库。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> skimage <span class=\"keyword\">import</span> data</span><br><span class=\"line\"><span class=\"keyword\">from</span> skimage <span class=\"keyword\">import</span> transform</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br></pre></td></tr></table></figure>\n<pre><code>/home/song/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n</code></pre>","more":"<h2 id=\"1-加载数据和分析数据\"><a href=\"#1-加载数据和分析数据\" class=\"headerlink\" title=\"1  加载数据和分析数据\"></a>1  加载数据和分析数据</h2><h3 id=\"1-1-加载数据\"><a href=\"#1-1-加载数据\" class=\"headerlink\" title=\"1.1 加载数据\"></a>1.1 加载数据</h3><p>我们使用的是Belgian Traffic Sign Dataset。网址为<a href=\"http://btsd.ethz.ch/shareddata/\" target=\"_blank\" rel=\"noopener\">http://btsd.ethz.ch/shareddata/</a><br>在这个网站可以下载到我们需要的数据集。你只需要下载BelgiumTS for Classification (cropped images):后面的两个数据集：  </p>\n<pre><code>BelgiumTSC_Training (171.3MBytes)  \nBelgiumTSC_Testing (76.5MBytes)  \n</code></pre><p>  我把这两个数据集分别放在了以下的路径：    </p>\n<pre><code>/home/song/Downloads/BelgiumTSC_Training/Training  \n/home/song/Downloads/BelgiumTSC_Testing/Testing  \n</code></pre><p>  Training目录包含具有从00000到00061的序列号的子目录。目录名称表示从0到61的标签，每个目录中的图像表示属于该标签的交通标志。 图像以不常见的.ppm格式保存，但幸运的是，这种格式在skimage库中得到了支持。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">load_data</span><span class=\"params\">(data_dir)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># Get all subdirectories of data_dir. Each represents a label.</span></span><br><span class=\"line\">    directories = [d <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> os.listdir(data_dir)</span><br><span class=\"line\">                  <span class=\"keyword\">if</span> os.path.isdir(os.path.join(data_dir, d))]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Loop through the label directories and collect the data in</span></span><br><span class=\"line\">    <span class=\"comment\"># two lists, labels and images.</span></span><br><span class=\"line\">    labels = []</span><br><span class=\"line\">    images = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> d <span class=\"keyword\">in</span> directories:</span><br><span class=\"line\">        label_dir = os.path.join(data_dir, d)</span><br><span class=\"line\">        file_names = [os.path.join(label_dir, f) </span><br><span class=\"line\">                      <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> os.listdir(label_dir) </span><br><span class=\"line\">                      <span class=\"keyword\">if</span> f.endswith(<span class=\"string\">\".ppm\"</span>)]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> f <span class=\"keyword\">in</span> file_names:</span><br><span class=\"line\">            images.append(data.imread(f))</span><br><span class=\"line\">            labels.append(int(d))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> images, labels</span><br><span class=\"line\"></span><br><span class=\"line\">ROOT_PATH = <span class=\"string\">\"/home/song/Downloads/\"</span></span><br><span class=\"line\">train_data_dir = os.path.join(ROOT_PATH, <span class=\"string\">\"BelgiumTSC_Training/Training\"</span>)</span><br><span class=\"line\">test_data_dir = os.path.join(ROOT_PATH, <span class=\"string\">\"BelgiumTSC_Testing/Testing\"</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">images, labels = load_data(train_data_dir)</span><br></pre></td></tr></table></figure>\n<h3 id=\"1-2-分析数据\"><a href=\"#1-2-分析数据\" class=\"headerlink\" title=\"1.2 分析数据\"></a>1.2 分析数据</h3><p>我们可以看一下我们的训练集中有多少图片和标签：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(<span class=\"string\">\"Unique Labels: &#123;0&#125;\\nTotal Images: &#123;1&#125;\"</span>.format(len(set(labels)), len(images)))</span><br></pre></td></tr></table></figure>\n<pre><code>Unique Labels: 62\nTotal Images: 4575\n</code></pre><p>这里的set很有意思，可以看一下这篇文章：<a href=\"http://www.voidcn.com/article/p-uekeyeby-hn.html\" target=\"_blank\" rel=\"noopener\">http://www.voidcn.com/article/p-uekeyeby-hn.html</a><br>这里的set很有意思，可以看一下这篇文章：<a href=\"http://www.voidcn.com/article/p-uekeyeby-hn.html\" target=\"_blank\" rel=\"noopener\">http://www.voidcn.com/article/p-uekeyeby-hn.html</a><br>在处理一系列数据时，如果需要剔除重复项，则通常采用set数据类型。本身labels里面是有很多重复的元素的，但set(labels)就剔除了重复项。可以通过print(labels)和print(set(labels))命令查看一下两者输出的有什么区别。<br>我们还可以通过画直方图来看一下数据的分布情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.hist(labels,<span class=\"number\">62</span>)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/58270023.jpg\" alt=\"png\"></p>\n<p>可以看出，该数据集中有的标签的分量比其它标签更重：标签 22、32、38 和 61 显然出类拔萃。这一点之后我们会更深入地了解。</p>\n<h3 id=\"1-3-可视化数据\"><a href=\"#1-3-可视化数据\" class=\"headerlink\" title=\"1.3 可视化数据\"></a>1.3 可视化数据</h3><h4 id=\"1-3-1-热身\"><a href=\"#1-3-1-热身\" class=\"headerlink\" title=\"1.3.1 热身\"></a>1.3.1 热身</h4><p>我们可以先随机地选取几个交通标志将其显示出来。我们还可以看一下图片的尺寸。我们还可以看一下图片的最小值和最大值，这是验证数据范围并及早发现错误的一个简单方法。其中的plt.axis(‘off’)是为了不在图片上显示坐标尺，大家可以注释掉这句话看看如果去掉有什么不一样。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">traffic_signs=[<span class=\"number\">100</span>,<span class=\"number\">1050</span>,<span class=\"number\">3650</span>,<span class=\"number\">4000</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(traffic_signs)):</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">4</span>, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">    plt.imshow(images[traffic_signs[i]])</span><br><span class=\"line\">    <span class=\"comment\">#plt.subplots_adjust(wspace=0.5)</span></span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    print(<span class=\"string\">\"shape: &#123;0&#125;, min: &#123;1&#125;, max: &#123;2&#125;\"</span>.format(images[traffic_signs[i]].shape, </span><br><span class=\"line\">                                                  images[traffic_signs[i]].min(), </span><br><span class=\"line\">                                                  images[traffic_signs[i]].max()))</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/91631184.jpg\" alt=\"png\"></p>\n<pre><code>shape: (292, 290, 3), min: 0, max: 255\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94366993.jpg\" alt=\"png\"></p>\n<pre><code>shape: (132, 139, 3), min: 4, max: 255\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/55563405.jpg\" alt=\"png\"></p>\n<pre><code>shape: (146, 110, 3), min: 7, max: 255\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/66096551.jpg\" alt=\"png\"></p>\n<pre><code>shape: (110, 105, 3), min: 3, max: 255\n</code></pre><p>大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32<em>32。<br>大多数神经网络需要固定大小的输入，我们的网络也不例外。 但正如我们上面所看到的，我们的图像大小并不完全相同。 一种常见的方法是将图像裁剪并填充到选定的纵横比，但是我们必须确保在这个过程中我们不会切断部分交通标志。 这似乎需要进行手动操作！ 我们其实有一个更简单的解决方案，即我们将图像大小调整为固定大小，并忽略由不同长宽比导致的失真。 这时，即使图片被压缩或拉伸了一点，我们也可以很容易地识别交通标志。我们用下面的命令将图片的尺寸调整为32</em>32。</p>\n<h4 id=\"1-3-2-重调图片的大小\"><a href=\"#1-3-2-重调图片的大小\" class=\"headerlink\" title=\"1.3.2 重调图片的大小\"></a>1.3.2 重调图片的大小</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">images32 = [transform.resize(image,(<span class=\"number\">32</span>,<span class=\"number\">32</span>)) <span class=\"keyword\">for</span> image <span class=\"keyword\">in</span> images]</span><br></pre></td></tr></table></figure>\n<pre><code>/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in skimage 0.15.\n  warn(&quot;The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in &quot;\n/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;\n</code></pre><p>重新运行上面随机显示交通标志的代码。<br>重新运行上面随机显示交通标志的代码。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">traffic_signs=[<span class=\"number\">100</span>,<span class=\"number\">1050</span>,<span class=\"number\">3650</span>,<span class=\"number\">4000</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(traffic_signs)):</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">1</span>, <span class=\"number\">4</span>, i+<span class=\"number\">1</span>)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">    plt.imshow(images32[traffic_signs[i]])</span><br><span class=\"line\">    plt.subplots_adjust(wspace=<span class=\"number\">0.5</span>)</span><br><span class=\"line\">    plt.show()</span><br><span class=\"line\">    print(<span class=\"string\">\"shape: &#123;0&#125;, min: &#123;1&#125;, max: &#123;2&#125;\"</span>.format(images32[traffic_signs[i]].shape, </span><br><span class=\"line\">                                                  images32[traffic_signs[i]].min(), </span><br><span class=\"line\">                                                  images32[traffic_signs[i]].max()))</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/65434369.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.0, max: 1.0\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/59269206.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.038373161764705975, max: 1.0\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/62497379.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.05559895833333348, max: 1.0\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/90866008.jpg\" alt=\"png\"></p>\n<pre><code>shape: (32, 32, 3), min: 0.048665364583333495, max: 1.0\n</code></pre><p>从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。<br>从上面的图和shape的值都能看出，图片的尺寸一样大了。最小值和最大值现在的范围在0和1.0之间，和我们未调整图片大小时的范围不同。</p>\n<h4 id=\"1-3-3-显示每一个标签下的第一张图片\"><a href=\"#1-3-3-显示每一个标签下的第一张图片\" class=\"headerlink\" title=\"1.3.3 显示每一个标签下的第一张图片\"></a>1.3.3 显示每一个标签下的第一张图片</h4><p>之前我们在直方图中看过62个标签的分布情况。现在我们尝试将每个标签下的第一张图片显示出来，另外还可以通过列表的count()方法来统计某个标签出现的次数，也就是能统计出有多少张图片对应该标签。我们可以定义一个函数，名为display_images_and_labels，你当然可以定义成别的名字，不过定义函数是为了之后可以方便地调用。以下分别显示出了未调整尺寸和已调整尺寸的交通标志图。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">display_images_and_labels</span><span class=\"params\">(images, labels)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Display the first image of each label.\"\"\"</span></span><br><span class=\"line\">    unique_labels = set(labels)</span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">15</span>, <span class=\"number\">15</span>))</span><br><span class=\"line\">    i = <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> label <span class=\"keyword\">in</span> unique_labels:</span><br><span class=\"line\">        <span class=\"comment\"># Pick the first image for each label.</span></span><br><span class=\"line\">        image = images[labels.index(label)]</span><br><span class=\"line\">        plt.subplot(<span class=\"number\">8</span>, <span class=\"number\">8</span>, i)  <span class=\"comment\"># A grid of 8 rows x 8 columns</span></span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        plt.title(<span class=\"string\">\"Label &#123;0&#125; (&#123;1&#125;)\"</span>.format(label, labels.count(label)))</span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\">        plt.imshow(image)</span><br><span class=\"line\">    </span><br><span class=\"line\"></span><br><span class=\"line\">display_images_and_labels(images, labels)</span><br><span class=\"line\">display_images_and_labels(images32, labels)</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/19586062.jpg\" alt=\"png\"></p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/15441432.jpg\" alt=\"png\"></p>\n<p>正如我们在直方图中看到的那样，具有标签 22、32、38 和 61 的交通标志要明显多得多。图中可以看到标签 22 有 375 个实例，标签 32 有 316 实例，标签 38 有 285 个实例，标签 61 有 282 个实例。</p>\n<h4 id=\"1-3-4-显示某一个标签下的交通标志\"><a href=\"#1-3-4-显示某一个标签下的交通标志\" class=\"headerlink\" title=\"1.3.4 显示某一个标签下的交通标志\"></a>1.3.4 显示某一个标签下的交通标志</h4><p>看过每个标签下的第一张图片之后，我们可以将某一个标签下的图片展开显示出来，看看这个标签下的是否是同一类交通标志。我们不需要把该标签下的所有图片都显示出来，可以只展示24张，你可以更改为其他的数字，显示更多或者更少。我们这里选择标签为21的看一下，在之前的图片中可以看到，label 21对应于stop标志。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">display_label_images</span><span class=\"params\">(images, label)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"Display images of a specific label.\"\"\"</span></span><br><span class=\"line\">    limit = <span class=\"number\">24</span>  <span class=\"comment\"># show a max of 24 images</span></span><br><span class=\"line\">    plt.figure(figsize=(<span class=\"number\">15</span>, <span class=\"number\">5</span>))</span><br><span class=\"line\">    i = <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    start = labels.index(label)</span><br><span class=\"line\">    end = start + labels.count(label)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> image <span class=\"keyword\">in</span> images[start:end][:limit]:</span><br><span class=\"line\">        plt.subplot(<span class=\"number\">3</span>, <span class=\"number\">8</span>, i)  <span class=\"comment\"># 3 rows, 8 per row</span></span><br><span class=\"line\">        plt.axis(<span class=\"string\">'off'</span>)</span><br><span class=\"line\">        i += <span class=\"number\">1</span></span><br><span class=\"line\">        plt.imshow(image)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">display_label_images(images32,<span class=\"number\">21</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/52152061.jpg\" alt=\"png\"></p>\n<p>可以看出，label 21对应的前24张图片都是stop标志。不难推测，整个label 21对应的应都是stop标志。</p>\n<h2 id=\"2-构建深度网络\"><a href=\"#2-构建深度网络\" class=\"headerlink\" title=\"2 构建深度网络\"></a>2 构建深度网络</h2><h3 id=\"2-1-构建TensorFlow图并训练\"><a href=\"#2-1-构建TensorFlow图并训练\" class=\"headerlink\" title=\"2.1 构建TensorFlow图并训练\"></a>2.1 构建TensorFlow图并训练</h3><p>首先，我们创建一个Graph对象。TensorFlow有一个默认的全局图，但是我们不建议使用它。设置全局变量通常太容易引入错误了，因此我们自己创建一个图。之后设置占位符来放图片和标签。注意这里参数x的维度是 [None, 32, 32, 3]，这四个参数分别表示 [批量大小，高度，宽度，通道] （通常缩写为 NHWC）。我们定义了一个全连接层，并使用了relu激活函数进行非线性操作。我们通过argmax()函数找到logits最大值对应的索引，也就是预测的标签了。之后定义loss函数，并选择合适的优化算法。这里选择Adam算法，因为它的收敛速度比一般的梯度下降算法更快。这个时候我们只刚刚构建图，并且描述了输入。我们定义的变量，比如，loss和predicted_labels，它们都不包含具体的数值。它们是我们接下来要执行的操作的引用。我们要创建会话才能开始训练。我这里把循环次数设置为301，并且如果i是10的倍数，就打印loss的值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">g = tf.Graph()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">with</span> g.as_default():</span><br><span class=\"line\">    <span class=\"comment\"># Initialize placeholders </span></span><br><span class=\"line\">    x = tf.placeholder(dtype = tf.float32, shape = [<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>,<span class=\"number\">3</span>])</span><br><span class=\"line\">    y = tf.placeholder(dtype = tf.int32, shape = [<span class=\"keyword\">None</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Flatten the input data</span></span><br><span class=\"line\">    images_flat = tf.contrib.layers.flatten(x)</span><br><span class=\"line\">    <span class=\"comment\">#print(images_flat)</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Fully connected layer </span></span><br><span class=\"line\">    logits = tf.contrib.layers.fully_connected(images_flat, <span class=\"number\">62</span>, tf.nn.relu)</span><br><span class=\"line\"></span><br><span class=\"line\">     <span class=\"comment\"># Convert logits to label </span></span><br><span class=\"line\">    predicted_labels = tf.argmax(logits, <span class=\"number\">1</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># Define a loss function</span></span><br><span class=\"line\">    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, </span><br><span class=\"line\">                                logits = logits))</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># Define an optimizer </span></span><br><span class=\"line\">    train_op = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>).minimize(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    print(<span class=\"string\">\"images_flat: \"</span>, images_flat)</span><br><span class=\"line\">    print(<span class=\"string\">\"logits: \"</span>, logits)</span><br><span class=\"line\">    print(<span class=\"string\">\"loss: \"</span>, loss)</span><br><span class=\"line\">    print(<span class=\"string\">\"predicted_labels: \"</span>, predicted_labels)</span><br><span class=\"line\"></span><br><span class=\"line\">    sess=tf.Session(graph=g)</span><br><span class=\"line\">    sess.run(tf.global_variables_initializer())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">301</span>):</span><br><span class=\"line\">        <span class=\"comment\">#print('EPOCH', i)</span></span><br><span class=\"line\">        _,loss_value = sess.run([train_op, loss], feed_dict=&#123;x: images32, y: labels&#125;) </span><br><span class=\"line\">        <span class=\"keyword\">if</span> i % <span class=\"number\">10</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">\"Loss: \"</span>, loss_value)</span><br><span class=\"line\">        <span class=\"comment\">#print('DONE WITH EPOCH')</span></span><br></pre></td></tr></table></figure>\n<pre><code>images_flat:  Tensor(&quot;Flatten/flatten/Reshape:0&quot;, shape=(?, 3072), dtype=float32)\nlogits:  Tensor(&quot;fully_connected/Relu:0&quot;, shape=(?, 62), dtype=float32)\nloss:  Tensor(&quot;Mean:0&quot;, shape=(), dtype=float32)\npredicted_labels:  Tensor(&quot;ArgMax:0&quot;, shape=(?,), dtype=int64)\nLoss:  4.181018\nLoss:  3.0714655\nLoss:  2.6622696\nLoss:  2.4586942\nLoss:  2.3419585\nLoss:  2.2633858\nLoss:  2.2044215\nLoss:  2.157206\nLoss:  2.1180305\nLoss:  2.0847433\nLoss:  2.0559382\nLoss:  2.030667\nLoss:  2.008251\nLoss:  1.9882014\nLoss:  1.9701369\nLoss:  1.9537587\nLoss:  1.938837\nLoss:  1.9251733\nLoss:  1.912607\nLoss:  1.9010073\nLoss:  1.8902632\nLoss:  1.8802778\nLoss:  1.8709714\nLoss:  1.8622767\nLoss:  1.8541412\nLoss:  1.8465083\nLoss:  1.8393359\nLoss:  1.8325756\nLoss:  1.8261962\nLoss:  1.8201678\nLoss:  1.8144621\n</code></pre><h3 id=\"2-2使用模型\"><a href=\"#2-2使用模型\" class=\"headerlink\" title=\"2.2使用模型\"></a>2.2使用模型</h3><h3 id=\"2-2使用模型-1\"><a href=\"#2-2使用模型-1\" class=\"headerlink\" title=\"2.2使用模型\"></a>2.2使用模型</h3><p>现在我们用sess.run()来使用我们训练好的模型，并随机取了训练集中的10个图片进行分类，并同时打印了真实的标签结果和预测结果。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Pick 10 random images</span></span><br><span class=\"line\">sample_indexes = random.sample(range(len(images32)), <span class=\"number\">10</span>)</span><br><span class=\"line\">sample_images = [images32[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_indexes]</span><br><span class=\"line\">sample_labels = [labels[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_indexes]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Run the \"predicted_labels\" op.</span></span><br><span class=\"line\">predicted = sess.run([predicted_labels], </span><br><span class=\"line\">                        feed_dict=&#123;x: sample_images&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\">print(sample_labels)</span><br><span class=\"line\">print(predicted)</span><br></pre></td></tr></table></figure>\n<pre><code>[41, 39, 1, 53, 21, 22, 38, 48, 7, 53]\n[41 39  1 53 21 22 40 47  7 53]\n</code></pre><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">​```python</span><br><span class=\"line\">fig=plt.figure(figsize=(<span class=\"number\">10</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(sample_images)):</span><br><span class=\"line\">    truth = sample_labels[i]</span><br><span class=\"line\">    prediction = predicted[i]</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>+i)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">    color=<span class=\"string\">'green'</span> <span class=\"keyword\">if</span> truth == prediction <span class=\"keyword\">else</span> <span class=\"string\">'red'</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">40</span>,<span class=\"number\">10</span>,<span class=\"string\">\"Truth:        &#123;0&#125;\\nPrediction: &#123;1&#125;\"</span>.format(truth, prediction), </span><br><span class=\"line\">             fontsize=<span class=\"number\">12</span>, color=color)</span><br><span class=\"line\">    plt.imshow(sample_images[i])</span><br></pre></td></tr></table></figure>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/94071068.jpg\" alt=\"png\"></p>\n<h3 id=\"2-3评估模型\"><a href=\"#2-3评估模型\" class=\"headerlink\" title=\"2.3评估模型\"></a>2.3评估模型</h3><p>以上，我们的模型只在训练集上是可以正常运行的，但是它对于其他的未知数据集的泛化能力如何呢？我们可以在测试集当中进行评估。我们还可以计算一下准确率。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">test_images, test_labels = load_data(test_data_dir)</span><br><span class=\"line\">test_images32 = [transform.resize(image, (<span class=\"number\">32</span>, <span class=\"number\">32</span>))</span><br><span class=\"line\">                 <span class=\"keyword\">for</span> image <span class=\"keyword\">in</span> test_images]</span><br><span class=\"line\">display_images_and_labels(test_images32, test_labels)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Calculate how many matches we got.</span></span><br><span class=\"line\">predicted = sess.run([predicted_labels], </span><br><span class=\"line\">                        feed_dict=&#123;x: test_images32&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\">match_count = sum([int(y == y_) </span><br><span class=\"line\">                   <span class=\"keyword\">for</span> y, y_ <span class=\"keyword\">in</span> zip(test_labels, predicted)])</span><br><span class=\"line\">accuracy = match_count / len(test_labels)</span><br><span class=\"line\">print(<span class=\"string\">\"Accuracy: &#123;:.4f&#125;\"</span>.format(accuracy))</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Pick 10 random images</span></span><br><span class=\"line\">sample_test_indexes = random.sample(range(len(test_images32)), <span class=\"number\">10</span>)</span><br><span class=\"line\">sample_test_images = [test_images32[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_test_indexes]</span><br><span class=\"line\">sample_test_labels = [test_labels[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> sample_test_indexes]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Run the \"predicted_labels\" op.</span></span><br><span class=\"line\">test_predicted = sess.run([predicted_labels], </span><br><span class=\"line\">                        feed_dict=&#123;x: sample_test_images&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\">print(sample_test_labels)</span><br><span class=\"line\">print(test_predicted)</span><br><span class=\"line\"></span><br><span class=\"line\">fig=plt.figure(figsize=(<span class=\"number\">10</span>,<span class=\"number\">10</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(sample_test_images)):</span><br><span class=\"line\">    truth = sample_test_labels[i]</span><br><span class=\"line\">    prediction = test_predicted[i]</span><br><span class=\"line\">    plt.subplot(<span class=\"number\">5</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>+i)</span><br><span class=\"line\">    plt.axis(<span class=\"string\">\"off\"</span>)</span><br><span class=\"line\">    color=<span class=\"string\">'green'</span> <span class=\"keyword\">if</span> truth == prediction <span class=\"keyword\">else</span> <span class=\"string\">'red'</span></span><br><span class=\"line\">    plt.text(<span class=\"number\">40</span>,<span class=\"number\">10</span>,<span class=\"string\">\"Truth:        &#123;0&#125;\\nPrediction: &#123;1&#125;\"</span>.format(truth, prediction), </span><br><span class=\"line\">             fontsize=<span class=\"number\">12</span>, color=color)</span><br><span class=\"line\">    plt.imshow(sample_test_images[i])</span><br></pre></td></tr></table></figure>\n<pre><code>/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in skimage 0.15.\n  warn(&quot;The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in &quot;\n/home/song/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;\n\n\nAccuracy: 0.5631\n[38, 35, 19, 32, 32, 7, 13, 38, 18, 38]\n[39  0 19 32 32  7 13 40 17 39]\n</code></pre><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/67359700.jpg\" alt=\"png\"></p>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-9-13/73349594.jpg\" alt=\"png\"></p>\n<h3 id=\"2-4关闭会话\"><a href=\"#2-4关闭会话\" class=\"headerlink\" title=\"2.4关闭会话\"></a>2.4关闭会话</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sess.close()</span><br></pre></td></tr></table></figure>\n<p>最后，记得关闭会话。</p>"},{"title":"异常行为检测文献整理(顶会顶刊)","date":"2018-11-11T05:25:57.000Z","comments":1,"_content":"\n## 2018\n\n### real-world anomaly detection in surveillance videos【有代码】\n\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/89215767.jpg)\n**使用的方法：**MIL（multiple instance learning）多示例学习\n\n**方法步骤：**\n（1）     positive(某一部分包含异常)，negative（不包含异常）视频。把positive和negative视频分别分成固定数量的segments。每个视频表示为一个包，每个temporal segment表示包里的一个instance。\n（2）     对video segments提取C3D features。\n（3）     用一个novel ranking loss function（positive bag和negative bag中，在最高分数的instances之间计算ranking loss）来训练一个全连接神经网络。\n简言之就是数据处理、提特征【提取到的特征应该是时空特征吧】、训练网络、通过得分预测是否异常。\n**创新点：**同时利用正常和异常的视频来学习异常行为。不需要标记训练视频中的异常segments or clips（非常浪费时间），而是利用弱标记（weakly labeled）的训练视频，通过deep multiple instance ranking framework来学习异常。视频标记（异常或正常）是video-level，而不是clip-level的。我们把正常和异常的视频看作是bags，把video segments看作是instances。【采用MIL的方法引入到异常检测中来】\n另外，对ranking loss function引入了sparsity和temporal smoothness constraints 来在训练中更好的定位异常。\n（有新的数据集）\n**关键词：**weakly-supervised learning，MIL\n**针对问题：**1**.**其他的方法都是假设偏离正常的行为就是异常。但是这样假设是有问题的，因为把所有可能的正常行为考虑进去是不太可能的。2.正常和异常之间的界限是模糊的。在现实场景中，是否异常可能和条件的不同有关。\n**Baseline methods：**C3D,TCNN（这两种方法在数据集上的效果很差，证明提出来的数据集非常challenging）\n**比较：**主要和Learning temporal regularity in video sequences和Abnormal event detection at 150 fps in matlab的方法比较。\n【the first to formulate the video anomaly detection problem in the context of MIL】\n**个人感想与总结：**（采用了什么方法，达到了什么效果，还有什么不太好的地方可以改进）作者采用MIL方法，同时利用正常和异常的视频，使用提出的deep MIL ranking loss来进行异常检测。\n\n<!--more-->\n\n### Future Frame Prediction for anomaly detection-a new baseline\n\n在视频预测框架中解决异常检测问题。除了加spatial（**Appearance**）约束还加了temporal（**motion**）约束（光流）。也用到了GAN。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/34727984.jpg)\n\n## 2017\n\n### Joint detection and recounting of abnormal events by learning deep generic knowledge\n\n**创新点：**把检测和描述视频中的异常事件联合起来。Recounting of abnormal events,就是解释为什么他们是异常的。我们把一个generic CNN model和environment-dependent anomaly detection融合起来。\n（异常检测是有场景依赖性的）【动作理解动作识别的方法能不能用上？】\n**关键词：**anomaly detector\n**方法：**based on multi-task Fast R-CNN\n\n1. 用大量带标签的数据集来学习multi-task Fast R-CNN，学习到generic model。这样提取出deep features 和visual concept classification scores（同时提出的）。\n2. Anomaly detectors 对每个环境在这些特征和分数上学习 \n\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/86054775.jpg)\n【the first step in abnormal event detection using generic knowledge of visual concepts】\n方法简言之就是提semantic特征（怎么感觉作者提的是appearance特征）、判异常/正常、recount，**我们的方法不需要使用motion features。**\n**个人感想：**semantic feature和appearance feature 之间有什么区别？\n\n### unmasking the abnormal events in video\n\n**创新点：**不需要training sequences，我们的网络基于unmasking，是之前用来在文本文件中做授权认证的。\n【the first work to apply unmasking for a computer vision task】\n作者和【6 （2016）A Discriminative Framework for Anomaly Detection in Large Videos】还有一些监督的方法进行比较。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/51723764.jpg)\n\n还是要提特征\n**作者说：**他们采用**了融合运动和外观特征的方法**，但是没有看到大量的改善，需要进一步改进融合的方法。\n\n### abnormal event detection in videos using generative adversarial nets\n\n**方法：**用正常的帧和对应的光流图来训练GAN，来学习正常场景的internal  representation。在测试的时候把真实的数据和GAN产生的外观和运动表示比较，通过计算local 不同来检测异常区域。\n从raw-pixel frames**产生光流图**。\n\n### a revisit of sparse coding based anomaly detection in stacked RNN framework\n\n**摘要：**提出了TSC（Temporally-coherent sparse coding），enforce 相似的相邻帧用相似的重建系数编码。之后用srnn映射TSC,方便了参数优化加速了异常预测。用sRNN同时学习所有参数，能够避免TSC的non-trivial的超参数选择。另外用浅层的sRNN，重建稀疏系数可以在前向传播中推断出来，节约了计算成本。\n**创新点：**（1）提了TSC（2）提了新数据集\n**baseline的缺点：**2016conv-AE基于3D ConvNet，但是之前的工作表明用双流网络分别提取外观和运动信息是视频中特征的提取的一个better solution。 而且conv-AE的输入是video cube，需要在所有的帧上对数据集进行中心采样，计算代价大。\n**方法：**学习能够编码外观上的正常行为的字典，之后，为了提高在相邻帧的预测的平滑性，加上了一个temporally-coherent term。（作者说，有意思的是得到的TSC的公式可以看成是一个特殊的sRNN）。\n**评估：**作者有个**挺好的**想法，先用一个Synthesized Dataset评估自己的方法对于外观的突然变化导致的异常的表现如何。这个数据集是这样做的：从MINIST里面随意找两个数字，然后把他们放在225x225尺寸的黑背景中。然后在之后的19帧里，这两个数字随意的横向纵向运动。训练的时候用了10000个序列，对于每个测试的序列，5个连续的帧由随意插入的3x3白色的小方块随意的遮挡。测试集一共有3000个序列。如下图所示：\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/24681967.jpg)\n\n## 2016\n\n### Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection\n\n【the first work to employ the existing CNN models for motion representation in crowd analysis】\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/95069565.jpg)\n**创新点：**通过将semantic information（从已有的CNN中得到）和low-level optical-flow结合来measure local abnormality 。不需要fine-tuning阶段。\nTrack the changes in the CNN features **across time.**\n（1）     引入了一个新的Binary Quantization Layer\n（2）     提出了一个Temporal CNN Pattern measure 来表示人群中的运动。\n（无监督的方法比监督方法在异常检测上更好，因为标注的主观性和训练数据少）\n**方法步骤：**\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97738054.jpg)\n1.从输入的视频帧序列中提取CNN-based binary maps.**具体来说是所有的帧输入到一个FCN，把一个binary layer 插在FCN的顶部为了把高维的特征图量化成压缩的二值模式。这个binary layer是一个卷积层，当时权重是用一个external hashing method来初始化的。\n**2.用提到的CNN-based binary maps来计算Temporal CNN Pattern值。**先根据binary maps来计算histograms。然后根据这些histograms计算TCP。【TCP measure 是用来表示人群的运动的motion representation】\n**3.将TCP值和低层次的运动特征（光流）来找到refined motion segments。**\n\n**关键词：**BFCN、TCP\n\n**个人感想与总结：**跟踪随着时间变化的CNN 特征。\n\n### learning temporal regularity in video sequences\n\n**创新点：**提出基于自编码器的方法。\n学习正常行为模式very limited supervision。Reconstruction\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/98412371.jpg)\n第一种Learning Motions on Handcrafted Features。首先用HOG和HOF作为时空appearance feature 描述子。\n**有意思的地方：**4.4节predicting the Regular Past and the Future。给中间的帧，能预测near过去的和未来的帧。（预测过去有什么用？）\n\n### A discriminative framework for anomaly detection in large videos\n\n训练序列不可用，并且异常的分数是独立于时间顺序的。\n\n## 2015\n\n### crowd motion monitoring using tracklet-based commotion measure\n\n无监督方法unsupervised context-dependent statistical commotion measure.\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/85776732.jpg)\n\n### Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression\n\n通过层级框架来**检测局部和全局的异常**。通过层级特征表示和GPR（高斯过程回归）。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/35454189.jpg)\n\n## 2014\n\n### Anomaly Detection and Localization in Crowded Scenes\n\n考虑异常行为的检测和定位。提出了同时检测时空异常的detector。\nTemporal normalcy用MDT(mixtures of dynamic textures)建模，spatial normalcy由基于MDT的一个discriminant saliency 检测器来检测。\n考虑了外观和动态，时间和空间和多种空间规模。提出了USCD数据集。数据集的相关介绍可以看看这篇的6.1.这伙人在cvpr2010anomaly detection in crowded scenes出现过。\n 【就是一篇论文吧。】\n\n## 2013\n\n### abnormal event detection at 150 fps in matlab\n\n**优势：**快~每秒140-150帧\n**方法：**Sparse combination learning，和子空间聚类subspace clustering有关系但是又和传统的方法大不相同。\n稀疏组合学习有两个目标：一是有效的表示，即找到K个基底组合，有较小的重建误差。二是让组合的总数K足够小。因为K大的话会让重建误差总是接近0，对于异常的事件也是这样。\n**引言：**影响高效率的一个阻碍是建立稀疏表示的inherently intensive computation 。\n**细节：**每帧resize成3个不同的scale(20x20,30x40,120x160)，每种scale的frame分成很多小块（10x10的不重叠小块）。\n\n## 2011\n\n### Video Parsing for abnormality detection\n\n**中心点：**Parse video frames by establishing a set of hypotheses that jointly explain all the foreground while, at same time, trying to find normal training samples that explain the hypotheses.\n**关键词：**object  hypotheses\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/31332758.jpg)\n\n### Sparse Reconstruction Cost for Abnormal Event Detection\n\n**摘要：**引入了sparse reconstruction cost。我们的方法提供了一个unified solution来同时检测local abnormal events和global abnormal events。（什么是全局异常呢？就是整个场景是异常的，即使individual local behavior can be normal，什么是局部异常呢？就是local behavior is different from its spatio-temporal neighborhoods.）\n**引言：**稀疏表示能够表示高维度的sample。\n**贡献：**  **1.**support an efficient and robust dstimation of SRC\n**2.**方便地处理LAE和GAE异常。\n**3.**通过逐步更新字典，我们的方法能够支持在线的异常检测。\n**细节：**USCD ped1 数据集处理方法--------把每帧分成了7x7的local patches，有4像素的重叠。用了Type C basis（spatio-temporal basis），dimension 7x16=102.\nsubway数据集处理方法---------把帧从512x384大小resize成了320x240大小，并把新的视频帧分成了15x15local patches，有6像素的重叠，用了Type B basis（temporal basis），dimension 16x5=80？？\n**方法：**\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/42528664.jpg)\n绿色或红色的点是正常或异常的测试样本。representatives（深蓝色的点）的optimal subset通过redundant training features(浅蓝色的点)作为basis来构成正常的字典。深蓝色点的半径表示权重，越大表示越正常。异常检测就是measure 测试样本（绿点或红点）在深蓝色点上的稀疏重建成本。\n\n### Online Detection of Unusual Events in Videos via Dynamic Sparse Coding\n\n**创新点：**We propose a fully **unsupervised dynamic sparse coding approach** for detecting unusual events in videos based on **online** sparse reconstructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases. \n**误检的情况：**Subway Exit数据集里面，出现了小孩误检为异常，一个人停在出口并且回头看也误检。\n**相比前人来说成功的地方：**our method not only detects abnormalities in a fine scale, but also unusual events caused by irregular interactions between people\n\n## 2010\n\n### Anomaly Detection in Crowded Scenes\n\nMDT模型\n时间异常检测：[23]背景帧差法。GMM  MDT\n空间异常检测：center surround saliency with the MDT\n\n \n\n## 2009\n\n### Abnormal crowd behavior detection using social force model\n\n社会力模型\nBag of words方法来分类异常和正常\n这个方法比基于纯光流的方法好。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/12979627.jpg)\n\n### Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates\n\n**创新点：**提出了一个空时MRF模型。为了学习每个local node的正常行为模式，用Mixture of Probabilistic Principal Component  Analyzers(MPPCA) 来capture光流的分布。另外，模型参数可以在新的观测进来的时候updated incrementally。\n**方法：**We extract optical flow features at each frame, use MPPCA to identify the typical patterns, and construct a space-time MRF to enable inference at each local site. \n**作者说自己的优势：**1.可以在local和global context检测异常活动。比单纯是local的方法好，local的方法fails to detect abnormal activities with irregular temporal orderings，并且local的方法对于光流参数很敏感导致高的false alarm rate.比单纯是global的方法好，global的方法fails to detect abnormal activity happens within a region so small,这个region在全局的场景中简单的被视为可以忽略的噪声。并且global的方法在拥挤的环境中会产生false alarm。\n**对前人的方法做了什么改进：**用了08年Robust Real-Time Unusual Event Detection Using Multiple...的subway数据集的gt，但是capture 更微小的异常，比如“no payment”和“loitering\".\n**误检或者漏检的情况：**entrance gate数据集中1.走的慢的人。2.对于far-filed area，产生了false alarm，因为光流对于far-filel area是不靠谱的。3.走的很快的人。4.没刷卡的人。exit gate数据集中”from right exit to left exit\"\n\n## 2008\n\n### Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors\n\n**方法：**local-monitors-based 。\n通过multiple, local, low-level feature monitors来监视不寻常的事件。每个monitor是从视频流提取local low-level observation的object。这个observation可以是在monitor的位置的现在的光流方向，或者是local flow的magnitude。\n**异常检测需要什么：**1.对于给定的视频流的tuning 算法应该简单快速。2.算法应该adaptive，适应环境的变换。3.short learning period。4.低成本。5.predictable performance。\n**局限性：**不能检测loitering person或者在进入安检的时候不刷卡。总结里面说，局限性是the lack of **sequential** monitoring.\n**术语：**aperture problem孔径问题[https://blog.csdn.net/hankai1024/article/details/23433157](https://blog.csdn.net/hankai1024/article/details/23433157)；SSD error matrix\n**评价：**2009年Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates评价：focus attention on individual local activities,where\ntypical flow directions and speeds are measured on a grid in the video frame. While efficient and simple to implement, **such an approach fails to model temporal relationships between motions.**\n\n## 2007\n\n### Chaotic invariants of lagrangian particle trajectories for anomaly adetection in crowded scenes\n\n特殊的粒子轨迹的应用 \n引入了chaotic dynamics\n\n","source":"_posts/异常行为检测文献整理-顶会顶刊.md","raw":"---\ntitle: 异常行为检测文献整理(顶会顶刊)\ndate: 2018-11-11 13:25:57\ntags:\n- anomaly detection\ncategories:\n- paper\ncomments: true\n---\n\n## 2018\n\n### real-world anomaly detection in surveillance videos【有代码】\n\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/89215767.jpg)\n**使用的方法：**MIL（multiple instance learning）多示例学习\n\n**方法步骤：**\n（1）     positive(某一部分包含异常)，negative（不包含异常）视频。把positive和negative视频分别分成固定数量的segments。每个视频表示为一个包，每个temporal segment表示包里的一个instance。\n（2）     对video segments提取C3D features。\n（3）     用一个novel ranking loss function（positive bag和negative bag中，在最高分数的instances之间计算ranking loss）来训练一个全连接神经网络。\n简言之就是数据处理、提特征【提取到的特征应该是时空特征吧】、训练网络、通过得分预测是否异常。\n**创新点：**同时利用正常和异常的视频来学习异常行为。不需要标记训练视频中的异常segments or clips（非常浪费时间），而是利用弱标记（weakly labeled）的训练视频，通过deep multiple instance ranking framework来学习异常。视频标记（异常或正常）是video-level，而不是clip-level的。我们把正常和异常的视频看作是bags，把video segments看作是instances。【采用MIL的方法引入到异常检测中来】\n另外，对ranking loss function引入了sparsity和temporal smoothness constraints 来在训练中更好的定位异常。\n（有新的数据集）\n**关键词：**weakly-supervised learning，MIL\n**针对问题：**1**.**其他的方法都是假设偏离正常的行为就是异常。但是这样假设是有问题的，因为把所有可能的正常行为考虑进去是不太可能的。2.正常和异常之间的界限是模糊的。在现实场景中，是否异常可能和条件的不同有关。\n**Baseline methods：**C3D,TCNN（这两种方法在数据集上的效果很差，证明提出来的数据集非常challenging）\n**比较：**主要和Learning temporal regularity in video sequences和Abnormal event detection at 150 fps in matlab的方法比较。\n【the first to formulate the video anomaly detection problem in the context of MIL】\n**个人感想与总结：**（采用了什么方法，达到了什么效果，还有什么不太好的地方可以改进）作者采用MIL方法，同时利用正常和异常的视频，使用提出的deep MIL ranking loss来进行异常检测。\n\n<!--more-->\n\n### Future Frame Prediction for anomaly detection-a new baseline\n\n在视频预测框架中解决异常检测问题。除了加spatial（**Appearance**）约束还加了temporal（**motion**）约束（光流）。也用到了GAN。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/34727984.jpg)\n\n## 2017\n\n### Joint detection and recounting of abnormal events by learning deep generic knowledge\n\n**创新点：**把检测和描述视频中的异常事件联合起来。Recounting of abnormal events,就是解释为什么他们是异常的。我们把一个generic CNN model和environment-dependent anomaly detection融合起来。\n（异常检测是有场景依赖性的）【动作理解动作识别的方法能不能用上？】\n**关键词：**anomaly detector\n**方法：**based on multi-task Fast R-CNN\n\n1. 用大量带标签的数据集来学习multi-task Fast R-CNN，学习到generic model。这样提取出deep features 和visual concept classification scores（同时提出的）。\n2. Anomaly detectors 对每个环境在这些特征和分数上学习 \n\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/86054775.jpg)\n【the first step in abnormal event detection using generic knowledge of visual concepts】\n方法简言之就是提semantic特征（怎么感觉作者提的是appearance特征）、判异常/正常、recount，**我们的方法不需要使用motion features。**\n**个人感想：**semantic feature和appearance feature 之间有什么区别？\n\n### unmasking the abnormal events in video\n\n**创新点：**不需要training sequences，我们的网络基于unmasking，是之前用来在文本文件中做授权认证的。\n【the first work to apply unmasking for a computer vision task】\n作者和【6 （2016）A Discriminative Framework for Anomaly Detection in Large Videos】还有一些监督的方法进行比较。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/51723764.jpg)\n\n还是要提特征\n**作者说：**他们采用**了融合运动和外观特征的方法**，但是没有看到大量的改善，需要进一步改进融合的方法。\n\n### abnormal event detection in videos using generative adversarial nets\n\n**方法：**用正常的帧和对应的光流图来训练GAN，来学习正常场景的internal  representation。在测试的时候把真实的数据和GAN产生的外观和运动表示比较，通过计算local 不同来检测异常区域。\n从raw-pixel frames**产生光流图**。\n\n### a revisit of sparse coding based anomaly detection in stacked RNN framework\n\n**摘要：**提出了TSC（Temporally-coherent sparse coding），enforce 相似的相邻帧用相似的重建系数编码。之后用srnn映射TSC,方便了参数优化加速了异常预测。用sRNN同时学习所有参数，能够避免TSC的non-trivial的超参数选择。另外用浅层的sRNN，重建稀疏系数可以在前向传播中推断出来，节约了计算成本。\n**创新点：**（1）提了TSC（2）提了新数据集\n**baseline的缺点：**2016conv-AE基于3D ConvNet，但是之前的工作表明用双流网络分别提取外观和运动信息是视频中特征的提取的一个better solution。 而且conv-AE的输入是video cube，需要在所有的帧上对数据集进行中心采样，计算代价大。\n**方法：**学习能够编码外观上的正常行为的字典，之后，为了提高在相邻帧的预测的平滑性，加上了一个temporally-coherent term。（作者说，有意思的是得到的TSC的公式可以看成是一个特殊的sRNN）。\n**评估：**作者有个**挺好的**想法，先用一个Synthesized Dataset评估自己的方法对于外观的突然变化导致的异常的表现如何。这个数据集是这样做的：从MINIST里面随意找两个数字，然后把他们放在225x225尺寸的黑背景中。然后在之后的19帧里，这两个数字随意的横向纵向运动。训练的时候用了10000个序列，对于每个测试的序列，5个连续的帧由随意插入的3x3白色的小方块随意的遮挡。测试集一共有3000个序列。如下图所示：\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/24681967.jpg)\n\n## 2016\n\n### Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection\n\n【the first work to employ the existing CNN models for motion representation in crowd analysis】\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/95069565.jpg)\n**创新点：**通过将semantic information（从已有的CNN中得到）和low-level optical-flow结合来measure local abnormality 。不需要fine-tuning阶段。\nTrack the changes in the CNN features **across time.**\n（1）     引入了一个新的Binary Quantization Layer\n（2）     提出了一个Temporal CNN Pattern measure 来表示人群中的运动。\n（无监督的方法比监督方法在异常检测上更好，因为标注的主观性和训练数据少）\n**方法步骤：**\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97738054.jpg)\n1.从输入的视频帧序列中提取CNN-based binary maps.**具体来说是所有的帧输入到一个FCN，把一个binary layer 插在FCN的顶部为了把高维的特征图量化成压缩的二值模式。这个binary layer是一个卷积层，当时权重是用一个external hashing method来初始化的。\n**2.用提到的CNN-based binary maps来计算Temporal CNN Pattern值。**先根据binary maps来计算histograms。然后根据这些histograms计算TCP。【TCP measure 是用来表示人群的运动的motion representation】\n**3.将TCP值和低层次的运动特征（光流）来找到refined motion segments。**\n\n**关键词：**BFCN、TCP\n\n**个人感想与总结：**跟踪随着时间变化的CNN 特征。\n\n### learning temporal regularity in video sequences\n\n**创新点：**提出基于自编码器的方法。\n学习正常行为模式very limited supervision。Reconstruction\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/98412371.jpg)\n第一种Learning Motions on Handcrafted Features。首先用HOG和HOF作为时空appearance feature 描述子。\n**有意思的地方：**4.4节predicting the Regular Past and the Future。给中间的帧，能预测near过去的和未来的帧。（预测过去有什么用？）\n\n### A discriminative framework for anomaly detection in large videos\n\n训练序列不可用，并且异常的分数是独立于时间顺序的。\n\n## 2015\n\n### crowd motion monitoring using tracklet-based commotion measure\n\n无监督方法unsupervised context-dependent statistical commotion measure.\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/85776732.jpg)\n\n### Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression\n\n通过层级框架来**检测局部和全局的异常**。通过层级特征表示和GPR（高斯过程回归）。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/35454189.jpg)\n\n## 2014\n\n### Anomaly Detection and Localization in Crowded Scenes\n\n考虑异常行为的检测和定位。提出了同时检测时空异常的detector。\nTemporal normalcy用MDT(mixtures of dynamic textures)建模，spatial normalcy由基于MDT的一个discriminant saliency 检测器来检测。\n考虑了外观和动态，时间和空间和多种空间规模。提出了USCD数据集。数据集的相关介绍可以看看这篇的6.1.这伙人在cvpr2010anomaly detection in crowded scenes出现过。\n 【就是一篇论文吧。】\n\n## 2013\n\n### abnormal event detection at 150 fps in matlab\n\n**优势：**快~每秒140-150帧\n**方法：**Sparse combination learning，和子空间聚类subspace clustering有关系但是又和传统的方法大不相同。\n稀疏组合学习有两个目标：一是有效的表示，即找到K个基底组合，有较小的重建误差。二是让组合的总数K足够小。因为K大的话会让重建误差总是接近0，对于异常的事件也是这样。\n**引言：**影响高效率的一个阻碍是建立稀疏表示的inherently intensive computation 。\n**细节：**每帧resize成3个不同的scale(20x20,30x40,120x160)，每种scale的frame分成很多小块（10x10的不重叠小块）。\n\n## 2011\n\n### Video Parsing for abnormality detection\n\n**中心点：**Parse video frames by establishing a set of hypotheses that jointly explain all the foreground while, at same time, trying to find normal training samples that explain the hypotheses.\n**关键词：**object  hypotheses\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/31332758.jpg)\n\n### Sparse Reconstruction Cost for Abnormal Event Detection\n\n**摘要：**引入了sparse reconstruction cost。我们的方法提供了一个unified solution来同时检测local abnormal events和global abnormal events。（什么是全局异常呢？就是整个场景是异常的，即使individual local behavior can be normal，什么是局部异常呢？就是local behavior is different from its spatio-temporal neighborhoods.）\n**引言：**稀疏表示能够表示高维度的sample。\n**贡献：**  **1.**support an efficient and robust dstimation of SRC\n**2.**方便地处理LAE和GAE异常。\n**3.**通过逐步更新字典，我们的方法能够支持在线的异常检测。\n**细节：**USCD ped1 数据集处理方法--------把每帧分成了7x7的local patches，有4像素的重叠。用了Type C basis（spatio-temporal basis），dimension 7x16=102.\nsubway数据集处理方法---------把帧从512x384大小resize成了320x240大小，并把新的视频帧分成了15x15local patches，有6像素的重叠，用了Type B basis（temporal basis），dimension 16x5=80？？\n**方法：**\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/42528664.jpg)\n绿色或红色的点是正常或异常的测试样本。representatives（深蓝色的点）的optimal subset通过redundant training features(浅蓝色的点)作为basis来构成正常的字典。深蓝色点的半径表示权重，越大表示越正常。异常检测就是measure 测试样本（绿点或红点）在深蓝色点上的稀疏重建成本。\n\n### Online Detection of Unusual Events in Videos via Dynamic Sparse Coding\n\n**创新点：**We propose a fully **unsupervised dynamic sparse coding approach** for detecting unusual events in videos based on **online** sparse reconstructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases. \n**误检的情况：**Subway Exit数据集里面，出现了小孩误检为异常，一个人停在出口并且回头看也误检。\n**相比前人来说成功的地方：**our method not only detects abnormalities in a fine scale, but also unusual events caused by irregular interactions between people\n\n## 2010\n\n### Anomaly Detection in Crowded Scenes\n\nMDT模型\n时间异常检测：[23]背景帧差法。GMM  MDT\n空间异常检测：center surround saliency with the MDT\n\n \n\n## 2009\n\n### Abnormal crowd behavior detection using social force model\n\n社会力模型\nBag of words方法来分类异常和正常\n这个方法比基于纯光流的方法好。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/12979627.jpg)\n\n### Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates\n\n**创新点：**提出了一个空时MRF模型。为了学习每个local node的正常行为模式，用Mixture of Probabilistic Principal Component  Analyzers(MPPCA) 来capture光流的分布。另外，模型参数可以在新的观测进来的时候updated incrementally。\n**方法：**We extract optical flow features at each frame, use MPPCA to identify the typical patterns, and construct a space-time MRF to enable inference at each local site. \n**作者说自己的优势：**1.可以在local和global context检测异常活动。比单纯是local的方法好，local的方法fails to detect abnormal activities with irregular temporal orderings，并且local的方法对于光流参数很敏感导致高的false alarm rate.比单纯是global的方法好，global的方法fails to detect abnormal activity happens within a region so small,这个region在全局的场景中简单的被视为可以忽略的噪声。并且global的方法在拥挤的环境中会产生false alarm。\n**对前人的方法做了什么改进：**用了08年Robust Real-Time Unusual Event Detection Using Multiple...的subway数据集的gt，但是capture 更微小的异常，比如“no payment”和“loitering\".\n**误检或者漏检的情况：**entrance gate数据集中1.走的慢的人。2.对于far-filed area，产生了false alarm，因为光流对于far-filel area是不靠谱的。3.走的很快的人。4.没刷卡的人。exit gate数据集中”from right exit to left exit\"\n\n## 2008\n\n### Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors\n\n**方法：**local-monitors-based 。\n通过multiple, local, low-level feature monitors来监视不寻常的事件。每个monitor是从视频流提取local low-level observation的object。这个observation可以是在monitor的位置的现在的光流方向，或者是local flow的magnitude。\n**异常检测需要什么：**1.对于给定的视频流的tuning 算法应该简单快速。2.算法应该adaptive，适应环境的变换。3.short learning period。4.低成本。5.predictable performance。\n**局限性：**不能检测loitering person或者在进入安检的时候不刷卡。总结里面说，局限性是the lack of **sequential** monitoring.\n**术语：**aperture problem孔径问题[https://blog.csdn.net/hankai1024/article/details/23433157](https://blog.csdn.net/hankai1024/article/details/23433157)；SSD error matrix\n**评价：**2009年Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates评价：focus attention on individual local activities,where\ntypical flow directions and speeds are measured on a grid in the video frame. While efficient and simple to implement, **such an approach fails to model temporal relationships between motions.**\n\n## 2007\n\n### Chaotic invariants of lagrangian particle trajectories for anomaly adetection in crowded scenes\n\n特殊的粒子轨迹的应用 \n引入了chaotic dynamics\n\n","slug":"异常行为检测文献整理-顶会顶刊","published":1,"updated":"2018-11-11T11:05:11.241Z","layout":"post","photos":[],"link":"","_id":"cjofrrc2e0008k8cy3izl4gat","content":"<h2 id=\"2018\"><a href=\"#2018\" class=\"headerlink\" title=\"2018\"></a>2018</h2><h3 id=\"real-world-anomaly-detection-in-surveillance-videos【有代码】\"><a href=\"#real-world-anomaly-detection-in-surveillance-videos【有代码】\" class=\"headerlink\" title=\"real-world anomaly detection in surveillance videos【有代码】\"></a>real-world anomaly detection in surveillance videos【有代码】</h3><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/89215767.jpg\" alt=\"图片\"><br><strong>使用的方法：</strong>MIL（multiple instance learning）多示例学习</p>\n<p><strong>方法步骤：</strong><br>（1）     positive(某一部分包含异常)，negative（不包含异常）视频。把positive和negative视频分别分成固定数量的segments。每个视频表示为一个包，每个temporal segment表示包里的一个instance。<br>（2）     对video segments提取C3D features。<br>（3）     用一个novel ranking loss function（positive bag和negative bag中，在最高分数的instances之间计算ranking loss）来训练一个全连接神经网络。<br>简言之就是数据处理、提特征【提取到的特征应该是时空特征吧】、训练网络、通过得分预测是否异常。<br><strong>创新点：</strong>同时利用正常和异常的视频来学习异常行为。不需要标记训练视频中的异常segments or clips（非常浪费时间），而是利用弱标记（weakly labeled）的训练视频，通过deep multiple instance ranking framework来学习异常。视频标记（异常或正常）是video-level，而不是clip-level的。我们把正常和异常的视频看作是bags，把video segments看作是instances。【采用MIL的方法引入到异常检测中来】<br>另外，对ranking loss function引入了sparsity和temporal smoothness constraints 来在训练中更好的定位异常。<br>（有新的数据集）<br><strong>关键词：</strong>weakly-supervised learning，MIL<br><strong>针对问题：</strong>1<strong>.</strong>其他的方法都是假设偏离正常的行为就是异常。但是这样假设是有问题的，因为把所有可能的正常行为考虑进去是不太可能的。2.正常和异常之间的界限是模糊的。在现实场景中，是否异常可能和条件的不同有关。<br><strong>Baseline methods：</strong>C3D,TCNN（这两种方法在数据集上的效果很差，证明提出来的数据集非常challenging）<br><strong>比较：</strong>主要和Learning temporal regularity in video sequences和Abnormal event detection at 150 fps in matlab的方法比较。<br>【the first to formulate the video anomaly detection problem in the context of MIL】<br><strong>个人感想与总结：</strong>（采用了什么方法，达到了什么效果，还有什么不太好的地方可以改进）作者采用MIL方法，同时利用正常和异常的视频，使用提出的deep MIL ranking loss来进行异常检测。</p>\n<a id=\"more\"></a>\n<h3 id=\"Future-Frame-Prediction-for-anomaly-detection-a-new-baseline\"><a href=\"#Future-Frame-Prediction-for-anomaly-detection-a-new-baseline\" class=\"headerlink\" title=\"Future Frame Prediction for anomaly detection-a new baseline\"></a>Future Frame Prediction for anomaly detection-a new baseline</h3><p>在视频预测框架中解决异常检测问题。除了加spatial（<strong>Appearance</strong>）约束还加了temporal（<strong>motion</strong>）约束（光流）。也用到了GAN。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/34727984.jpg\" alt=\"图片\"></p>\n<h2 id=\"2017\"><a href=\"#2017\" class=\"headerlink\" title=\"2017\"></a>2017</h2><h3 id=\"Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge\"><a href=\"#Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge\" class=\"headerlink\" title=\"Joint detection and recounting of abnormal events by learning deep generic knowledge\"></a>Joint detection and recounting of abnormal events by learning deep generic knowledge</h3><p><strong>创新点：</strong>把检测和描述视频中的异常事件联合起来。Recounting of abnormal events,就是解释为什么他们是异常的。我们把一个generic CNN model和environment-dependent anomaly detection融合起来。<br>（异常检测是有场景依赖性的）【动作理解动作识别的方法能不能用上？】<br><strong>关键词：</strong>anomaly detector<br><strong>方法：</strong>based on multi-task Fast R-CNN</p>\n<ol>\n<li>用大量带标签的数据集来学习multi-task Fast R-CNN，学习到generic model。这样提取出deep features 和visual concept classification scores（同时提出的）。</li>\n<li>Anomaly detectors 对每个环境在这些特征和分数上学习 </li>\n</ol>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/86054775.jpg\" alt=\"图片\"><br>【the first step in abnormal event detection using generic knowledge of visual concepts】<br>方法简言之就是提semantic特征（怎么感觉作者提的是appearance特征）、判异常/正常、recount，<strong>我们的方法不需要使用motion features。</strong><br><strong>个人感想：</strong>semantic feature和appearance feature 之间有什么区别？</p>\n<h3 id=\"unmasking-the-abnormal-events-in-video\"><a href=\"#unmasking-the-abnormal-events-in-video\" class=\"headerlink\" title=\"unmasking the abnormal events in video\"></a>unmasking the abnormal events in video</h3><p><strong>创新点：</strong>不需要training sequences，我们的网络基于unmasking，是之前用来在文本文件中做授权认证的。<br>【the first work to apply unmasking for a computer vision task】<br>作者和【6 （2016）A Discriminative Framework for Anomaly Detection in Large Videos】还有一些监督的方法进行比较。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/51723764.jpg\" alt=\"图片\"></p>\n<p>还是要提特征<br><strong>作者说：</strong>他们采用<strong>了融合运动和外观特征的方法</strong>，但是没有看到大量的改善，需要进一步改进融合的方法。</p>\n<h3 id=\"abnormal-event-detection-in-videos-using-generative-adversarial-nets\"><a href=\"#abnormal-event-detection-in-videos-using-generative-adversarial-nets\" class=\"headerlink\" title=\"abnormal event detection in videos using generative adversarial nets\"></a>abnormal event detection in videos using generative adversarial nets</h3><p><strong>方法：</strong>用正常的帧和对应的光流图来训练GAN，来学习正常场景的internal  representation。在测试的时候把真实的数据和GAN产生的外观和运动表示比较，通过计算local 不同来检测异常区域。<br>从raw-pixel frames<strong>产生光流图</strong>。</p>\n<h3 id=\"a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework\"><a href=\"#a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework\" class=\"headerlink\" title=\"a revisit of sparse coding based anomaly detection in stacked RNN framework\"></a>a revisit of sparse coding based anomaly detection in stacked RNN framework</h3><p><strong>摘要：</strong>提出了TSC（Temporally-coherent sparse coding），enforce 相似的相邻帧用相似的重建系数编码。之后用srnn映射TSC,方便了参数优化加速了异常预测。用sRNN同时学习所有参数，能够避免TSC的non-trivial的超参数选择。另外用浅层的sRNN，重建稀疏系数可以在前向传播中推断出来，节约了计算成本。<br><strong>创新点：</strong>（1）提了TSC（2）提了新数据集<br><strong>baseline的缺点：</strong>2016conv-AE基于3D ConvNet，但是之前的工作表明用双流网络分别提取外观和运动信息是视频中特征的提取的一个better solution。 而且conv-AE的输入是video cube，需要在所有的帧上对数据集进行中心采样，计算代价大。<br><strong>方法：</strong>学习能够编码外观上的正常行为的字典，之后，为了提高在相邻帧的预测的平滑性，加上了一个temporally-coherent term。（作者说，有意思的是得到的TSC的公式可以看成是一个特殊的sRNN）。<br><strong>评估：</strong>作者有个<strong>挺好的</strong>想法，先用一个Synthesized Dataset评估自己的方法对于外观的突然变化导致的异常的表现如何。这个数据集是这样做的：从MINIST里面随意找两个数字，然后把他们放在225x225尺寸的黑背景中。然后在之后的19帧里，这两个数字随意的横向纵向运动。训练的时候用了10000个序列，对于每个测试的序列，5个连续的帧由随意插入的3x3白色的小方块随意的遮挡。测试集一共有3000个序列。如下图所示：<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/24681967.jpg\" alt=\"图片\"></p>\n<h2 id=\"2016\"><a href=\"#2016\" class=\"headerlink\" title=\"2016\"></a>2016</h2><h3 id=\"Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection\"><a href=\"#Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection\" class=\"headerlink\" title=\"Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection\"></a>Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</h3><p>【the first work to employ the existing CNN models for motion representation in crowd analysis】<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/95069565.jpg\" alt=\"图片\"><br><strong>创新点：</strong>通过将semantic information（从已有的CNN中得到）和low-level optical-flow结合来measure local abnormality 。不需要fine-tuning阶段。<br>Track the changes in the CNN features <strong>across time.</strong><br>（1）     引入了一个新的Binary Quantization Layer<br>（2）     提出了一个Temporal CNN Pattern measure 来表示人群中的运动。<br>（无监督的方法比监督方法在异常检测上更好，因为标注的主观性和训练数据少）<br><strong>方法步骤：</strong><br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97738054.jpg\" alt=\"图片\"><br>1.从输入的视频帧序列中提取CNN-based binary maps.<strong>具体来说是所有的帧输入到一个FCN，把一个binary layer 插在FCN的顶部为了把高维的特征图量化成压缩的二值模式。这个binary layer是一个卷积层，当时权重是用一个external hashing method来初始化的。\n</strong>2.用提到的CNN-based binary maps来计算Temporal CNN Pattern值。<strong>先根据binary maps来计算histograms。然后根据这些histograms计算TCP。【TCP measure 是用来表示人群的运动的motion representation】\n</strong>3.将TCP值和低层次的运动特征（光流）来找到refined motion segments。**</p>\n<p><strong>关键词：</strong>BFCN、TCP</p>\n<p><strong>个人感想与总结：</strong>跟踪随着时间变化的CNN 特征。</p>\n<h3 id=\"learning-temporal-regularity-in-video-sequences\"><a href=\"#learning-temporal-regularity-in-video-sequences\" class=\"headerlink\" title=\"learning temporal regularity in video sequences\"></a>learning temporal regularity in video sequences</h3><p><strong>创新点：</strong>提出基于自编码器的方法。<br>学习正常行为模式very limited supervision。Reconstruction<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/98412371.jpg\" alt=\"图片\"><br>第一种Learning Motions on Handcrafted Features。首先用HOG和HOF作为时空appearance feature 描述子。<br><strong>有意思的地方：</strong>4.4节predicting the Regular Past and the Future。给中间的帧，能预测near过去的和未来的帧。（预测过去有什么用？）</p>\n<h3 id=\"A-discriminative-framework-for-anomaly-detection-in-large-videos\"><a href=\"#A-discriminative-framework-for-anomaly-detection-in-large-videos\" class=\"headerlink\" title=\"A discriminative framework for anomaly detection in large videos\"></a>A discriminative framework for anomaly detection in large videos</h3><p>训练序列不可用，并且异常的分数是独立于时间顺序的。</p>\n<h2 id=\"2015\"><a href=\"#2015\" class=\"headerlink\" title=\"2015\"></a>2015</h2><h3 id=\"crowd-motion-monitoring-using-tracklet-based-commotion-measure\"><a href=\"#crowd-motion-monitoring-using-tracklet-based-commotion-measure\" class=\"headerlink\" title=\"crowd motion monitoring using tracklet-based commotion measure\"></a>crowd motion monitoring using tracklet-based commotion measure</h3><p>无监督方法unsupervised context-dependent statistical commotion measure.<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/85776732.jpg\" alt=\"图片\"></p>\n<h3 id=\"Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression\"><a href=\"#Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression\" class=\"headerlink\" title=\"Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression\"></a>Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression</h3><p>通过层级框架来<strong>检测局部和全局的异常</strong>。通过层级特征表示和GPR（高斯过程回归）。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/35454189.jpg\" alt=\"图片\"></p>\n<h2 id=\"2014\"><a href=\"#2014\" class=\"headerlink\" title=\"2014\"></a>2014</h2><h3 id=\"Anomaly-Detection-and-Localization-in-Crowded-Scenes\"><a href=\"#Anomaly-Detection-and-Localization-in-Crowded-Scenes\" class=\"headerlink\" title=\"Anomaly Detection and Localization in Crowded Scenes\"></a>Anomaly Detection and Localization in Crowded Scenes</h3><p>考虑异常行为的检测和定位。提出了同时检测时空异常的detector。<br>Temporal normalcy用MDT(mixtures of dynamic textures)建模，spatial normalcy由基于MDT的一个discriminant saliency 检测器来检测。<br>考虑了外观和动态，时间和空间和多种空间规模。提出了USCD数据集。数据集的相关介绍可以看看这篇的6.1.这伙人在cvpr2010anomaly detection in crowded scenes出现过。<br> 【就是一篇论文吧。】</p>\n<h2 id=\"2013\"><a href=\"#2013\" class=\"headerlink\" title=\"2013\"></a>2013</h2><h3 id=\"abnormal-event-detection-at-150-fps-in-matlab\"><a href=\"#abnormal-event-detection-at-150-fps-in-matlab\" class=\"headerlink\" title=\"abnormal event detection at 150 fps in matlab\"></a>abnormal event detection at 150 fps in matlab</h3><p><strong>优势：</strong>快~每秒140-150帧<br><strong>方法：</strong>Sparse combination learning，和子空间聚类subspace clustering有关系但是又和传统的方法大不相同。<br>稀疏组合学习有两个目标：一是有效的表示，即找到K个基底组合，有较小的重建误差。二是让组合的总数K足够小。因为K大的话会让重建误差总是接近0，对于异常的事件也是这样。<br><strong>引言：</strong>影响高效率的一个阻碍是建立稀疏表示的inherently intensive computation 。<br><strong>细节：</strong>每帧resize成3个不同的scale(20x20,30x40,120x160)，每种scale的frame分成很多小块（10x10的不重叠小块）。</p>\n<h2 id=\"2011\"><a href=\"#2011\" class=\"headerlink\" title=\"2011\"></a>2011</h2><h3 id=\"Video-Parsing-for-abnormality-detection\"><a href=\"#Video-Parsing-for-abnormality-detection\" class=\"headerlink\" title=\"Video Parsing for abnormality detection\"></a>Video Parsing for abnormality detection</h3><p><strong>中心点：</strong>Parse video frames by establishing a set of hypotheses that jointly explain all the foreground while, at same time, trying to find normal training samples that explain the hypotheses.<br><strong>关键词：</strong>object  hypotheses<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/31332758.jpg\" alt=\"图片\"></p>\n<h3 id=\"Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection\"><a href=\"#Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection\" class=\"headerlink\" title=\"Sparse Reconstruction Cost for Abnormal Event Detection\"></a>Sparse Reconstruction Cost for Abnormal Event Detection</h3><p><strong>摘要：</strong>引入了sparse reconstruction cost。我们的方法提供了一个unified solution来同时检测local abnormal events和global abnormal events。（什么是全局异常呢？就是整个场景是异常的，即使individual local behavior can be normal，什么是局部异常呢？就是local behavior is different from its spatio-temporal neighborhoods.）<br><strong>引言：</strong>稀疏表示能够表示高维度的sample。<br><strong>贡献：</strong>  <strong>1.</strong>support an efficient and robust dstimation of SRC<br><strong>2.</strong>方便地处理LAE和GAE异常。<br><strong>3.</strong>通过逐步更新字典，我们的方法能够支持在线的异常检测。<br><strong>细节：</strong>USCD ped1 数据集处理方法——–把每帧分成了7x7的local patches，有4像素的重叠。用了Type C basis（spatio-temporal basis），dimension 7x16=102.<br>subway数据集处理方法———把帧从512x384大小resize成了320x240大小，并把新的视频帧分成了15x15local patches，有6像素的重叠，用了Type B basis（temporal basis），dimension 16x5=80？？<br><strong>方法：</strong><br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/42528664.jpg\" alt=\"图片\"><br>绿色或红色的点是正常或异常的测试样本。representatives（深蓝色的点）的optimal subset通过redundant training features(浅蓝色的点)作为basis来构成正常的字典。深蓝色点的半径表示权重，越大表示越正常。异常检测就是measure 测试样本（绿点或红点）在深蓝色点上的稀疏重建成本。</p>\n<h3 id=\"Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding\"><a href=\"#Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding\" class=\"headerlink\" title=\"Online Detection of Unusual Events in Videos via Dynamic Sparse Coding\"></a>Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</h3><p><strong>创新点：</strong>We propose a fully <strong>unsupervised dynamic sparse coding approach</strong> for detecting unusual events in videos based on <strong>online</strong> sparse reconstructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases.<br><strong>误检的情况：</strong>Subway Exit数据集里面，出现了小孩误检为异常，一个人停在出口并且回头看也误检。<br><strong>相比前人来说成功的地方：</strong>our method not only detects abnormalities in a fine scale, but also unusual events caused by irregular interactions between people</p>\n<h2 id=\"2010\"><a href=\"#2010\" class=\"headerlink\" title=\"2010\"></a>2010</h2><h3 id=\"Anomaly-Detection-in-Crowded-Scenes\"><a href=\"#Anomaly-Detection-in-Crowded-Scenes\" class=\"headerlink\" title=\"Anomaly Detection in Crowded Scenes\"></a>Anomaly Detection in Crowded Scenes</h3><p>MDT模型<br>时间异常检测：[23]背景帧差法。GMM  MDT<br>空间异常检测：center surround saliency with the MDT</p>\n<h2 id=\"2009\"><a href=\"#2009\" class=\"headerlink\" title=\"2009\"></a>2009</h2><h3 id=\"Abnormal-crowd-behavior-detection-using-social-force-model\"><a href=\"#Abnormal-crowd-behavior-detection-using-social-force-model\" class=\"headerlink\" title=\"Abnormal crowd behavior detection using social force model\"></a>Abnormal crowd behavior detection using social force model</h3><p>社会力模型<br>Bag of words方法来分类异常和正常<br>这个方法比基于纯光流的方法好。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/12979627.jpg\" alt=\"图片\"></p>\n<h3 id=\"Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates\"><a href=\"#Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates\" class=\"headerlink\" title=\"Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates\"></a>Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates</h3><p><strong>创新点：</strong>提出了一个空时MRF模型。为了学习每个local node的正常行为模式，用Mixture of Probabilistic Principal Component  Analyzers(MPPCA) 来capture光流的分布。另外，模型参数可以在新的观测进来的时候updated incrementally。<br><strong>方法：</strong>We extract optical flow features at each frame, use MPPCA to identify the typical patterns, and construct a space-time MRF to enable inference at each local site.<br><strong>作者说自己的优势：</strong>1.可以在local和global context检测异常活动。比单纯是local的方法好，local的方法fails to detect abnormal activities with irregular temporal orderings，并且local的方法对于光流参数很敏感导致高的false alarm rate.比单纯是global的方法好，global的方法fails to detect abnormal activity happens within a region so small,这个region在全局的场景中简单的被视为可以忽略的噪声。并且global的方法在拥挤的环境中会产生false alarm。<br><strong>对前人的方法做了什么改进：</strong>用了08年Robust Real-Time Unusual Event Detection Using Multiple…的subway数据集的gt，但是capture 更微小的异常，比如“no payment”和“loitering”.<br><strong>误检或者漏检的情况：</strong>entrance gate数据集中1.走的慢的人。2.对于far-filed area，产生了false alarm，因为光流对于far-filel area是不靠谱的。3.走的很快的人。4.没刷卡的人。exit gate数据集中”from right exit to left exit”</p>\n<h2 id=\"2008\"><a href=\"#2008\" class=\"headerlink\" title=\"2008\"></a>2008</h2><h3 id=\"Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors\"><a href=\"#Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors\" class=\"headerlink\" title=\"Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors\"></a>Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</h3><p><strong>方法：</strong>local-monitors-based 。<br>通过multiple, local, low-level feature monitors来监视不寻常的事件。每个monitor是从视频流提取local low-level observation的object。这个observation可以是在monitor的位置的现在的光流方向，或者是local flow的magnitude。<br><strong>异常检测需要什么：</strong>1.对于给定的视频流的tuning 算法应该简单快速。2.算法应该adaptive，适应环境的变换。3.short learning period。4.低成本。5.predictable performance。<br><strong>局限性：</strong>不能检测loitering person或者在进入安检的时候不刷卡。总结里面说，局限性是the lack of <strong>sequential</strong> monitoring.<br><strong>术语：</strong>aperture problem孔径问题<a href=\"https://blog.csdn.net/hankai1024/article/details/23433157\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/hankai1024/article/details/23433157</a>；SSD error matrix<br><strong>评价：</strong>2009年Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates评价：focus attention on individual local activities,where<br>typical flow directions and speeds are measured on a grid in the video frame. While efficient and simple to implement, <strong>such an approach fails to model temporal relationships between motions.</strong></p>\n<h2 id=\"2007\"><a href=\"#2007\" class=\"headerlink\" title=\"2007\"></a>2007</h2><h3 id=\"Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-adetection-in-crowded-scenes\"><a href=\"#Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-adetection-in-crowded-scenes\" class=\"headerlink\" title=\"Chaotic invariants of lagrangian particle trajectories for anomaly adetection in crowded scenes\"></a>Chaotic invariants of lagrangian particle trajectories for anomaly adetection in crowded scenes</h3><p>特殊的粒子轨迹的应用<br>引入了chaotic dynamics</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"2018\"><a href=\"#2018\" class=\"headerlink\" title=\"2018\"></a>2018</h2><h3 id=\"real-world-anomaly-detection-in-surveillance-videos【有代码】\"><a href=\"#real-world-anomaly-detection-in-surveillance-videos【有代码】\" class=\"headerlink\" title=\"real-world anomaly detection in surveillance videos【有代码】\"></a>real-world anomaly detection in surveillance videos【有代码】</h3><p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/89215767.jpg\" alt=\"图片\"><br><strong>使用的方法：</strong>MIL（multiple instance learning）多示例学习</p>\n<p><strong>方法步骤：</strong><br>（1）     positive(某一部分包含异常)，negative（不包含异常）视频。把positive和negative视频分别分成固定数量的segments。每个视频表示为一个包，每个temporal segment表示包里的一个instance。<br>（2）     对video segments提取C3D features。<br>（3）     用一个novel ranking loss function（positive bag和negative bag中，在最高分数的instances之间计算ranking loss）来训练一个全连接神经网络。<br>简言之就是数据处理、提特征【提取到的特征应该是时空特征吧】、训练网络、通过得分预测是否异常。<br><strong>创新点：</strong>同时利用正常和异常的视频来学习异常行为。不需要标记训练视频中的异常segments or clips（非常浪费时间），而是利用弱标记（weakly labeled）的训练视频，通过deep multiple instance ranking framework来学习异常。视频标记（异常或正常）是video-level，而不是clip-level的。我们把正常和异常的视频看作是bags，把video segments看作是instances。【采用MIL的方法引入到异常检测中来】<br>另外，对ranking loss function引入了sparsity和temporal smoothness constraints 来在训练中更好的定位异常。<br>（有新的数据集）<br><strong>关键词：</strong>weakly-supervised learning，MIL<br><strong>针对问题：</strong>1<strong>.</strong>其他的方法都是假设偏离正常的行为就是异常。但是这样假设是有问题的，因为把所有可能的正常行为考虑进去是不太可能的。2.正常和异常之间的界限是模糊的。在现实场景中，是否异常可能和条件的不同有关。<br><strong>Baseline methods：</strong>C3D,TCNN（这两种方法在数据集上的效果很差，证明提出来的数据集非常challenging）<br><strong>比较：</strong>主要和Learning temporal regularity in video sequences和Abnormal event detection at 150 fps in matlab的方法比较。<br>【the first to formulate the video anomaly detection problem in the context of MIL】<br><strong>个人感想与总结：</strong>（采用了什么方法，达到了什么效果，还有什么不太好的地方可以改进）作者采用MIL方法，同时利用正常和异常的视频，使用提出的deep MIL ranking loss来进行异常检测。</p>","more":"<h3 id=\"Future-Frame-Prediction-for-anomaly-detection-a-new-baseline\"><a href=\"#Future-Frame-Prediction-for-anomaly-detection-a-new-baseline\" class=\"headerlink\" title=\"Future Frame Prediction for anomaly detection-a new baseline\"></a>Future Frame Prediction for anomaly detection-a new baseline</h3><p>在视频预测框架中解决异常检测问题。除了加spatial（<strong>Appearance</strong>）约束还加了temporal（<strong>motion</strong>）约束（光流）。也用到了GAN。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/34727984.jpg\" alt=\"图片\"></p>\n<h2 id=\"2017\"><a href=\"#2017\" class=\"headerlink\" title=\"2017\"></a>2017</h2><h3 id=\"Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge\"><a href=\"#Joint-detection-and-recounting-of-abnormal-events-by-learning-deep-generic-knowledge\" class=\"headerlink\" title=\"Joint detection and recounting of abnormal events by learning deep generic knowledge\"></a>Joint detection and recounting of abnormal events by learning deep generic knowledge</h3><p><strong>创新点：</strong>把检测和描述视频中的异常事件联合起来。Recounting of abnormal events,就是解释为什么他们是异常的。我们把一个generic CNN model和environment-dependent anomaly detection融合起来。<br>（异常检测是有场景依赖性的）【动作理解动作识别的方法能不能用上？】<br><strong>关键词：</strong>anomaly detector<br><strong>方法：</strong>based on multi-task Fast R-CNN</p>\n<ol>\n<li>用大量带标签的数据集来学习multi-task Fast R-CNN，学习到generic model。这样提取出deep features 和visual concept classification scores（同时提出的）。</li>\n<li>Anomaly detectors 对每个环境在这些特征和分数上学习 </li>\n</ol>\n<p><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/86054775.jpg\" alt=\"图片\"><br>【the first step in abnormal event detection using generic knowledge of visual concepts】<br>方法简言之就是提semantic特征（怎么感觉作者提的是appearance特征）、判异常/正常、recount，<strong>我们的方法不需要使用motion features。</strong><br><strong>个人感想：</strong>semantic feature和appearance feature 之间有什么区别？</p>\n<h3 id=\"unmasking-the-abnormal-events-in-video\"><a href=\"#unmasking-the-abnormal-events-in-video\" class=\"headerlink\" title=\"unmasking the abnormal events in video\"></a>unmasking the abnormal events in video</h3><p><strong>创新点：</strong>不需要training sequences，我们的网络基于unmasking，是之前用来在文本文件中做授权认证的。<br>【the first work to apply unmasking for a computer vision task】<br>作者和【6 （2016）A Discriminative Framework for Anomaly Detection in Large Videos】还有一些监督的方法进行比较。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/51723764.jpg\" alt=\"图片\"></p>\n<p>还是要提特征<br><strong>作者说：</strong>他们采用<strong>了融合运动和外观特征的方法</strong>，但是没有看到大量的改善，需要进一步改进融合的方法。</p>\n<h3 id=\"abnormal-event-detection-in-videos-using-generative-adversarial-nets\"><a href=\"#abnormal-event-detection-in-videos-using-generative-adversarial-nets\" class=\"headerlink\" title=\"abnormal event detection in videos using generative adversarial nets\"></a>abnormal event detection in videos using generative adversarial nets</h3><p><strong>方法：</strong>用正常的帧和对应的光流图来训练GAN，来学习正常场景的internal  representation。在测试的时候把真实的数据和GAN产生的外观和运动表示比较，通过计算local 不同来检测异常区域。<br>从raw-pixel frames<strong>产生光流图</strong>。</p>\n<h3 id=\"a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework\"><a href=\"#a-revisit-of-sparse-coding-based-anomaly-detection-in-stacked-RNN-framework\" class=\"headerlink\" title=\"a revisit of sparse coding based anomaly detection in stacked RNN framework\"></a>a revisit of sparse coding based anomaly detection in stacked RNN framework</h3><p><strong>摘要：</strong>提出了TSC（Temporally-coherent sparse coding），enforce 相似的相邻帧用相似的重建系数编码。之后用srnn映射TSC,方便了参数优化加速了异常预测。用sRNN同时学习所有参数，能够避免TSC的non-trivial的超参数选择。另外用浅层的sRNN，重建稀疏系数可以在前向传播中推断出来，节约了计算成本。<br><strong>创新点：</strong>（1）提了TSC（2）提了新数据集<br><strong>baseline的缺点：</strong>2016conv-AE基于3D ConvNet，但是之前的工作表明用双流网络分别提取外观和运动信息是视频中特征的提取的一个better solution。 而且conv-AE的输入是video cube，需要在所有的帧上对数据集进行中心采样，计算代价大。<br><strong>方法：</strong>学习能够编码外观上的正常行为的字典，之后，为了提高在相邻帧的预测的平滑性，加上了一个temporally-coherent term。（作者说，有意思的是得到的TSC的公式可以看成是一个特殊的sRNN）。<br><strong>评估：</strong>作者有个<strong>挺好的</strong>想法，先用一个Synthesized Dataset评估自己的方法对于外观的突然变化导致的异常的表现如何。这个数据集是这样做的：从MINIST里面随意找两个数字，然后把他们放在225x225尺寸的黑背景中。然后在之后的19帧里，这两个数字随意的横向纵向运动。训练的时候用了10000个序列，对于每个测试的序列，5个连续的帧由随意插入的3x3白色的小方块随意的遮挡。测试集一共有3000个序列。如下图所示：<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/24681967.jpg\" alt=\"图片\"></p>\n<h2 id=\"2016\"><a href=\"#2016\" class=\"headerlink\" title=\"2016\"></a>2016</h2><h3 id=\"Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection\"><a href=\"#Plug-and-Play-CNN-for-Crowd-Motion-Analysis-An-Application-in-Abnormal-Event-Detection\" class=\"headerlink\" title=\"Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection\"></a>Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</h3><p>【the first work to employ the existing CNN models for motion representation in crowd analysis】<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/95069565.jpg\" alt=\"图片\"><br><strong>创新点：</strong>通过将semantic information（从已有的CNN中得到）和low-level optical-flow结合来measure local abnormality 。不需要fine-tuning阶段。<br>Track the changes in the CNN features <strong>across time.</strong><br>（1）     引入了一个新的Binary Quantization Layer<br>（2）     提出了一个Temporal CNN Pattern measure 来表示人群中的运动。<br>（无监督的方法比监督方法在异常检测上更好，因为标注的主观性和训练数据少）<br><strong>方法步骤：</strong><br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97738054.jpg\" alt=\"图片\"><br>1.从输入的视频帧序列中提取CNN-based binary maps.<strong>具体来说是所有的帧输入到一个FCN，把一个binary layer 插在FCN的顶部为了把高维的特征图量化成压缩的二值模式。这个binary layer是一个卷积层，当时权重是用一个external hashing method来初始化的。\n</strong>2.用提到的CNN-based binary maps来计算Temporal CNN Pattern值。<strong>先根据binary maps来计算histograms。然后根据这些histograms计算TCP。【TCP measure 是用来表示人群的运动的motion representation】\n</strong>3.将TCP值和低层次的运动特征（光流）来找到refined motion segments。**</p>\n<p><strong>关键词：</strong>BFCN、TCP</p>\n<p><strong>个人感想与总结：</strong>跟踪随着时间变化的CNN 特征。</p>\n<h3 id=\"learning-temporal-regularity-in-video-sequences\"><a href=\"#learning-temporal-regularity-in-video-sequences\" class=\"headerlink\" title=\"learning temporal regularity in video sequences\"></a>learning temporal regularity in video sequences</h3><p><strong>创新点：</strong>提出基于自编码器的方法。<br>学习正常行为模式very limited supervision。Reconstruction<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/98412371.jpg\" alt=\"图片\"><br>第一种Learning Motions on Handcrafted Features。首先用HOG和HOF作为时空appearance feature 描述子。<br><strong>有意思的地方：</strong>4.4节predicting the Regular Past and the Future。给中间的帧，能预测near过去的和未来的帧。（预测过去有什么用？）</p>\n<h3 id=\"A-discriminative-framework-for-anomaly-detection-in-large-videos\"><a href=\"#A-discriminative-framework-for-anomaly-detection-in-large-videos\" class=\"headerlink\" title=\"A discriminative framework for anomaly detection in large videos\"></a>A discriminative framework for anomaly detection in large videos</h3><p>训练序列不可用，并且异常的分数是独立于时间顺序的。</p>\n<h2 id=\"2015\"><a href=\"#2015\" class=\"headerlink\" title=\"2015\"></a>2015</h2><h3 id=\"crowd-motion-monitoring-using-tracklet-based-commotion-measure\"><a href=\"#crowd-motion-monitoring-using-tracklet-based-commotion-measure\" class=\"headerlink\" title=\"crowd motion monitoring using tracklet-based commotion measure\"></a>crowd motion monitoring using tracklet-based commotion measure</h3><p>无监督方法unsupervised context-dependent statistical commotion measure.<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/85776732.jpg\" alt=\"图片\"></p>\n<h3 id=\"Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression\"><a href=\"#Video-Anomaly-Detection-and-Localization-Using-Hierarchical-Feature-Representation-and-Gaussian-Process-Regression\" class=\"headerlink\" title=\"Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression\"></a>Video Anomaly Detection and Localization Using Hierarchical Feature Representation and Gaussian Process Regression</h3><p>通过层级框架来<strong>检测局部和全局的异常</strong>。通过层级特征表示和GPR（高斯过程回归）。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/35454189.jpg\" alt=\"图片\"></p>\n<h2 id=\"2014\"><a href=\"#2014\" class=\"headerlink\" title=\"2014\"></a>2014</h2><h3 id=\"Anomaly-Detection-and-Localization-in-Crowded-Scenes\"><a href=\"#Anomaly-Detection-and-Localization-in-Crowded-Scenes\" class=\"headerlink\" title=\"Anomaly Detection and Localization in Crowded Scenes\"></a>Anomaly Detection and Localization in Crowded Scenes</h3><p>考虑异常行为的检测和定位。提出了同时检测时空异常的detector。<br>Temporal normalcy用MDT(mixtures of dynamic textures)建模，spatial normalcy由基于MDT的一个discriminant saliency 检测器来检测。<br>考虑了外观和动态，时间和空间和多种空间规模。提出了USCD数据集。数据集的相关介绍可以看看这篇的6.1.这伙人在cvpr2010anomaly detection in crowded scenes出现过。<br> 【就是一篇论文吧。】</p>\n<h2 id=\"2013\"><a href=\"#2013\" class=\"headerlink\" title=\"2013\"></a>2013</h2><h3 id=\"abnormal-event-detection-at-150-fps-in-matlab\"><a href=\"#abnormal-event-detection-at-150-fps-in-matlab\" class=\"headerlink\" title=\"abnormal event detection at 150 fps in matlab\"></a>abnormal event detection at 150 fps in matlab</h3><p><strong>优势：</strong>快~每秒140-150帧<br><strong>方法：</strong>Sparse combination learning，和子空间聚类subspace clustering有关系但是又和传统的方法大不相同。<br>稀疏组合学习有两个目标：一是有效的表示，即找到K个基底组合，有较小的重建误差。二是让组合的总数K足够小。因为K大的话会让重建误差总是接近0，对于异常的事件也是这样。<br><strong>引言：</strong>影响高效率的一个阻碍是建立稀疏表示的inherently intensive computation 。<br><strong>细节：</strong>每帧resize成3个不同的scale(20x20,30x40,120x160)，每种scale的frame分成很多小块（10x10的不重叠小块）。</p>\n<h2 id=\"2011\"><a href=\"#2011\" class=\"headerlink\" title=\"2011\"></a>2011</h2><h3 id=\"Video-Parsing-for-abnormality-detection\"><a href=\"#Video-Parsing-for-abnormality-detection\" class=\"headerlink\" title=\"Video Parsing for abnormality detection\"></a>Video Parsing for abnormality detection</h3><p><strong>中心点：</strong>Parse video frames by establishing a set of hypotheses that jointly explain all the foreground while, at same time, trying to find normal training samples that explain the hypotheses.<br><strong>关键词：</strong>object  hypotheses<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/31332758.jpg\" alt=\"图片\"></p>\n<h3 id=\"Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection\"><a href=\"#Sparse-Reconstruction-Cost-for-Abnormal-Event-Detection\" class=\"headerlink\" title=\"Sparse Reconstruction Cost for Abnormal Event Detection\"></a>Sparse Reconstruction Cost for Abnormal Event Detection</h3><p><strong>摘要：</strong>引入了sparse reconstruction cost。我们的方法提供了一个unified solution来同时检测local abnormal events和global abnormal events。（什么是全局异常呢？就是整个场景是异常的，即使individual local behavior can be normal，什么是局部异常呢？就是local behavior is different from its spatio-temporal neighborhoods.）<br><strong>引言：</strong>稀疏表示能够表示高维度的sample。<br><strong>贡献：</strong>  <strong>1.</strong>support an efficient and robust dstimation of SRC<br><strong>2.</strong>方便地处理LAE和GAE异常。<br><strong>3.</strong>通过逐步更新字典，我们的方法能够支持在线的异常检测。<br><strong>细节：</strong>USCD ped1 数据集处理方法——–把每帧分成了7x7的local patches，有4像素的重叠。用了Type C basis（spatio-temporal basis），dimension 7x16=102.<br>subway数据集处理方法———把帧从512x384大小resize成了320x240大小，并把新的视频帧分成了15x15local patches，有6像素的重叠，用了Type B basis（temporal basis），dimension 16x5=80？？<br><strong>方法：</strong><br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/42528664.jpg\" alt=\"图片\"><br>绿色或红色的点是正常或异常的测试样本。representatives（深蓝色的点）的optimal subset通过redundant training features(浅蓝色的点)作为basis来构成正常的字典。深蓝色点的半径表示权重，越大表示越正常。异常检测就是measure 测试样本（绿点或红点）在深蓝色点上的稀疏重建成本。</p>\n<h3 id=\"Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding\"><a href=\"#Online-Detection-of-Unusual-Events-in-Videos-via-Dynamic-Sparse-Coding\" class=\"headerlink\" title=\"Online Detection of Unusual Events in Videos via Dynamic Sparse Coding\"></a>Online Detection of Unusual Events in Videos via Dynamic Sparse Coding</h3><p><strong>创新点：</strong>We propose a fully <strong>unsupervised dynamic sparse coding approach</strong> for detecting unusual events in videos based on <strong>online</strong> sparse reconstructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases.<br><strong>误检的情况：</strong>Subway Exit数据集里面，出现了小孩误检为异常，一个人停在出口并且回头看也误检。<br><strong>相比前人来说成功的地方：</strong>our method not only detects abnormalities in a fine scale, but also unusual events caused by irregular interactions between people</p>\n<h2 id=\"2010\"><a href=\"#2010\" class=\"headerlink\" title=\"2010\"></a>2010</h2><h3 id=\"Anomaly-Detection-in-Crowded-Scenes\"><a href=\"#Anomaly-Detection-in-Crowded-Scenes\" class=\"headerlink\" title=\"Anomaly Detection in Crowded Scenes\"></a>Anomaly Detection in Crowded Scenes</h3><p>MDT模型<br>时间异常检测：[23]背景帧差法。GMM  MDT<br>空间异常检测：center surround saliency with the MDT</p>\n<h2 id=\"2009\"><a href=\"#2009\" class=\"headerlink\" title=\"2009\"></a>2009</h2><h3 id=\"Abnormal-crowd-behavior-detection-using-social-force-model\"><a href=\"#Abnormal-crowd-behavior-detection-using-social-force-model\" class=\"headerlink\" title=\"Abnormal crowd behavior detection using social force model\"></a>Abnormal crowd behavior detection using social force model</h3><p>社会力模型<br>Bag of words方法来分类异常和正常<br>这个方法比基于纯光流的方法好。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/12979627.jpg\" alt=\"图片\"></p>\n<h3 id=\"Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates\"><a href=\"#Observe-Locally-Infer-Globally-a-Space-Time-MRF-for-Detecting-Abnormal-Activities-with-Incremental-Updates\" class=\"headerlink\" title=\"Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates\"></a>Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates</h3><p><strong>创新点：</strong>提出了一个空时MRF模型。为了学习每个local node的正常行为模式，用Mixture of Probabilistic Principal Component  Analyzers(MPPCA) 来capture光流的分布。另外，模型参数可以在新的观测进来的时候updated incrementally。<br><strong>方法：</strong>We extract optical flow features at each frame, use MPPCA to identify the typical patterns, and construct a space-time MRF to enable inference at each local site.<br><strong>作者说自己的优势：</strong>1.可以在local和global context检测异常活动。比单纯是local的方法好，local的方法fails to detect abnormal activities with irregular temporal orderings，并且local的方法对于光流参数很敏感导致高的false alarm rate.比单纯是global的方法好，global的方法fails to detect abnormal activity happens within a region so small,这个region在全局的场景中简单的被视为可以忽略的噪声。并且global的方法在拥挤的环境中会产生false alarm。<br><strong>对前人的方法做了什么改进：</strong>用了08年Robust Real-Time Unusual Event Detection Using Multiple…的subway数据集的gt，但是capture 更微小的异常，比如“no payment”和“loitering”.<br><strong>误检或者漏检的情况：</strong>entrance gate数据集中1.走的慢的人。2.对于far-filed area，产生了false alarm，因为光流对于far-filel area是不靠谱的。3.走的很快的人。4.没刷卡的人。exit gate数据集中”from right exit to left exit”</p>\n<h2 id=\"2008\"><a href=\"#2008\" class=\"headerlink\" title=\"2008\"></a>2008</h2><h3 id=\"Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors\"><a href=\"#Robust-Real-Time-Unusual-Event-Detection-Using-Multiple-Fixed-Location-Monitors\" class=\"headerlink\" title=\"Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors\"></a>Robust Real-Time Unusual Event Detection Using Multiple Fixed-Location Monitors</h3><p><strong>方法：</strong>local-monitors-based 。<br>通过multiple, local, low-level feature monitors来监视不寻常的事件。每个monitor是从视频流提取local low-level observation的object。这个observation可以是在monitor的位置的现在的光流方向，或者是local flow的magnitude。<br><strong>异常检测需要什么：</strong>1.对于给定的视频流的tuning 算法应该简单快速。2.算法应该adaptive，适应环境的变换。3.short learning period。4.低成本。5.predictable performance。<br><strong>局限性：</strong>不能检测loitering person或者在进入安检的时候不刷卡。总结里面说，局限性是the lack of <strong>sequential</strong> monitoring.<br><strong>术语：</strong>aperture problem孔径问题<a href=\"https://blog.csdn.net/hankai1024/article/details/23433157\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/hankai1024/article/details/23433157</a>；SSD error matrix<br><strong>评价：</strong>2009年Observe Locally, Infer Globally: a Space-Time MRF for Detecting Abnormal Activities with Incremental Updates评价：focus attention on individual local activities,where<br>typical flow directions and speeds are measured on a grid in the video frame. While efficient and simple to implement, <strong>such an approach fails to model temporal relationships between motions.</strong></p>\n<h2 id=\"2007\"><a href=\"#2007\" class=\"headerlink\" title=\"2007\"></a>2007</h2><h3 id=\"Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-adetection-in-crowded-scenes\"><a href=\"#Chaotic-invariants-of-lagrangian-particle-trajectories-for-anomaly-adetection-in-crowded-scenes\" class=\"headerlink\" title=\"Chaotic invariants of lagrangian particle trajectories for anomaly adetection in crowded scenes\"></a>Chaotic invariants of lagrangian particle trajectories for anomaly adetection in crowded scenes</h3><p>特殊的粒子轨迹的应用<br>引入了chaotic dynamics</p>"},{"title":"异常行为检测文献新论文跟进","date":"2018-11-11T05:29:43.000Z","comments":1,"_content":"\n# PredGAN - a deep multi-scale video prediction framework for detecting anomalies in videos\n\n**来源：**引用了cvpr2018future frame prediction for anomaly detection\n**创新点：**引入了EMD评价指标。Earth Mover's Distance(EMD)。来判断帧是否是异常。是第一次将EMD作为评价video prediction framework的结果的评价指标。\n**贡献：**一、提出了一个video prediction framework来检测视频中的异常。用正常事件进行训练，能够准确预测视频帧的evolution。二、引入了EMD作为评估生成帧的质量的指标，用这个标准将帧标为异常或正常。三、证明在UCSD 行人数据集和Avenue数据集上的结果达到了state-of-the-art。\n\n<!--more-->\n\n# Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection\n\n**来源：**引用了cvpr2018future frame prediction for anomaly detection\n**摘要：**很多方法都采用了supervised setting，即需要收集正常的事件来训练，但是很少的能在不事先知道正常事件的前提下检测异常。现在的无监督方法检测剧烈局部变化的为异常，忽略了全局的时空上下文。\n**创新点：**提出了一个新的无监督方法，包括两个阶段：首先是normality estimation stage，训练了一个自编码器，并通过自适应重建误差阈值从整个未标记的视频中全局地估计正常事件。第二，normality modeling stage，将从上个阶段估计的正常事件喂给one-class svm来建立一个refined normality model，后续可以排除异常事件并且提高异常检测的性能。\n**引言：**现有的方法分成两类：对正常异常进行建模和仅对正常进行建模。第一类的泛化能力差，不能处理没见过的异常行为。现有的方法采用了一种supervised setting，需要人工来确定训练集，（我觉得这点考虑很好），就是说需要人来把视频分成仅包含正常的和其他的包含异常的。作者想通过unsupervised setting，实现不需要事先知道正常事件来训练一个正常模型。【什么意思，没太看懂】[8,28]两篇文章也用了这样的思路但是没有考虑全局的时空上下文。\n**结果：**在ucsd ped1上面的结果还挺差的。\n**有意思的地方：**在图6中，c图当一个人扔包的时候，另外一个人受到了惊吓，也被检测出来了。有意思有意思。\n**细节：**在Normality Estimation Stage，以self-adaptive 的方式选择合适的阈值T。 通过让重建误差损失函数的inter-class variance。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/29287285.jpg)\n\n# Spatio-Temporal AutoEncoder for Video Anormaly Detection\n\n**来源：**ACM mm2017 作者是alibaba的【感觉是个水会】\n**代码地址：**[https://github.com/yshean/abnormal-spatiotemporal-ae](https://github.com/yshean/abnormal-spatiotemporal-ae)\n**创新点：**STAE 时空编码器来自动学习视频的表示，提取时间和空间特征。引入了一个weight-decreasing prediction loss来产生未来的帧。（这个loss guides 编码器更好的提取时间特征）。\n引入这个weight-decreasing prediction loss是因为模型训练更容易被后续帧里面出现的新的目标影响。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97828510.jpg)\n\n# Generative Neural Networks for Anomaly Detection in Crowded Scenes\n\n**来源：**期刊IEEE Transactions on Information Forensics and Security二区\n**代码地址：**[https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection](https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection)\n**创新点：**S2-VAE（2是上标）。SF-VAE（F是下标）是一个**浅层**的生成网络生成来得到一个像Gaussian mixture 的模型来适合真实数据的分布。SC-VAE（C是下标），是个**深度**生成网络，利用了CNN和skip connection的优点。**方法细节：**SF-VAE用来从原始的样本中过滤中一些明显正常的样本，可以减少在下一个阶段的训练和测试时间。在第二个阶段，剩下的样本是先enlarged，然后进入到SC-VAE中。SC-VAE的卷积操作可以从输入中学习到hierarchical 特征和local relationship。SC-VAE比SF-VAE有更强的学习能力。\n**实验细节：**先做预处理，先用FCN提取前景。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/78253764.jpg)\n**作者的一些思考？：**根据模式识别和机器学习，simple Gaussian 分布没有能力来描述复杂的结构，然而，the mixture of Gaussian distribution在适应真实数据的分布上更有能力。\n\n# Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models\n\n**来源：**pattern recognition letters 大类3区小类4区\n**创新点：**用了energy-based models。【这篇文章的切入点是深度置信网络】\n**异常检测有挑战的地方：**标注数据耗费劳动力，能够利用未标注的数据就好了。第二个是定义不明确。\n**作者称自己的优势：**大多数存在的系统能高性能的检测异常，但是不能解释为什么得到了这些检测。我们的模型可以理解场景，解释为什么产生了fault alarms，因此我们的检测结果是可解释的。我们是第一次将DBM用在视频数据中的异常检测的，也是第一次在DBM的文献中用a single model来同时聚类和重建数据的。\n**实验部分： **  **A.Scene clustering** 这个部分的结果和k-means 聚类进行了比较**B.Scene reconstructing**  **C.Anomaly detection **和一些无监督的异常检测系统进行比较。无监督的异常检测系统可以分为(a)无监督学习方法，包括PCA，OC-SVM和GMM(高斯混合模型).(b)CAE和ConvAE。**D.Video analysis and model explanation **从图10可以看出，帧90和帧110中都有一个骑自行车的人，这个人渐行渐远，当这个人变得特别小的时候，就和别的行人颜色什么一样了，因此就解释了为什么产生了误检。\n**题外话：**这个paper的图都好好看好有意思啊......\n另外作者一直在说为什么没用RBM而用了DBM。因为在RBM中聚类模块和重建模块是分开的，所以不能保证在abstract representation和detection decision中得到一个校准（对齐？），因此我们看到的pattern maps不能反应模型真正的做的东西。而DBM可以同时训练聚类层和重建层。","source":"_posts/异常行为检测文献新论文跟进.md","raw":"---\ntitle: 异常行为检测文献新论文跟进\ndate: 2018-11-11 13:29:43\ntags:\n- anomaly detection\ncategories:\n- paper\ncomments: true\n---\n\n# PredGAN - a deep multi-scale video prediction framework for detecting anomalies in videos\n\n**来源：**引用了cvpr2018future frame prediction for anomaly detection\n**创新点：**引入了EMD评价指标。Earth Mover's Distance(EMD)。来判断帧是否是异常。是第一次将EMD作为评价video prediction framework的结果的评价指标。\n**贡献：**一、提出了一个video prediction framework来检测视频中的异常。用正常事件进行训练，能够准确预测视频帧的evolution。二、引入了EMD作为评估生成帧的质量的指标，用这个标准将帧标为异常或正常。三、证明在UCSD 行人数据集和Avenue数据集上的结果达到了state-of-the-art。\n\n<!--more-->\n\n# Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection\n\n**来源：**引用了cvpr2018future frame prediction for anomaly detection\n**摘要：**很多方法都采用了supervised setting，即需要收集正常的事件来训练，但是很少的能在不事先知道正常事件的前提下检测异常。现在的无监督方法检测剧烈局部变化的为异常，忽略了全局的时空上下文。\n**创新点：**提出了一个新的无监督方法，包括两个阶段：首先是normality estimation stage，训练了一个自编码器，并通过自适应重建误差阈值从整个未标记的视频中全局地估计正常事件。第二，normality modeling stage，将从上个阶段估计的正常事件喂给one-class svm来建立一个refined normality model，后续可以排除异常事件并且提高异常检测的性能。\n**引言：**现有的方法分成两类：对正常异常进行建模和仅对正常进行建模。第一类的泛化能力差，不能处理没见过的异常行为。现有的方法采用了一种supervised setting，需要人工来确定训练集，（我觉得这点考虑很好），就是说需要人来把视频分成仅包含正常的和其他的包含异常的。作者想通过unsupervised setting，实现不需要事先知道正常事件来训练一个正常模型。【什么意思，没太看懂】[8,28]两篇文章也用了这样的思路但是没有考虑全局的时空上下文。\n**结果：**在ucsd ped1上面的结果还挺差的。\n**有意思的地方：**在图6中，c图当一个人扔包的时候，另外一个人受到了惊吓，也被检测出来了。有意思有意思。\n**细节：**在Normality Estimation Stage，以self-adaptive 的方式选择合适的阈值T。 通过让重建误差损失函数的inter-class variance。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/29287285.jpg)\n\n# Spatio-Temporal AutoEncoder for Video Anormaly Detection\n\n**来源：**ACM mm2017 作者是alibaba的【感觉是个水会】\n**代码地址：**[https://github.com/yshean/abnormal-spatiotemporal-ae](https://github.com/yshean/abnormal-spatiotemporal-ae)\n**创新点：**STAE 时空编码器来自动学习视频的表示，提取时间和空间特征。引入了一个weight-decreasing prediction loss来产生未来的帧。（这个loss guides 编码器更好的提取时间特征）。\n引入这个weight-decreasing prediction loss是因为模型训练更容易被后续帧里面出现的新的目标影响。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97828510.jpg)\n\n# Generative Neural Networks for Anomaly Detection in Crowded Scenes\n\n**来源：**期刊IEEE Transactions on Information Forensics and Security二区\n**代码地址：**[https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection](https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection)\n**创新点：**S2-VAE（2是上标）。SF-VAE（F是下标）是一个**浅层**的生成网络生成来得到一个像Gaussian mixture 的模型来适合真实数据的分布。SC-VAE（C是下标），是个**深度**生成网络，利用了CNN和skip connection的优点。**方法细节：**SF-VAE用来从原始的样本中过滤中一些明显正常的样本，可以减少在下一个阶段的训练和测试时间。在第二个阶段，剩下的样本是先enlarged，然后进入到SC-VAE中。SC-VAE的卷积操作可以从输入中学习到hierarchical 特征和local relationship。SC-VAE比SF-VAE有更强的学习能力。\n**实验细节：**先做预处理，先用FCN提取前景。\n![图片](http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/78253764.jpg)\n**作者的一些思考？：**根据模式识别和机器学习，simple Gaussian 分布没有能力来描述复杂的结构，然而，the mixture of Gaussian distribution在适应真实数据的分布上更有能力。\n\n# Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models\n\n**来源：**pattern recognition letters 大类3区小类4区\n**创新点：**用了energy-based models。【这篇文章的切入点是深度置信网络】\n**异常检测有挑战的地方：**标注数据耗费劳动力，能够利用未标注的数据就好了。第二个是定义不明确。\n**作者称自己的优势：**大多数存在的系统能高性能的检测异常，但是不能解释为什么得到了这些检测。我们的模型可以理解场景，解释为什么产生了fault alarms，因此我们的检测结果是可解释的。我们是第一次将DBM用在视频数据中的异常检测的，也是第一次在DBM的文献中用a single model来同时聚类和重建数据的。\n**实验部分： **  **A.Scene clustering** 这个部分的结果和k-means 聚类进行了比较**B.Scene reconstructing**  **C.Anomaly detection **和一些无监督的异常检测系统进行比较。无监督的异常检测系统可以分为(a)无监督学习方法，包括PCA，OC-SVM和GMM(高斯混合模型).(b)CAE和ConvAE。**D.Video analysis and model explanation **从图10可以看出，帧90和帧110中都有一个骑自行车的人，这个人渐行渐远，当这个人变得特别小的时候，就和别的行人颜色什么一样了，因此就解释了为什么产生了误检。\n**题外话：**这个paper的图都好好看好有意思啊......\n另外作者一直在说为什么没用RBM而用了DBM。因为在RBM中聚类模块和重建模块是分开的，所以不能保证在abstract representation和detection decision中得到一个校准（对齐？），因此我们看到的pattern maps不能反应模型真正的做的东西。而DBM可以同时训练聚类层和重建层。","slug":"异常行为检测文献新论文跟进","published":1,"updated":"2018-11-11T11:03:07.379Z","layout":"post","photos":[],"link":"","_id":"cjofrrc2g000ak8cyx4tu5yfh","content":"<h1 id=\"PredGAN-a-deep-multi-scale-video-prediction-framework-for-detecting-anomalies-in-videos\"><a href=\"#PredGAN-a-deep-multi-scale-video-prediction-framework-for-detecting-anomalies-in-videos\" class=\"headerlink\" title=\"PredGAN - a deep multi-scale video prediction framework for detecting anomalies in videos\"></a>PredGAN - a deep multi-scale video prediction framework for detecting anomalies in videos</h1><p><strong>来源：</strong>引用了cvpr2018future frame prediction for anomaly detection<br><strong>创新点：</strong>引入了EMD评价指标。Earth Mover’s Distance(EMD)。来判断帧是否是异常。是第一次将EMD作为评价video prediction framework的结果的评价指标。<br><strong>贡献：</strong>一、提出了一个video prediction framework来检测视频中的异常。用正常事件进行训练，能够准确预测视频帧的evolution。二、引入了EMD作为评估生成帧的质量的指标，用这个标准将帧标为异常或正常。三、证明在UCSD 行人数据集和Avenue数据集上的结果达到了state-of-the-art。</p>\n<a id=\"more\"></a>\n<h1 id=\"Detecting-Abnormality-without-Knowing-Normality-A-Two-stage-Approach-for-Unsupervised-Video-Abnormal-Event-Detection\"><a href=\"#Detecting-Abnormality-without-Knowing-Normality-A-Two-stage-Approach-for-Unsupervised-Video-Abnormal-Event-Detection\" class=\"headerlink\" title=\"Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection\"></a>Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection</h1><p><strong>来源：</strong>引用了cvpr2018future frame prediction for anomaly detection<br><strong>摘要：</strong>很多方法都采用了supervised setting，即需要收集正常的事件来训练，但是很少的能在不事先知道正常事件的前提下检测异常。现在的无监督方法检测剧烈局部变化的为异常，忽略了全局的时空上下文。<br><strong>创新点：</strong>提出了一个新的无监督方法，包括两个阶段：首先是normality estimation stage，训练了一个自编码器，并通过自适应重建误差阈值从整个未标记的视频中全局地估计正常事件。第二，normality modeling stage，将从上个阶段估计的正常事件喂给one-class svm来建立一个refined normality model，后续可以排除异常事件并且提高异常检测的性能。<br><strong>引言：</strong>现有的方法分成两类：对正常异常进行建模和仅对正常进行建模。第一类的泛化能力差，不能处理没见过的异常行为。现有的方法采用了一种supervised setting，需要人工来确定训练集，（我觉得这点考虑很好），就是说需要人来把视频分成仅包含正常的和其他的包含异常的。作者想通过unsupervised setting，实现不需要事先知道正常事件来训练一个正常模型。【什么意思，没太看懂】[8,28]两篇文章也用了这样的思路但是没有考虑全局的时空上下文。<br><strong>结果：</strong>在ucsd ped1上面的结果还挺差的。<br><strong>有意思的地方：</strong>在图6中，c图当一个人扔包的时候，另外一个人受到了惊吓，也被检测出来了。有意思有意思。<br><strong>细节：</strong>在Normality Estimation Stage，以self-adaptive 的方式选择合适的阈值T。 通过让重建误差损失函数的inter-class variance。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/29287285.jpg\" alt=\"图片\"></p>\n<h1 id=\"Spatio-Temporal-AutoEncoder-for-Video-Anormaly-Detection\"><a href=\"#Spatio-Temporal-AutoEncoder-for-Video-Anormaly-Detection\" class=\"headerlink\" title=\"Spatio-Temporal AutoEncoder for Video Anormaly Detection\"></a>Spatio-Temporal AutoEncoder for Video Anormaly Detection</h1><p><strong>来源：</strong>ACM mm2017 作者是alibaba的【感觉是个水会】<br><strong>代码地址：</strong><a href=\"https://github.com/yshean/abnormal-spatiotemporal-ae\" target=\"_blank\" rel=\"noopener\">https://github.com/yshean/abnormal-spatiotemporal-ae</a><br><strong>创新点：</strong>STAE 时空编码器来自动学习视频的表示，提取时间和空间特征。引入了一个weight-decreasing prediction loss来产生未来的帧。（这个loss guides 编码器更好的提取时间特征）。<br>引入这个weight-decreasing prediction loss是因为模型训练更容易被后续帧里面出现的新的目标影响。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97828510.jpg\" alt=\"图片\"></p>\n<h1 id=\"Generative-Neural-Networks-for-Anomaly-Detection-in-Crowded-Scenes\"><a href=\"#Generative-Neural-Networks-for-Anomaly-Detection-in-Crowded-Scenes\" class=\"headerlink\" title=\"Generative Neural Networks for Anomaly Detection in Crowded Scenes\"></a>Generative Neural Networks for Anomaly Detection in Crowded Scenes</h1><p><strong>来源：</strong>期刊IEEE Transactions on Information Forensics and Security二区<br><strong>代码地址：</strong><a href=\"https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection\" target=\"_blank\" rel=\"noopener\">https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection</a><br><strong>创新点：</strong>S2-VAE（2是上标）。SF-VAE（F是下标）是一个<strong>浅层</strong>的生成网络生成来得到一个像Gaussian mixture 的模型来适合真实数据的分布。SC-VAE（C是下标），是个<strong>深度</strong>生成网络，利用了CNN和skip connection的优点。<strong>方法细节：</strong>SF-VAE用来从原始的样本中过滤中一些明显正常的样本，可以减少在下一个阶段的训练和测试时间。在第二个阶段，剩下的样本是先enlarged，然后进入到SC-VAE中。SC-VAE的卷积操作可以从输入中学习到hierarchical 特征和local relationship。SC-VAE比SF-VAE有更强的学习能力。<br><strong>实验细节：</strong>先做预处理，先用FCN提取前景。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/78253764.jpg\" alt=\"图片\"><br><strong>作者的一些思考？：</strong>根据模式识别和机器学习，simple Gaussian 分布没有能力来描述复杂的结构，然而，the mixture of Gaussian distribution在适应真实数据的分布上更有能力。</p>\n<h1 id=\"Detection-of-Unknown-Anomalies-in-Streaming-Videos-with-Generative-Energy-based-Boltzmann-Models\"><a href=\"#Detection-of-Unknown-Anomalies-in-Streaming-Videos-with-Generative-Energy-based-Boltzmann-Models\" class=\"headerlink\" title=\"Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models\"></a>Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models</h1><p><strong>来源：</strong>pattern recognition letters 大类3区小类4区<br><strong>创新点：</strong>用了energy-based models。【这篇文章的切入点是深度置信网络】<br><strong>异常检测有挑战的地方：</strong>标注数据耗费劳动力，能够利用未标注的数据就好了。第二个是定义不明确。<br><strong>作者称自己的优势：</strong>大多数存在的系统能高性能的检测异常，但是不能解释为什么得到了这些检测。我们的模型可以理解场景，解释为什么产生了fault alarms，因此我们的检测结果是可解释的。我们是第一次将DBM用在视频数据中的异常检测的，也是第一次在DBM的文献中用a single model来同时聚类和重建数据的。<br><strong>实验部分： </strong>  <strong>A.Scene clustering</strong> 这个部分的结果和k-means 聚类进行了比较<strong>B.Scene reconstructing</strong>  <strong>C.Anomaly detection </strong>和一些无监督的异常检测系统进行比较。无监督的异常检测系统可以分为(a)无监督学习方法，包括PCA，OC-SVM和GMM(高斯混合模型).(b)CAE和ConvAE。<strong>D.Video analysis and model explanation </strong>从图10可以看出，帧90和帧110中都有一个骑自行车的人，这个人渐行渐远，当这个人变得特别小的时候，就和别的行人颜色什么一样了，因此就解释了为什么产生了误检。<br><strong>题外话：</strong>这个paper的图都好好看好有意思啊……<br>另外作者一直在说为什么没用RBM而用了DBM。因为在RBM中聚类模块和重建模块是分开的，所以不能保证在abstract representation和detection decision中得到一个校准（对齐？），因此我们看到的pattern maps不能反应模型真正的做的东西。而DBM可以同时训练聚类层和重建层。</p>\n","site":{"data":{}},"excerpt":"<h1 id=\"PredGAN-a-deep-multi-scale-video-prediction-framework-for-detecting-anomalies-in-videos\"><a href=\"#PredGAN-a-deep-multi-scale-video-prediction-framework-for-detecting-anomalies-in-videos\" class=\"headerlink\" title=\"PredGAN - a deep multi-scale video prediction framework for detecting anomalies in videos\"></a>PredGAN - a deep multi-scale video prediction framework for detecting anomalies in videos</h1><p><strong>来源：</strong>引用了cvpr2018future frame prediction for anomaly detection<br><strong>创新点：</strong>引入了EMD评价指标。Earth Mover’s Distance(EMD)。来判断帧是否是异常。是第一次将EMD作为评价video prediction framework的结果的评价指标。<br><strong>贡献：</strong>一、提出了一个video prediction framework来检测视频中的异常。用正常事件进行训练，能够准确预测视频帧的evolution。二、引入了EMD作为评估生成帧的质量的指标，用这个标准将帧标为异常或正常。三、证明在UCSD 行人数据集和Avenue数据集上的结果达到了state-of-the-art。</p>","more":"<h1 id=\"Detecting-Abnormality-without-Knowing-Normality-A-Two-stage-Approach-for-Unsupervised-Video-Abnormal-Event-Detection\"><a href=\"#Detecting-Abnormality-without-Knowing-Normality-A-Two-stage-Approach-for-Unsupervised-Video-Abnormal-Event-Detection\" class=\"headerlink\" title=\"Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection\"></a>Detecting Abnormality without Knowing Normality: A Two-stage Approach for Unsupervised Video Abnormal Event Detection</h1><p><strong>来源：</strong>引用了cvpr2018future frame prediction for anomaly detection<br><strong>摘要：</strong>很多方法都采用了supervised setting，即需要收集正常的事件来训练，但是很少的能在不事先知道正常事件的前提下检测异常。现在的无监督方法检测剧烈局部变化的为异常，忽略了全局的时空上下文。<br><strong>创新点：</strong>提出了一个新的无监督方法，包括两个阶段：首先是normality estimation stage，训练了一个自编码器，并通过自适应重建误差阈值从整个未标记的视频中全局地估计正常事件。第二，normality modeling stage，将从上个阶段估计的正常事件喂给one-class svm来建立一个refined normality model，后续可以排除异常事件并且提高异常检测的性能。<br><strong>引言：</strong>现有的方法分成两类：对正常异常进行建模和仅对正常进行建模。第一类的泛化能力差，不能处理没见过的异常行为。现有的方法采用了一种supervised setting，需要人工来确定训练集，（我觉得这点考虑很好），就是说需要人来把视频分成仅包含正常的和其他的包含异常的。作者想通过unsupervised setting，实现不需要事先知道正常事件来训练一个正常模型。【什么意思，没太看懂】[8,28]两篇文章也用了这样的思路但是没有考虑全局的时空上下文。<br><strong>结果：</strong>在ucsd ped1上面的结果还挺差的。<br><strong>有意思的地方：</strong>在图6中，c图当一个人扔包的时候，另外一个人受到了惊吓，也被检测出来了。有意思有意思。<br><strong>细节：</strong>在Normality Estimation Stage，以self-adaptive 的方式选择合适的阈值T。 通过让重建误差损失函数的inter-class variance。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/29287285.jpg\" alt=\"图片\"></p>\n<h1 id=\"Spatio-Temporal-AutoEncoder-for-Video-Anormaly-Detection\"><a href=\"#Spatio-Temporal-AutoEncoder-for-Video-Anormaly-Detection\" class=\"headerlink\" title=\"Spatio-Temporal AutoEncoder for Video Anormaly Detection\"></a>Spatio-Temporal AutoEncoder for Video Anormaly Detection</h1><p><strong>来源：</strong>ACM mm2017 作者是alibaba的【感觉是个水会】<br><strong>代码地址：</strong><a href=\"https://github.com/yshean/abnormal-spatiotemporal-ae\" target=\"_blank\" rel=\"noopener\">https://github.com/yshean/abnormal-spatiotemporal-ae</a><br><strong>创新点：</strong>STAE 时空编码器来自动学习视频的表示，提取时间和空间特征。引入了一个weight-decreasing prediction loss来产生未来的帧。（这个loss guides 编码器更好的提取时间特征）。<br>引入这个weight-decreasing prediction loss是因为模型训练更容易被后续帧里面出现的新的目标影响。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/97828510.jpg\" alt=\"图片\"></p>\n<h1 id=\"Generative-Neural-Networks-for-Anomaly-Detection-in-Crowded-Scenes\"><a href=\"#Generative-Neural-Networks-for-Anomaly-Detection-in-Crowded-Scenes\" class=\"headerlink\" title=\"Generative Neural Networks for Anomaly Detection in Crowded Scenes\"></a>Generative Neural Networks for Anomaly Detection in Crowded Scenes</h1><p><strong>来源：</strong>期刊IEEE Transactions on Information Forensics and Security二区<br><strong>代码地址：</strong><a href=\"https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection\" target=\"_blank\" rel=\"noopener\">https://github.com/tianwangbuaa/VAE-for-abnormal-event-detection</a><br><strong>创新点：</strong>S2-VAE（2是上标）。SF-VAE（F是下标）是一个<strong>浅层</strong>的生成网络生成来得到一个像Gaussian mixture 的模型来适合真实数据的分布。SC-VAE（C是下标），是个<strong>深度</strong>生成网络，利用了CNN和skip connection的优点。<strong>方法细节：</strong>SF-VAE用来从原始的样本中过滤中一些明显正常的样本，可以减少在下一个阶段的训练和测试时间。在第二个阶段，剩下的样本是先enlarged，然后进入到SC-VAE中。SC-VAE的卷积操作可以从输入中学习到hierarchical 特征和local relationship。SC-VAE比SF-VAE有更强的学习能力。<br><strong>实验细节：</strong>先做预处理，先用FCN提取前景。<br><img src=\"http://boketuchuang.oss-cn-beijing.aliyuncs.com/18-11-11/78253764.jpg\" alt=\"图片\"><br><strong>作者的一些思考？：</strong>根据模式识别和机器学习，simple Gaussian 分布没有能力来描述复杂的结构，然而，the mixture of Gaussian distribution在适应真实数据的分布上更有能力。</p>\n<h1 id=\"Detection-of-Unknown-Anomalies-in-Streaming-Videos-with-Generative-Energy-based-Boltzmann-Models\"><a href=\"#Detection-of-Unknown-Anomalies-in-Streaming-Videos-with-Generative-Energy-based-Boltzmann-Models\" class=\"headerlink\" title=\"Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models\"></a>Detection of Unknown Anomalies in Streaming Videos with Generative Energy-based Boltzmann Models</h1><p><strong>来源：</strong>pattern recognition letters 大类3区小类4区<br><strong>创新点：</strong>用了energy-based models。【这篇文章的切入点是深度置信网络】<br><strong>异常检测有挑战的地方：</strong>标注数据耗费劳动力，能够利用未标注的数据就好了。第二个是定义不明确。<br><strong>作者称自己的优势：</strong>大多数存在的系统能高性能的检测异常，但是不能解释为什么得到了这些检测。我们的模型可以理解场景，解释为什么产生了fault alarms，因此我们的检测结果是可解释的。我们是第一次将DBM用在视频数据中的异常检测的，也是第一次在DBM的文献中用a single model来同时聚类和重建数据的。<br><strong>实验部分： </strong>  <strong>A.Scene clustering</strong> 这个部分的结果和k-means 聚类进行了比较<strong>B.Scene reconstructing</strong>  <strong>C.Anomaly detection </strong>和一些无监督的异常检测系统进行比较。无监督的异常检测系统可以分为(a)无监督学习方法，包括PCA，OC-SVM和GMM(高斯混合模型).(b)CAE和ConvAE。<strong>D.Video analysis and model explanation </strong>从图10可以看出，帧90和帧110中都有一个骑自行车的人，这个人渐行渐远，当这个人变得特别小的时候，就和别的行人颜色什么一样了，因此就解释了为什么产生了误检。<br><strong>题外话：</strong>这个paper的图都好好看好有意思啊……<br>另外作者一直在说为什么没用RBM而用了DBM。因为在RBM中聚类模块和重建模块是分开的，所以不能保证在abstract representation和detection decision中得到一个校准（对齐？），因此我们看到的pattern maps不能反应模型真正的做的东西。而DBM可以同时训练聚类层和重建层。</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjofrrc1s0000k8cynaaj2fft","category_id":"cjofrrc220004k8cyo1hakb2e","_id":"cjofrrc2m000fk8cyi4tjq3o0"},{"post_id":"cjofrrc200002k8cyvs6exyag","category_id":"cjofrrc220004k8cyo1hakb2e","_id":"cjofrrc2n000ik8cyouo3inyh"},{"post_id":"cjofrrc2a0006k8cyiyu0syhx","category_id":"cjofrrc2l000ek8cygr1p5cta","_id":"cjofrrc2p000mk8cyy96f2x9a"},{"post_id":"cjofrrc2e0008k8cy3izl4gat","category_id":"cjofrrc2n000jk8cyn81reohn","_id":"cjofrrc2q000qk8cy8cnvtu65"},{"post_id":"cjofrrc2g000ak8cyx4tu5yfh","category_id":"cjofrrc2n000jk8cyn81reohn","_id":"cjofrrc2r000sk8cykxwekigo"}],"PostTag":[{"post_id":"cjofrrc1s0000k8cynaaj2fft","tag_id":"cjofrrc260005k8cynedk56o5","_id":"cjofrrc2k000dk8cysth6p457"},{"post_id":"cjofrrc200002k8cyvs6exyag","tag_id":"cjofrrc2i000ck8cyegh0b1t8","_id":"cjofrrc2m000hk8cypi4bfdjg"},{"post_id":"cjofrrc2a0006k8cyiyu0syhx","tag_id":"cjofrrc2m000gk8cyyepzdy3z","_id":"cjofrrc2n000lk8cyczo5n9iy"},{"post_id":"cjofrrc2e0008k8cy3izl4gat","tag_id":"cjofrrc2n000kk8cyqawzlem6","_id":"cjofrrc2q000pk8cypu2cpzhd"},{"post_id":"cjofrrc2g000ak8cyx4tu5yfh","tag_id":"cjofrrc2n000kk8cyqawzlem6","_id":"cjofrrc2r000rk8cyf2u9y47k"}],"Tag":[{"name":"hexo","_id":"cjofrrc260005k8cynedk56o5"},{"name":"git","_id":"cjofrrc2i000ck8cyegh0b1t8"},{"name":"tensorflow","_id":"cjofrrc2m000gk8cyyepzdy3z"},{"name":"anomaly detection","_id":"cjofrrc2n000kk8cyqawzlem6"}]}}